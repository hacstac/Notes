<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="https://hacstac.github.io/Notes/feed.xml" rel="self" type="application/atom+xml" /><link href="https://hacstac.github.io/Notes/" rel="alternate" type="text/html" /><updated>2020-10-08T12:40:02-05:00</updated><id>https://hacstac.github.io/Notes/feed.xml</id><title type="html">Notebook</title><subtitle>A Daily Notebook for @Akash</subtitle><entry><title type="html">Kubernetes Volumes</title><link href="https://hacstac.github.io/Notes/kubernetes/2020/10/08/Kubernetes-Volumes.html" rel="alternate" type="text/html" title="Kubernetes Volumes" /><published>2020-10-08T00:00:00-05:00</published><updated>2020-10-08T00:00:00-05:00</updated><id>https://hacstac.github.io/Notes/kubernetes/2020/10/08/Kubernetes-Volumes</id><content type="html" xml:base="https://hacstac.github.io/Notes/kubernetes/2020/10/08/Kubernetes-Volumes.html">&lt;hr /&gt;

&lt;h2 id=&quot;10-kubernetes-volumes-attaching-disk-storage-to-containers&quot;&gt;1.0 Kubernetes Volumes: Attaching disk storage to containers&lt;/h2&gt;

&lt;p&gt;Kubernetes volumes are a component of a pod and are thus defined in the pod’s specification much like containers. They aren’t a standalone Kubernetes object and cannot be created or deleted on their own. A volume is available to all containers in the pod, but it must be mounted in each container that needs to access it. In each container, you can mount the volume in any location of its filesystem&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;11-why-we-need-persistant-volumes-with-pods&quot;&gt;1.1 Why we need persistant volumes with pods&lt;/h3&gt;

&lt;p&gt;In kubernetes each container in a pod has its own isolated filesystem, because the filesystem comes from the container’s image. Every new container starts off with the exact set of files that was added to the image at build time. Combine this with the fact that containers in a pod get restarted (either because the process died or because the liveness probe signaled to Kubernetes that the container wasn’t healthy anymore) and you’ll realize that the new container will not see anything that was written to the filesystem by the previous container, even though the newly started container runs in the same pod.&lt;/p&gt;

&lt;p&gt;In certain scenarios you want the new container to continue where the last one finished, such as when restarting a process on a physical machine. You may not need (or want) the whole filesystem to be persisted, but you do want to preserve the directories that hold actual data.&lt;/p&gt;

&lt;p&gt;Kubernetes provides this by defining storage volumes. They aren’t top-level resources like pods, but are instead defined as a part of a pod and share the same lifecycle as the pod. This means a volume is created when the pod is started and is destroyed when the pod is deleted. Because of this, a volume’s contents will persist across container restarts. After a container is restarted, the new container can see all the files that were written to the volume by the previous container. Also, if a pod contains multiple containers, the volume can be used by all of them at once.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;12-volume-types&quot;&gt;1.2 Volume Types&lt;/h3&gt;

&lt;p&gt;A wide variety of volume types is available. Several are generic, while others are specific to the actual storage technologies used underneath. Here’s a list of
several of the available volume types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;emptyDir&lt;/code&gt; — A simple empty directory used for storing transient data.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;hostPath&lt;/code&gt; — Used for mounting directories from the worker node’s filesystem
into the pod.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gitRepo&lt;/code&gt; — A volume initialized by checking out the contents of a Git repository.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nfs&lt;/code&gt; — An NFS share mounted into the pod.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;gcePersistentDisk&lt;/code&gt; (Google Compute Engine Persistent Disk), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;awsElasticBlockStore&lt;/code&gt; (Amazon Web Services Elastic Block Store Volume), &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;azureDisk&lt;/code&gt; (Microsoft Azure Disk Volume)—Used for mounting cloud provider-specific storage.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cinder&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;cephfs&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;iscsi&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flocker&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;glusterfs&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;quobyte&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;rbd&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;flexVolume&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;vsphere-Volume&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;photonPersistentDisk&lt;/code&gt;, scaleIO—Used for mounting other types of network storage.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;configMap&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;secret&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;downwardAPI&lt;/code&gt; — Special types of volumes used to expose certain Kubernetes resources and cluster information to the pod.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;persistentVolumeClaim&lt;/code&gt; — A way to use a pre- or dynamically provisioned per`sistent storage.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These volume types serve various purposes. You’ll learn about some of them in the
following sections. Special types of volumes ( secret, downwardAPI, configMap) are covered in the next phases, because they aren’t used for storing data, but for exposing Kubernetes metadata to apps running in the pod.&lt;/p&gt;

&lt;h4 id=&quot;121-emptydir-volume&quot;&gt;1.2.1 emptyDir Volume&lt;/h4&gt;

&lt;p&gt;The simplest volume type is the emptyDir volume. The app running inside the pod can then write any files it needs to it. Because the volume’s lifetime is tied to that of the pod, the volume’s contents are lost when the pod is deleted. An emptyDir volume is especially useful for sharing files between containers running in the same pod. But it can also be used by a single container for when a container needs to write data to disk temporarily, such as when performing a sort operation on a large dataset, which can’t fit into the available memory. The data could also be written to the container’s filesystem itself (remember the top read-write layer in a container?), but subtle differences exist between the two options. A container’s filesystem may not even be writable.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Ex : The pod contains two containers and a single volume that’s mounted in both of them, yet at different paths. When the html-generator container starts, it starts writing the output of the fortune command to the /var/htdocs/index.html file every 10 seconds. Because the volume is mounted at /var/htdocs, the index.html file is writ- ten to the volume instead of the container’s top layer. As soon as the web-server container starts, it starts serving whatever HTML files are in the /usr/share/nginx/html directory (this is the default directory Nginx serves files from). Because you mounted the volume in that exact location, Nginx will serve the index.html file written there by the container running the fortune loop. The end effect is that a client sending an HTTP request to the pod on port 80 will receive the current fortune message as the response.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The emptyDir you used as the volume was created on the actual disk of the worker node hosting your pod, so its performance depends on the type of the node’s disks.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;---yaml&lt;/span&gt;
apiVersion: v1
kind: Pod
metadata:
  name: fortune
spec:
  containers:
  - image: luksa/fortune
    name: html-generator
    volumeMounts:
    - name: html
      mountPath: /var/htdocs
  - image: nginx:alpine
    name: web-server
    volumeMounts:
    - name: html
      mountPath: /usr/share/nginx/html
      readOnly: &lt;span class=&quot;nb&quot;&gt;true
    &lt;/span&gt;ports:
    - containerPort: 80
      protocol: TCP
    volumes:
    - name: html
    emptyDir: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./fortune.yaml
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl port-forward fortune 8080:80

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl http://localhost:8080
Beware of a tall blond man with one black shoe.
&lt;span class=&quot;c&quot;&gt;# If you wait a few seconds and send another request, you should receive a different message.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# To use emptyDir on tmpfs ( on ram instead of disk )&lt;/span&gt;
volumes:
  - name: html
    emptyDir:
      medium: Memory
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;122-git-repo&quot;&gt;1.2.2 Git Repo&lt;/h4&gt;

&lt;p&gt;A gitRepo volume is basically an emptyDir volume that gets populated by cloning a
Git repository and checking out a specific revision when the pod is starting up (but
before its containers are created).&lt;/p&gt;

&lt;p&gt;For example, you can use a Git repository to store static HTML files of your website
and create a pod containing a web server container and a gitRepo volume. Every time
the pod is created, it pulls the latest version of your website and starts serving it. The only drawback to this is that you need to delete the pod every time you push changes to the gitRepo and want to start serving the new version of the website.&lt;/p&gt;

&lt;p&gt;Note: You need a github repo for the website/App&lt;/p&gt;

&lt;p&gt;When you create the pod, the volume is first initialized as an empty directory and then the specified Git repository is cloned into it. If you hadn’t set the directory to . (dot), the repository would have been cloned into the kubia-website-example subdirectory, which isn’t what you want. You want the repo to be cloned into the root directory of your volume. Along with the repository, you also specified you want Kubernetes to check out whatever revision the master branch is pointing to at the time the volume is created.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;gitrepo-volume-pod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;nginx:alpine&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;web-server&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumeMounts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;html&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;mountPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/usr/share/nginx/html&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;readOnly&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;true&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;html&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;gitRepo&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;repository&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;https://github.com/luksa/kubia-website-example.git&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;revision&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;master&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;directory&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;To keep your app sync with github, you’ll need a sidecar container because git sync process shouldn’t run in the same container as the Nginx web server, but in a second container: a sidecar container. A sidecar container is a container that augments the operation of the main container of the pod. You add a sidecar to a pod so you can use an existing container image instead of cramming additional logic into the main app’s code, which would make it overly complex and less reusable. To find an existing container image, which keeps a local directory synchronized with a Git repository, go to Docker Hub and search for “git sync.” You’ll find many images that do that. Then use the image in a new container in the pod from the previous example, mount the pod’s existing gitRepo volume in the new container, and configure the Git sync container to keep the files in sync with your Git repo. If you set everything up correctly, you should see that the files the web server is serving are kept in sync with your GitHub repo.&lt;/p&gt;

&lt;p&gt;A gitRepo volume, like the emptyDir volume, is basically a dedicated directory created specifically for, and used exclusively by, the pod that contains the volume. When the pod is deleted, the volume and its contents are deleted. Other types of volumes, however, don’t create a new directory, but instead mount an existing external directory into the pod’s container’s filesystem. The contents of that volume can survive multiple pod instantiations.&lt;/p&gt;

&lt;h4 id=&quot;123-hostpath-volume&quot;&gt;1.2.3 hostPath Volume&lt;/h4&gt;

&lt;p&gt;Most pods should be oblivious of their host node, so they shouldn’t access any files on the node’s filesystem. But certain system-level pods (remember, these will usually be managed by a DaemonSet) do need to either read the node’s files or use the node’s
filesystem to access the node’s devices through the filesystem. Kubernetes makes this
possible through a hostPath volume. A hostPath volume points to a specific file or directory on the node’s filesystem.&lt;/p&gt;

&lt;p&gt;hostPath volumes are the first type of persistent storage we’re introducing, because both the gitRepo and emptyDir volumes’ contents get deleted when a pod is torn down, whereas a hostPath volume’s contents don’t. If a pod is deleted and the next pod uses a hostPath volume pointing to the same path on the host, the new pod will see whatever was left behind by the previous pod, but only if it’s scheduled to the same node as the first pod.&lt;/p&gt;

&lt;p&gt;If you’re thinking of using a hostPath volume as the place to store a database’s data directory, think again. Because the volume’s contents are stored on a specific node’s filesystem, when the database pod gets rescheduled to another node, it will no longer see the data. This explains why it’s not a good idea to use a hostPath volume for regular pods, because it makes the pod sensitive to what node it’s scheduled to.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# To check existing pods using the hostPath or not&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl describe pod kubia-x-x &lt;span class=&quot;nt&quot;&gt;--namespace&lt;/span&gt; kube-system
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;124-persistent-storage&quot;&gt;1.2.4 Persistent Storage&lt;/h4&gt;

&lt;p&gt;When an application running in a pod needs to persist data to disk and have that same data available even when the pod is rescheduled to another node, you can’t use any of the volume types we’ve mentioned so far. Because this data needs to be accessible from any cluster node, it must be stored on some type of network-attached storage (NAS).&lt;/p&gt;

&lt;p&gt;To learn about volumes that allow persisting data, you’ll create a pod that will run the MongoDB document-oriented NoSQL database. Running a database pod without a volume or with a non-persistent volume doesn’t make sense, except for testing purposes, so you’ll add an appropriate type of volume to the pod and mount it in the MongoDB container.&lt;/p&gt;

&lt;p&gt;for example : we can use google kubernetes engine, where we can use GCE persistent disk as underlying storage mechanism.&lt;/p&gt;

&lt;p&gt;Ex : Creating a GCE Persistent Disk in a pod volume&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Create a cluster&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;gcloud compute disks create &lt;span class=&quot;nt&quot;&gt;--size&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1GB &lt;span class=&quot;nt&quot;&gt;--zone&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;southeast-asia1-a mongodb

&lt;span class=&quot;c&quot;&gt;# Create a pod using gcePersistentDisk&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---yaml&lt;/span&gt;
apiVersion: v1
kind: Pod
metadata:
  name: mongodb
spec:
  volumes:
  - name: mongodb-data
    gcePersistentDisk:
    pdName: mongodb
    fsType: ext4
  containers:
  - image: mongo
    name: mongodb
    volumeMounts:
    - name: mongodb-data
      mountPath: /data/db
    ports:
    - containerPort: 27017
      protocol: TCP

&lt;span class=&quot;c&quot;&gt;# Spin up the pod&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./mongodb.yaml

&lt;span class=&quot;c&quot;&gt;# Entering the mongoDB Shell&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; mongodb mongo
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If you delete the pod and recreate it. Mongo uses exact same data, because it uses the persistent disk. This is an example of GCE, If your Kubernetes cluster is running on Amazon’s AWS EC2, for example, you can use an awsElasticBlockStore volume to provide persistent storage for your pods. If your cluster runs on Microsoft Azure, you can use the azureFile or the azureDisk volume.&lt;/p&gt;

&lt;h4 id=&quot;125-nfs-storage&quot;&gt;1.2.5 NFS Storage&lt;/h4&gt;

&lt;p&gt;If your cluster is running on your own set of servers, you have a vast array of other sup- ported options for mounting external storage inside your volume. For example, to mount a simple NFS share, you only need to specify the NFS server and the path exported by the server.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongodb-data&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;nfs&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;server&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;11.151.150.124&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/some/path&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;126-other-storage-technologies&quot;&gt;1.2.6 Other Storage Technologies&lt;/h4&gt;

&lt;p&gt;Other supported options include iscsi for mounting an ISCSI disk resource, glusterfs for a GlusterFS mount, rbd for a RADOS Block Device, flexVolume , cinder, cephfs, flocker, fc (Fibre Channel), and others. You don’t need to know all of them if you’re not using them. They’re mentioned here to show you that Kubernetes supports a broad range of storage technologies and you can use whichever you prefer and are used to.&lt;/p&gt;

&lt;p&gt;To see details on what properties you need to set for each of these volume types,
you can either turn to the Kubernetes API definitions in the Kubernetes API refer-
ence or look up the information through kubectl explain.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;13-deepdive-with-kubernetes-storage-technology&quot;&gt;1.3 DeepDive with Kubernetes Storage Technology&lt;/h3&gt;

&lt;p&gt;To enable apps to request storage in a Kubernetes cluster without having to deal with infrastructure specifics, two new resources were introduced. They are PersistentVolumes and PersistentVolumeClaims. The names may be a bit misleading, because as you’ve seen in the previous few sections, even regular Kubernetes volumes can be used to store persistent data.&lt;/p&gt;

&lt;p&gt;Using a PersistentVolume inside a pod is a little more complex than using a regular
pod volume, so let’s illustrate how pods, PersistentVolumeClaims, PersistentVolumes,
and the actual underlying storage relate to each other.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Kubernetes/PersistentStorage.png&quot; alt=&quot;PersistentVolume&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Explaination&lt;/strong&gt;: Instead of the developer adding a technology-specific volume to their pod, it’s the cluster administrator who sets up the underlying storage and then registers it in Kubernetes by creating a PersistentVolume resource through the Kubernetes API server. When creating the PersistentVolume, the admin specifies its size and the access modes it supports. When a cluster user needs to use persistent storage in one of their pods, they first create a PersistentVolumeClaim manifest, specifying the minimum size and the access mode they require. The user then submits the PersistentVolumeClaim manifest to the Kubernetes API server, and Kubernetes finds the appropriate PersistentVolume and binds the volume to the claim. The PersistentVolumeClaim can then be used as one of the volumes inside a pod. Other users cannot use the same PersistentVolume until it has been released by deleting the bound PersistentVolumeClaim.&lt;/p&gt;

&lt;h4 id=&quot;131-creating-a-persistentvolume&quot;&gt;1.3.1 Creating a PersistentVolume&lt;/h4&gt;

&lt;p&gt;When creating a PersistentVolume, the administrator needs to tell Kubernetes what its capacity is and whether it can be read from and/or written to by a single node or by multiple nodes at the same time. They also need to tell Kubernetes what to do with the PersistentVolume when it’s released (when the PersistentVolumeClaim it’s bound to is deleted. And last, but certainly not least, they need to specify the type, location, and other properties of the actual storage this PersistentVolume is backed by.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;PersistentVolume&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongodb-pv&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;capacity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;storage&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1Gi&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;accessModes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ReadOnlyMany&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;persistentVolumeReclaimPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Retain&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;gcePersistentDisk&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;pdName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;monodb&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;fsType&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ext4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Note: PersistentVolumes don’t belong to any namespaces&lt;/p&gt;

&lt;h4 id=&quot;132-persistentvolumeclaim&quot;&gt;1.3.2 PersistentVolumeClaim&lt;/h4&gt;

&lt;p&gt;Now let’s lay down our admin hats and put our developer hats back on. Say you need to deploy a pod that requires persistent storage. You’ll use the PersistentVolume you created earlier. But you can’t use it directly in the pod. You need to claim it first. Claiming a PersistentVolume is a completely separate process from creating a pod, because you want the same PersistentVolumeClaim to stay available even if the pod is rescheduled (remember, rescheduling means the previous pod is deleted and a new one is created).&lt;/p&gt;

&lt;p&gt;As soon as you create the claim, Kubernetes finds the appropriate PersistentVolume and binds it to the claim. The PersistentVolume’s capacity must be large enough to accommodate what the claim requests. Additionally, the volume’s access modes must include the access modes requested by the claim. In your case, the claim requests 1 GiB of storage and a ReadWriteOnce access mode. The PersistentVolume you created earlier matches those two requirements so it is bound to your claim.&lt;/p&gt;

&lt;p&gt;The claim is shown as Bound to PersistentVolume mongodb-pv. Note the abbreviations
used for the access modes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;RWO—ReadWriteOnce—Only a single node can mount the volume for reading and writing.&lt;/li&gt;
  &lt;li&gt;ROX—ReadOnlyMany—Multiple nodes can mount the volume for reading.&lt;/li&gt;
  &lt;li&gt;RWX—ReadWriteMany—Multiple nodes can mount the volume for both reading and writing.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;PersistentVolumeClaim&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongodb-pvc&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;resources&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;requests&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;storage&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;1Gi&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;accessModes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ReadWriteOnce&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;storageClassName:&quot;&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;133-create-a-pod-using-pvc&quot;&gt;1.3.3 Create a Pod using PVC&lt;/h4&gt;

&lt;p&gt;The PersistentVolume is now yours to use. Nobody else can claim the same volume
until you release it. To use it inside a pod, you need to reference the Persistent-
VolumeClaim by name inside the pod’s volume (yes, the PersistentVolumeClaim, not
the PersistentVolume directly!).&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Pod&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongodb&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongo&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongodb&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;volumeMounts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongodb-data&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;mountPath&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/data/db&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;containerPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;27017&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;protocol&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;TCP&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;volumes&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongodb-data&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;persistentVolumeClaim&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;claimName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mongodb-pvc&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# List out the PersistentVolumes&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pv

&lt;span class=&quot;c&quot;&gt;# List out the PersistentVolumesClaims&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pvc
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;134-recycling-persistentvolumes&quot;&gt;1.3.4 Recycling PersistentVolumes&lt;/h4&gt;

&lt;p&gt;If you delete the pod and the pvc and create a new PVC on it, It will show a pending state because The STATUS column shows the PersistentVolume as Released, not Available like before. Because you’ve already used the volume, it may contain data and shouldn’t be bound to a completely new claim without giving the cluster admin a chance to clean it up. Without this, a new pod using the same PersistentVolume could read the data stored there by the previous pod, even if the claim and pod were created in a different namespace (and thus likely belong to a different cluster tenant).&lt;/p&gt;

&lt;p&gt;To reclaim the pv, there are two policies exist: Recycle and Delete. The first one deletes
the volume’s contents and makes the volume available to be claimed again. This way, the PersistentVolume can be reused multiple times by different PersistentVolumeClaims and different pods. The Delete policy, on the other hand, deletes the underlying storage. Note that the Recycle option is currently not available for GCE Persistent Disks.&lt;/p&gt;

&lt;p&gt;A PersistentVolume only supports the Retain or Delete policies. Other PersistentVolume types may or may not support each of these options, so before creating your own PersistentVolume, be sure to check what reclaim policies are supported for the specific underlying storage you’ll use in the volume.&lt;/p&gt;

&lt;h4 id=&quot;135-dynamic-provisioning-of-persistentvolumes&quot;&gt;1.3.5 Dynamic provisioning of PersistentVolumes&lt;/h4&gt;

&lt;p&gt;You’ve seen how using PersistentVolumes and PersistentVolumeClaims makes it easy to obtain persistent storage without the developer having to deal with the actual stor- age technology used underneath. But this still requires a cluster administrator to pro- vision the actual storage up front. Luckily, Kubernetes can also perform this job automatically through dynamic provisioning of PersistentVolumes. The cluster admin, instead of creating PersistentVolumes, can deploy a Persistent- Volume provisioner and define one or more StorageClass objects to let users choose what type of PersistentVolume they want. The users can refer to the StorageClass in their PersistentVolumeClaims and the provisioner will take that into account when provisioning the persistent storage.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Kubernetes/dynamicPV.png&quot; alt=&quot;dynamicPeristentVolume&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;---storageClass&lt;/span&gt;.yaml
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast
provisioner: kubernetes.io/gce-pd
parameters:
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: pd-ssd
  zone: asia-southest1-b

&lt;span class=&quot;nt&quot;&gt;---PersistentVolume&lt;/span&gt;.yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: mongodb-pvc
spec:
  storageClassName: fast
  resources:
    requests:
      storage: 100Mi
  accessModes:
    - ReadWriteOnce

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get sc
NAME                     TYPE
fast                     kubernetes.io/gce-pd
standard &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;default&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;       kubernetes.io/gce-pd

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pv
NAME          CAPACITY      ACCESSMODES      RECLAIMPOLICY      STATUS      STORAGECLASS
mongodb-pv    1Gi           RWO,ROX          Retain             Released
pvc-1e6bc048  1Gi           RWO              Delete             Bound       fast

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pvc mongodb-pvc
NAME         STATUS      VOLUME       CAPACITY      ACCESSMODES      STORAGECLASS
mongodb-pvc  Bound      pvc-1e6bc048  1Gi           RWO              fast
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Kubernetes" /><summary type="html"></summary></entry><entry><title type="html">Kubernetes Service Discovery</title><link href="https://hacstac.github.io/Notes/kubernetes/2020/09/29/Kubernetes-Service-Discovery.html" rel="alternate" type="text/html" title="Kubernetes Service Discovery" /><published>2020-09-29T00:00:00-05:00</published><updated>2020-09-29T00:00:00-05:00</updated><id>https://hacstac.github.io/Notes/kubernetes/2020/09/29/Kubernetes-Service-Discovery</id><content type="html" xml:base="https://hacstac.github.io/Notes/kubernetes/2020/09/29/Kubernetes-Service-Discovery.html">&lt;hr /&gt;

&lt;h2 id=&quot;11-what-is-service-discovery&quot;&gt;1.1 What is Service Discovery&lt;/h2&gt;

&lt;p&gt;A Kubernetes Service is a resource you create to make a single, constant point of entry to a group of pods providing the same service. Each service has an IP address and port that never change while the service exists. Clients can open connections to that IP and port, and those connections are then routed to one of the pods backing that service. This way, clients of a service don’t need to know the location of individual pods providing the service, allowing those pods to be moved around the cluster at any time.&lt;/p&gt;

&lt;p&gt;Service-discovery is tool that help to solve the problem of finding which processes are listening at which addresses for which services. A good service-discovery system will enable users to resolve this information quickly and reliably. A good system is also low-latency; clients are updated soon after the information associated with a service changes.&lt;/p&gt;

&lt;p&gt;If we understand this with an example: Where you have a frontend web server and a backend data base server. There may be multiple pods that all act as the frontend, but there may only be a single backend database pod. You need to solve two problems to make the system function:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;External clients need to connect to the frontend pods without caring if there’s
only a single web server or hundreds.&lt;/li&gt;
  &lt;li&gt;The frontend pods need to connect to the backend database. Because the database runs inside a pod, it may be moved around the cluster over time, causing its IP address to change. You don’t want to reconfigure the frontend pods every time the backend database is moved.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;By creating a service for the frontend pods and configuring it to be accessible from outside the cluster, you expose a single, constant IP address through which external clients can connect to the pods. Similarly, by also creating a service for the backend pod, you create a stable address for the backend pod. The service address doesn’t change even if the pod’s IP address changes. Additionally, by creating the service, you also enable the frontend pods to easily find the backend service by its name through either environment variables or DNS.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Kubernetes/Service_Discovery.png&quot; alt=&quot;Service Discovery&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;12-why-we-need-service-discovery&quot;&gt;1.2 Why we need service discovery&lt;/h2&gt;

&lt;p&gt;To solve the problem of multi node networking, where each pods share same private network and all those pods should be accessible through a single IP address. To access these pods outside a private network, we need a Kubernetes Services.&lt;/p&gt;

&lt;p&gt;If we consider a scenario, where in non kubernetes env where a sysadmin would configure each client app by specifying the exact IP address or hostname of the server providing the service in the client’s configuration files, doing the same in Kubernetes wouldn’t work, because:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Pods are ephemeral—They may come and go at any time, whether it’s because a pod is removed from a node to make room for other pods, because someone scaled down the number of pods, or because a cluster node has failed.&lt;/li&gt;
  &lt;li&gt;Kubernetes assigns an IP address to a pod after the pod has been scheduled to a node and before it’s started —Clients thus can’t know the IP address of the server pod up front.&lt;/li&gt;
  &lt;li&gt;Horizontal scaling means multiple pods may provide the same service—Each of those pods has its own IP address. Clients shouldn’t care how many pods are backing the service and what their IPs are. They shouldn’t have to keep a list of all the individual IPs of pods.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;That’s why we need Kubernetes Services.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;13--the-service-object&quot;&gt;1.3  The Service Object&lt;/h2&gt;

&lt;p&gt;Real service discovery in Kubernetes starts with a Service object. A Service object is a way to create a named label selector. As we will see, the Service object does some other nice things for us, too.&lt;/p&gt;

&lt;p&gt;Just as the kubectl run command is an easy way to create a Kubernetes deployment,
we can use kubectl expose to create a service. Let’s create some deployments and
services so we can see how they work:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## 1.0 Create a Deployment&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Boot the deployment directly by the kubectl command like ( given belowe ) or use a YAML file to setup a deployment.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Boot up the Deployment&lt;/span&gt;
kubectl run cyberchef &lt;span class=&quot;nt&quot;&gt;--image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;mpepping/cyberchef &lt;span class=&quot;nt&quot;&gt;--replicas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;3 &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;8000 &lt;span class=&quot;nt&quot;&gt;--labels&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ver=1, env=dev&quot;&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## 2.0 Expose a Deployment&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Creating the service using the YAML&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;---cyberchef_service&lt;/span&gt;.yml
apiVersion: v1
kind: Service
metadata:
  name: cyberchef
spec:
  ports:
  - port: 80 &lt;span class=&quot;c&quot;&gt;# Host Port&lt;/span&gt;
    targetPort: 8000 &lt;span class=&quot;c&quot;&gt;# Container Port&lt;/span&gt;
  selector:
    app: cyberchef

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./cyberchef_service.yml

&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## 3.0 Get the Services&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Get the clusterIP using getting the service&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get svc &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide
NAME         CLUSTER-IP         PORT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;           SELECTOR
cyberchef    10.115.242.13      80/TCP            &lt;span class=&quot;nv&quot;&gt;ver&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1, &lt;span class=&quot;nb&quot;&gt;env&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;dev
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The list shows that the IP address assigned to the service is 10.115.242.13. Because this is the cluster IP, it’s only accessible from inside the cluster. The primary purpose of services is exposing groups of pods to other pods in the cluster, but you’ll usually also want to expose services externally.&lt;/p&gt;

&lt;h3 id=&quot;131-testing-your-service-from-within-the-cluster&quot;&gt;1.3.1 Testing your service from within the cluster&lt;/h3&gt;

&lt;p&gt;You can send requests to your service from within the cluster in a few ways:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The obvious way is to create a pod that will send the request to the service’s cluster IP and log the response. You can then examine the pod’s log to see what the service’s response was.&lt;/li&gt;
  &lt;li&gt;You can ssh into one of the Kubernetes nodes and use the curl command.&lt;/li&gt;
  &lt;li&gt;You can execute the curl command inside one of your existing pods through the kubectl exec command.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;remotely-executing-commands-in-running-containers&quot;&gt;Remotely executing commands in running containers&lt;/h4&gt;

&lt;p&gt;The kubectl exec command allows you to remotely run arbitrary commands inside an existing container of a pod. This comes in handy when you want to examine the contents, state, and/or environment of a container. List the pods with the kubectl get pods command and choose one as your target for the exec command.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;cyberchef-d987s &lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; curl &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; http://10.115.242.13
&amp;lt;&lt;span class=&quot;o&quot;&gt;!&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt;
    CyberChef - The Cyber Swiss Army Knife
.......
.......
&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&amp;lt; Full Front Page HTML &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;132-configuring-session-affinity-on-the-service&quot;&gt;1.3.2 Configuring session affinity on the service&lt;/h3&gt;

&lt;p&gt;If you execute the same command a few more times, you should hit a different pod with every invocation, because the service proxy normally forwards each connection to a randomly selected backing pod, even if the connections are coming from the same client. If, on the other hand, you want all requests made by a certain client to be redirected to the same pod every time, you can set the service’s sessionAffinity property to ClientIP&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;sessionAffinity&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ClientIP&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This makes the service proxy redirect all requests originating from the same client IP to the same pod. As an exercise, you can create an additional service with session affinity set to ClientIP and try sending requests to it.&lt;/p&gt;

&lt;h3 id=&quot;133-exposing-multiple-ports-in-the-same-service&quot;&gt;1.3.3 Exposing multiple ports in the same service&lt;/h3&gt;

&lt;p&gt;Your service exposes only a single port, but services can also support multiple ports. For example, if your pods listened on two ports—let’s say 8080 for HTTP and 8443 for HTTPS—you could use a single service to forward both port 80 and 443 to the pod’s ports 8080 and 8443. You don’t need to create two different services in such cases. Using a single, multi-port service exposes all the service’s ports through a single cluster IP.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;http&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;8080&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;https&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;443&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;8443&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;134-discovering-services&quot;&gt;1.3.4 Discovering services&lt;/h3&gt;

&lt;h4 id=&quot;method-1-discovering-services-through-env-variables&quot;&gt;Method 1: Discovering services through env variables&lt;/h4&gt;

&lt;p&gt;When a pod is started, Kubernetes initializes a set of environment variables pointing to each service that exists at that moment. If you create the service before creating the client pods, processes in those pods can get the IP address and port of the service by inspecting their environment variables.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# To check out the env variables&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;cyberchef-d978l2 &lt;span class=&quot;nb&quot;&gt;env
&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
&lt;span class=&quot;nv&quot;&gt;HOSTNAME&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef-d978l2
&lt;span class=&quot;nv&quot;&gt;KUBERNETES_SERVICE_HOST&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10.115.115.10
&lt;span class=&quot;nv&quot;&gt;KUBERNETES_SERVICE_PORT&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;80
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Environment variables are one way of looking up the IP and port of a service, but isn’t this usually the domain of DNS. Let look at the DNS way&lt;/p&gt;

&lt;h4 id=&quot;method-2-discovering-services-through-dns&quot;&gt;Method 2: Discovering services through DNS&lt;/h4&gt;

&lt;p&gt;In the kubernetes env, One of the pod called as kube-dns. The kube-system namespace also includes a corresponding service with the same name. As the name suggests, the pod runs a DNS server, which all other pods running in the cluster are automatically configured to use (Kubernetes does that by modifying each container’s /etc/resolv.conf file). Any DNS query performed by a process running in a pod will be handled by Kubernetes’ own DNS server, which knows all the services running in your system.&lt;/p&gt;

&lt;p&gt;Each service gets a DNS entry in the internal DNS server, and client pods that know the name of the service can access it through its fully qualified domain name (FQDN) instead of resorting to environment variables. Now we connect to the service through its FQDN&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Ex frontend.default.svc.cluster.local&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# frontend = Service Name&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# default = NameSpace&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# svc.cluster.local = Configurable cluster domain suffix&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;it cyberchef-973l2 bash
-&amp;gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl http://cyberchef.default.svc.cluster.local
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;135-connect-to-services-from-outside-the-cluster&quot;&gt;1.3.5 Connect to services from outside the cluster&lt;/h3&gt;

&lt;p&gt;In most cases we need apps will exposed to external services through the Kubernetes services feature. Instead of having the service redirect connections to pods in the cluster, you want it to redirect to external IP(s) and port(s). This allows you to take advantage of both service load balancing and service discovery. Client pods running in the cluster can connect to the external service like they connect to internal services.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Service Endpoints: An Endpoints resource (yes, plural) is a list of IP addresses and ports exposing a service. The Endpoints resource is like any other Kubernetes resource, so you can display its basic info with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl get endpoints&lt;/code&gt;:&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;creating-the-service-endpoints&quot;&gt;Creating the service endpoints&lt;/h4&gt;

&lt;p&gt;If you create a service without a pod selector, Kubernetes won’t even create the
Endpoints resource (after all, without a selector, it can’t know which pods to include
in the service). It’s up to you to create the Endpoints resource to specify the list of endpoints for the service. To create a service with manually managed endpoints, you need to create both a Service and an Endpoints resource.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# Creating an endpoints resource for a service without a selector&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Endpoints&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef-service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;subsets&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;addresses&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;11.11.11.11&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# Ip of endpoint that the service will forward connections to&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;ip&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;22.22.22.22&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The Endpoints object needs to have the same name as the service and contain the list
of target IP addresses and ports for the service. After both the Service and the Endpoints resource are posted to the server, the service is ready to be used like any regular service with a pod selector. Containers created after the service is created will include the environment variables for the service, and all connections to its IP:port pair will be load balanced between the service’s endpoints.&lt;/p&gt;

&lt;h4 id=&quot;creating-an-alias-for-an-external-service&quot;&gt;Creating an alias for an external service&lt;/h4&gt;

&lt;p&gt;Instead of exposing an external service by manually configuring the service’s End-
points, a simpler method allows you to refer to an external service by its fully qualified domain name (FQDN).&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef-service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ExternalName&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;externalName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;someapi.somecompany.com&lt;/span&gt; &lt;span class=&quot;c1&quot;&gt;# API Endpoint&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After the service is created, pods can connect to the external service through the cyberchef-service.default.svc.cluster.local domain name (or even external- service) instead of using the service’s actual FQDN. This hides the actual service name and its location from pods consuming the service, allowing you to modify the service definition and point it to a different service any time later, by only changing the externalName attribute or by changing the type back to ClusterIP and creating an Endpoints object for the service—either manually or by specifying a label selector on the service and having it created automatically.&lt;/p&gt;

&lt;h3 id=&quot;136-exposing-services-to-external-clients&quot;&gt;1.3.6 Exposing services to external clients&lt;/h3&gt;

&lt;p&gt;We need apps like web server to the outside, so external can access them.&lt;/p&gt;

&lt;p&gt;You have a few ways to make a service accessible externally:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Setting the service type to &lt;strong&gt;NodePort&lt;/strong&gt;—For a NodePort service, each cluster node
opens a port on the node itself (hence the name) and redirects traffic received
on that port to the underlying service. The service isn’t accessible only at the
internal cluster IP and port, but also through a dedicated port on all nodes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Setting the service type to &lt;strong&gt;LoadBalancer&lt;/strong&gt;, an extension of the NodePort type— This makes the service accessible through a dedicated load balancer, provisioned
from the cloud infrastructure Kubernetes is running on. The load balancer redirects traffic to the node port across all the nodes. Clients connect to the service
through the load balancer’s IP.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Creating an &lt;strong&gt;Ingress&lt;/strong&gt; resource, a radically different mechanism for exposing multiple services through a single IP address— It operates at the HTTP level (network layer 7) and can thus offer more features than layer 4 services.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;1361-nodeport&quot;&gt;1.3.6.1 NodePort&lt;/h4&gt;

&lt;p&gt;Creating a NodePort service, you make Kubernetes reserve a port on all its nodes (the same port number is used across all of them) and forward incoming connections to the pods that are part of the service. This is similar to a regular service (their actual type is ClusterIP), but a NodePort service can be accessed not only through the service’s internal cluster IP, but also through any node’s IP and the reserved node port.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;---cyberchef_nodeport&lt;/span&gt;.yml
apiVersion: v1
kind: Service
metadata:
  name: cyberchef-nodeport
spec:
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: NodePort
  ports:
  - port: 80
    targetPort: 8000
    nodePort: 30123
  selector:
  app: cyberchef

&lt;span class=&quot;nt&quot;&gt;--or--&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Directly using the kubectl&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl expose deployment cyberchef &lt;span class=&quot;nt&quot;&gt;--type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;NodePort &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;80 &lt;span class=&quot;nt&quot;&gt;--targetPort&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;8000

&lt;span class=&quot;c&quot;&gt;# Examining a NodePort service&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get svc cyberchef
NAME             CLUSTER-IP       EXTERNAL-IP    PORT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;        AGE
cyberchef        10.111.254.223   &amp;lt;nodes&amp;gt;        80:30123/TCP   2m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Look at the EXTERNAL-IP column. It shows &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nodes&lt;/code&gt;, indicating the service is accessible
through the IP address of any cluster node. The PORT(S) column shows both the internal port of the cluster IP (80) and the node port (30123).&lt;/p&gt;

&lt;p&gt;When an external client connects to a service through the node port (this also includes cases when it goes through the load balancer first), the randomly chosen pod may or may not be running on the same node that received the connection. An additional network hop is required to reach the pod, but this may not always be desirable. You can prevent this additional hop by configuring the service to redirect external traffic only to pods running on the node that received the connection. This is done by setting the externalTrafficPolicy field in the service’s spec section:&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;externalTrafficPolicy&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Local&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;If a service definition includes this setting and an external connection is opened
through the service’s node port, the service proxy will choose a locally running pod. If no local pods exist, the connection will hang (it won’t be forwarded to a random global pod, the way connections are when not using the annotation). You therefore need to ensure the load balancer forwards connections only to nodes that have at least one such pod.&lt;/p&gt;

&lt;h4 id=&quot;1362-external-loadbalancer&quot;&gt;1.3.6.2 External LoadBalancer&lt;/h4&gt;

&lt;p&gt;Finally, if you have support from the cloud that you are running on (and your cluster is configured to take advantage of it), you can use the LoadBalancer type. This builds on the NodePort type by additionally configuring the cloud to create a new load balancer and direct it at nodes in your cluster. Edit the cyberchef service again (kubectl edit service cyberchef) and change spec.type to LoadBalancer.&lt;/p&gt;

&lt;p&gt;If you do a kubectl get services right away you’ll see that the EXTERNAL-IP column for cyberchef now says &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;pending&lt;/code&gt;. Wait a bit and you should see a public address assigned by your cloud. You can look in the console for your cloud account and see the configuration work that Kubernetes did for you.&lt;/p&gt;

&lt;p&gt;If Kubernetes is running in an environment that doesn’t support LoadBalancer
services, the load balancer will not be provisioned, but the service will still behave like a NodePort service. That’s because a LoadBalancer service is an extension of a NodePort service.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;---cyberchef_loadbalancer&lt;/span&gt;.yml
apiVersion: v1
kind: Service
metadata:
  name: cyberchef
spec:
  &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: LoadBalancer
  ports:
  - port: 80
    targetPort: 8000
    selector:
      app: cyberchef

&lt;span class=&quot;c&quot;&gt;# Connecting through a load balancer&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get svc cyberchef
NAME             CLUSTER-IP       EXTERNAL-IP     PORT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;        AGE
cyberchef        10.111.254.223   210.110.15.4    80:30123/TCP   2m
&lt;span class=&quot;c&quot;&gt;# Access at http://210.110.15.4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;1363--ingress&quot;&gt;1.3.6.3  Ingress&lt;/h4&gt;

&lt;p&gt;One important reason is that each LoadBalancer service requires its own load balancer with its own public IP address, whereas an Ingress only requires one, even when providing access to dozens of services. When a client sends an HTTP request to the Ingress, the host and path in the request determine which service the request is forwarded to Ingresses operate at the application layer of the network stack (HTTP) and can provide features such as cookie-based session affinity and the like, which services can’t.&lt;/p&gt;

&lt;p&gt;To make ingress work, you need to install ingress as per your kubernetes enviornment.&lt;/p&gt;

&lt;div class=&quot;language-yml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Ingress&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef.example.com&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;serviceName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef-nodeport&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;servicePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This defines an Ingress with a single rule, which makes sure all HTTP requests received by the Ingress controller, in which the host cyberchef.example.com is requested, will be sent to the cyberchef-nodeport service on port 80.&lt;/p&gt;

&lt;p&gt;To access your service through &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;http://cyberchef.example.com&lt;/code&gt;, you’ll need to make sure the domain name resolves to the IP of the Ingress controller.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# List Ingress&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get ingresses
NAME       HOSTS                  ADDRESS         PORTS   AGE
cyberchef  cyberchef.example.com  192.168.99.100  80      29m

&lt;span class=&quot;c&quot;&gt;# Note: Make sure you need to add this to your hosts file ( /etc/hosts )&lt;/span&gt;
192.168.99.100  cyberchef.example.com
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;How ingress works??
The client first performed a DNS lookup of cyberchef.example.com, and the DNS server (or the local operating system) returned the IP of the Ingress controller. The client then sent an HTTP request to the Ingress controller and specified cyberchef.example.com in the Host header. From that header, the controller determined which service the client is trying to access, looked up the pod IPs through the Endpoints object associated with the service, and forwarded the client’s request to one of the pods.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Exposing multiple services through the same ingress&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;apps.example.com&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/cyberchef&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;serviceName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;servicePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/linkding&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;serviceName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;linkding&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;servicePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Exposing diff services to diff hosts&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nn&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef.example.com&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;na&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
     &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/&lt;/span&gt;
       &lt;span class=&quot;na&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;serviceName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef&lt;/span&gt;
         &lt;span class=&quot;na&quot;&gt;servicePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;linkding.example.com&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;serviceName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;linkding&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;servicePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;Configuring Ingress to handle TLS traffic ( for HTTPS )&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When a client opens a TLS connection to an Ingress controller, the controller terminates the TLS connection. The communication between the client and the controller is encrypted, whereas the communication between the controller and the backend pod isn’t. The application running in the pod doesn’t need to support TLS. For example, if the pod runs a web server, it can accept only HTTP traffic and let the Ingress controller take care of everything related to TLS. To enable the controller to do that, you need to attach a certificate and a private key to the Ingress. The two need to be stored in a Kubernetes resource called a Secret, which is then referenced in the Ingress manifest.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;openssl genrsa &lt;span class=&quot;nt&quot;&gt;--out&lt;/span&gt; tls.key 2048
openssl req &lt;span class=&quot;nt&quot;&gt;-new&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-x509&lt;/span&gt; tls.key &lt;span class=&quot;nt&quot;&gt;-out&lt;/span&gt; tls.cert &lt;span class=&quot;nt&quot;&gt;-days&lt;/span&gt; 360 &lt;span class=&quot;nt&quot;&gt;-subj&lt;/span&gt; /CN&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef.example.com

kubectl create secret tls tls-secret &lt;span class=&quot;nt&quot;&gt;--cert&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;tls.cert &lt;span class=&quot;nt&quot;&gt;--key&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;tls.key
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The private key and the certificate are now stored in the Secret called tls-secret. Now, you can update your Ingress object so it will also accept HTTPS requests for cyberchef.example.com.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;extensions/v1beta1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Ingress&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;tls&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;hosts&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef.example.com&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;secretName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;tls-secret&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;rules&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;host&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef.example.com&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;http&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;paths&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
      &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;path&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;backend&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;serviceName&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef-nodeport&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;servicePort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Create Ingress&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; cyberchef-ingress-tls.yaml

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;-k&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; https://cyberchef.example.com/cyberchef
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; About to connect&lt;span class=&quot;o&quot;&gt;()&lt;/span&gt; to cyberchef.example.com port 443 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;#0)&lt;/span&gt;
...
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; Server certificate:
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
 subject: &lt;span class=&quot;nv&quot;&gt;CN&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef.example.com
...
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; GET /cyberchef HTTP/1.1
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;137-signaling-when-a-pod-is-ready-to-accept-connections&quot;&gt;1.3.7 Signaling when a pod is ready to accept connections&lt;/h3&gt;

&lt;p&gt;Consider a scenario where a new pod started with proper labels is created, it becomes part of the service and requests start to be redirected to the pod. But what if the pod isn’t ready to start serving requests immediately?&lt;/p&gt;

&lt;p&gt;The pod may need time to load either configuration or data, or it may need to perform a warm-up procedure to prevent the first user request from taking too long and affecting the user experience. In such cases you don’t want the pod to start receiving requests immediately, especially when the already-running instances can process requests properly and quickly. It makes sense to not forward requests to a pod that’s in the process of starting up until it’s fully ready.&lt;/p&gt;

&lt;h4 id=&quot;readiness-probes&quot;&gt;Readiness probes&lt;/h4&gt;

&lt;p&gt;The readiness probe is invoked periodically and determines whether the specific pod should receive client requests or not. When a container’s readiness probe returns success, it’s signaling that the container is ready to accept requests.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;How readiness probes works
When a container is started, Kubernetes can be configured to wait for a configurable amount of time to pass before performing the first readiness check. After that, it invokes the probe periodically and acts based on the result of the readiness probe. If a pod reports that it’s not ready,&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;TYPES OF READINESS PROBES
    &lt;ul&gt;
      &lt;li&gt;An Exec probe, where a process is executed. The container’s status is determined by the process’ exit status code.&lt;/li&gt;
      &lt;li&gt;An HTTP GET probe, which sends an HTTP GET request to the container and the HTTP status code of the response determines whether the container is ready or not.&lt;/li&gt;
      &lt;li&gt;A TCP Socket probe, which opens a TCP connection to a specified port of the
container. If the connection is established, the container is considered ready.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Difference between liveness probe and rediness probe
Liveness probes keep pods healthy by killing off unhealthy containers and replacing
them with new, healthy ones, whereas readiness probes make sure that only pods that
are ready to serve requests receive them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Why we need readiness probe
Imagine that a group of pods (for example, pods running application servers) depends on a service provided by another pod (a backend database, for example). If at any point one of the frontend pods experiences connectivity problems and can’t reach the database anymore, it may be wise for its readiness probe to signal to Kubernetes that the pod isn’t ready to serve any requests at that time. If other pod instances aren’t experiencing the same type of connectivity issues, they can serve requests normally. A readiness probe makes sure clients only talk to those healthy pods and never notice there’s anything wrong with the system.&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ReplicationController&lt;/span&gt;
&lt;span class=&quot;nn&quot;&gt;...&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
  &lt;span class=&quot;s&quot;&gt;template&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;s&quot;&gt;...&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;containers&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;mpepping/cyberchef&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;readinessProbe&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;na&quot;&gt;exec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;na&quot;&gt;command&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;ls&lt;/span&gt;
          &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;/var/ready&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The readiness probe will periodically perform the command ls /var/ready inside the container. The ls command returns exit code zero if the file exists, or a non-zero exit code otherwise. If the file exists, the readiness probe will succeed; otherwise, it will fail. Note: To check status check logs.&lt;/p&gt;

&lt;p&gt;This mock readiness probe is useful only for demonstrating what readiness probes do.
In the real world, the readiness probe should return success or failure depending on whether the app can (and wants to) receive client requests or not. Manually removing pods from services should be performed by either deleting the pod or changing the pod’s labels instead of manually flipping a switch in the probe.&lt;/p&gt;

&lt;h3 id=&quot;138-using-a-headless-service-for-discovering-individual-pods&quot;&gt;1.3.8 Using a headless service for discovering individual pods&lt;/h3&gt;

&lt;p&gt;If you want to connect to all of the pods. you need a IP of an individual pod, which is can’t possible in default configuration in kubernetes.  One option is to have the client call the Kubernetes API server and get the list of pods and their IP addresses through an API call, but because you should always strive to keep your apps Kubernetes-agnostic, using the API server isn’t ideal.&lt;/p&gt;

&lt;p&gt;Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually, when you perform a DNS lookup for a service, the DNS server returns a single IP—the service’s cluster IP. But if you tell Kubernetes you don’t need a cluster IP for your service (you do this by setting the clusterIP field to None in the service specification ), the DNS server will return the pod IPs instead of the single service IP.&lt;/p&gt;

&lt;p&gt;Instead of returning a single DNS A record, the DNS server will return multiple A records for the service, each pointing to the IP of an individual pod backing the ser- vice at that moment. Clients can therefore do a simple DNS A record lookup and get the IPs of all the pods that are part of the service. The client can then use that information to connect to one, many, or all of them.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;# All you need to do is to set ClusterIP to None&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;apiVersion&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;v1&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;kind&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;Service&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;metadata&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef-headless&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;spec&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;clusterIP&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;None&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;ports&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;port&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;80&lt;/span&gt;
    &lt;span class=&quot;na&quot;&gt;targetPort&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;8000&lt;/span&gt;
&lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Setup and dns lookup container ( We need nslookup and dif : tutum/dnsutils )&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl run dnsutils &lt;span class=&quot;nt&quot;&gt;--image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;tutum/dnsutils &lt;span class=&quot;nt&quot;&gt;--generator&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;run-pod/v1 &lt;span class=&quot;nt&quot;&gt;--command&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sleep &lt;/span&gt;infinity

&lt;span class=&quot;c&quot;&gt;# Perform DNSLookup for headless&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;dnsutils nslookup cyberchef-headless
...
Name: cyberchef-headless.default.svc.cluster.local
Address: 10.108.1.4

Name: cyberchef-headless.default.svc.cluster.local
Address: 10.108.2.5 &lt;span class=&quot;c&quot;&gt;# Pod IP&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Perform DNSLookup for non-headless&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;dnsutils nslookup cyberchef
...
Name: cyberchef.default.svc.cluster.local
Address: 10.111.249.153 &lt;span class=&quot;c&quot;&gt;## ClusterIP&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;139-troubleshooting-best-practices&quot;&gt;1.3.9 Troubleshooting Best Practices&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;First, make sure you’re connecting to the service’s cluster IP from within the cluster, not from the outside.&lt;/li&gt;
  &lt;li&gt;Don’t bother pinging the service IP to figure out if the service is accessible (remember, the service’s cluster IP is a virtual IP and pinging it will never work).&lt;/li&gt;
  &lt;li&gt;If you’ve defined a readiness probe, make sure it’s succeeding; otherwise the pod won’t be part of the service.&lt;/li&gt;
  &lt;li&gt;To confirm that a pod is part of the service, examine the corresponding Endpoints object with kubectl get endpoints.&lt;/li&gt;
  &lt;li&gt;If you’re trying to access the service through its FQDN or a part of it (for example, myservice.mynamespace.svc.cluster.local or myservice.mynamespace) and it doesn’t work, see if you can access it using its cluster IP instead of the FQDN.&lt;/li&gt;
  &lt;li&gt;Check whether you’re connecting to the port exposed by the service and not the target port.&lt;/li&gt;
  &lt;li&gt;Try connecting to the pod IP directly to confirm your pod is accepting connec- tions on the correct port.&lt;/li&gt;
  &lt;li&gt;If you can’t even access your app through the pod’s IP, make sure your app isn’t only binding to localhost.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Kubernetes" /><summary type="html"></summary></entry><entry><title type="html">Kubernetes Controllers</title><link href="https://hacstac.github.io/Notes/kubernetes/2020/09/27/Kubernetes-Controllers.html" rel="alternate" type="text/html" title="Kubernetes Controllers" /><published>2020-09-27T00:00:00-05:00</published><updated>2020-09-27T00:00:00-05:00</updated><id>https://hacstac.github.io/Notes/kubernetes/2020/09/27/Kubernetes-Controllers</id><content type="html" xml:base="https://hacstac.github.io/Notes/kubernetes/2020/09/27/Kubernetes-Controllers.html">&lt;hr /&gt;

&lt;h2 id=&quot;10-replication-controller--deprecated-&quot;&gt;1.0 Replication Controller [ Deprecated ]&lt;/h2&gt;

&lt;p&gt;A ReplicationController is a Kubernetes resource that ensures its pods are always kept running. If the pod disappears for any reason, such as in the event of a node disappearing from the cluster or because the pod was evicted from the node, the ReplicationController notices the missing pod and creates a replacement pod.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Kubernetes/ReplicationController.png&quot; alt=&quot;Replication Controller&quot; /&gt;&lt;/p&gt;

&lt;p&gt;In the figure we saw that if a node goes down and takes two pods with it. Pod A was created directly and is therefore an unmanaged pod, while pod B is managed by a ReplicationController. After the node fails, the ReplicationController creates a
new pod (pod B2) to replace the missing pod B, whereas pod A is lost completely—
nothing will ever recreate it.&lt;/p&gt;

&lt;h3 id=&quot;11-the-opration-of-a-replication-controller&quot;&gt;1.1 The opration of a Replication Controller&lt;/h3&gt;

&lt;p&gt;A ReplicationController constantly monitors the list of running pods and makes sure the actual number of pods of a “type” always matches the desired number. If too few such pods are running, it creates new replicas from a pod template. If too many such pods are running, it removes the excess replicas.&lt;/p&gt;

&lt;h3 id=&quot;12-three-main-parts-of-replication-controller&quot;&gt;1.2 Three main parts of Replication Controller&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A label selector, which determines what pods are in the ReplicationController’s scope&lt;/li&gt;
  &lt;li&gt;A replica count, which specifies the desired number of pods that should be running&lt;/li&gt;
  &lt;li&gt;A pod template, which is used when creating new pod replicas&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;13-controller-reconciliation-loop&quot;&gt;1.3 Controller Reconciliation Loop&lt;/h3&gt;

&lt;p&gt;A ReplicationController’s job is to make sure that an exact number of pods always
matches its label selector. If it doesn’t, the ReplicationController takes the appropriate action to reconcile the actual with the desired number.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Kubernetes/ReplicationController_Reconciliation.png&quot; alt=&quot;Reconciliation&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;14-effect-of-changing-the-controllers-label-selector-or-pod-template&quot;&gt;1.4 Effect of changing the controller’s label selector or pod template&lt;/h3&gt;

&lt;p&gt;Changes to the label selector and the pod template have no effect on existing pods. Changing the label selector makes the existing pods fall out of the scope of the ReplicationController, so the controller stops caring about them. ReplicationCon- trollers also don’t care about the actual “contents” of its pods (the container images, environment variables, and other things) after they create the pod. The template therefore only affects new pods created by this ReplicationController.&lt;/p&gt;

&lt;h3 id=&quot;15-benefits-of-replication-controller&quot;&gt;1.5 Benefits of Replication Controller&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;It makes sure a pod (or multiple pod replicas) is always running by starting a
new pod when an existing one goes missing.&lt;/li&gt;
  &lt;li&gt;When a cluster node fails, it creates replacement replicas for all the pods that
were running on the failed node (those that were under the Replication-
Controller’s control).&lt;/li&gt;
  &lt;li&gt;It enables easy horizontal scaling of pods—both manual and automatic&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;16-lets-get-into-the-replication-controller&quot;&gt;1.6 Lets get into the Replication Controller&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Create a YAML file to create a Replication Controller&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;---cyberchef_rc&lt;/span&gt;.yml
apiVersion: v1
kind: ReplicationController
metadata:
  name: cyberchef
spec:
  replicas: 3
  selector:
    app: cyberchef
  template:
    metadata:
      labels:
        app: cyberchef
    spec:
      containers:
        - name: cyberchef
          image: mpepping/cyberchef
          ports:
            - containerPort: 8000

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl create &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./cyberchef_rc.yml

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Get Pods&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods
NAME              READY   STATUS    RESTARTS   AGE
cyberchef-gzg6n   1/1     Running   0          5m11s
cyberchef-h4crn   1/1     Running   0          5m11s
cyberchef-vlp7f   1/1     Running   0          5m11s

&lt;span class=&quot;c&quot;&gt;# Trying to delete the pod&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete pod cyberchef-vlp7f

&lt;span class=&quot;c&quot;&gt;# Replication Controller Immediately spins up a new pod&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods
NAME              READY   STATUS    RESTARTS   AGE
cyberchef-gzg6n   1/1     Running   0          6m30s
cyberchef-h4crn   1/1     Running   0          6m30s
cyberchef-hs9cp   1/1     Running   0          9s

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Getting details of replication controller&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get rc
NAME        DESIRED   CURRENT   READY   AGE
cyberchef   3         3         3       8m16s

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Getting additional details about rc&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl describe rc
Name:         cyberchef
Namespace:    default
Selector:     &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef
Labels:       &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef
Annotations:  &amp;lt;none&amp;gt;
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef
  Containers:
   cyberchef:
    Image:        mpepping/cyberchef
    Port:         8000/TCP
    Host Port:    0/TCP
    Environment:  &amp;lt;none&amp;gt;
    Mounts:       &amp;lt;none&amp;gt;
  Volumes:        &amp;lt;none&amp;gt;
Events:
  Type    Reason            Age    From                    Message
  &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;    &lt;span class=&quot;nt&quot;&gt;------&lt;/span&gt;            &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;   &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;                    &lt;span class=&quot;nt&quot;&gt;-------&lt;/span&gt;
  Normal  SuccessfulCreate  9m33s  replication-controller  Created pod: cyberchef-vlp7f
  Normal  SuccessfulCreate  9m33s  replication-controller  Created pod: cyberchef-h4crn
  Normal  SuccessfulCreate  9m33s  replication-controller  Created pod: cyberchef-gzg6n
  Normal  SuccessfulCreate  3m12s  replication-controller  Created pod: cyberchef-hs9cp
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;17-moving-pods-in-and-out-of-the-scope-of-a-replicationcontroller&quot;&gt;1.7 Moving pods in and out of the scope of a ReplicationController&lt;/h3&gt;

&lt;p&gt;Pods created by a ReplicationController aren’t tied to the ReplicationController in any way. At any moment, a ReplicationController manages pods that match its label selector. By changing a pod’s labels, it can be removed from or added to the scope of a ReplicationController. It can even be moved from one ReplicationController to another.&lt;/p&gt;

&lt;p&gt;If you change a pod’s labels so they no longer match a ReplicationController’s label selector, the pod becomes like any other manually created pod. It’s no longer managed by anything. If the node running the pod fails, the pod is obviously not rescheduled. But keep in mind that when you changed the pod’s labels, the replication controller noticed one pod was missing and spun up a new pod to replace it.&lt;/p&gt;

&lt;p&gt;There in below example, we now have four pods altogether: one that isn’t managed by our ReplicationController and three that are. Among them is the newly created pod. After we change the pod’s label from app=cyberchef to app=testing, the ReplicationController no longer cares about the pod. Because the controller’s replica count is set to 3 and only two pods match the label selector so replication controller will spin a new pod and this changed lebal pod is keep running but no one can manage this pod, so we can test this and perform operation on this.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods
NAME              READY   STATUS    RESTARTS   AGE
cyberchef-gbrgh   1/1     Running   0          12h
cyberchef-h728j   1/1     Running   0          12h
cyberchef-npql7   1/1     Running   0          12h

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl label pod cyberchef-h728j &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;vulnerable

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods &lt;span class=&quot;nt&quot;&gt;--show-labels&lt;/span&gt;
NAME              READY   STATUS    RESTARTS   AGE   LABELS
cyberchef-gbrgh   1/1     Running   0          12h   &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef
cyberchef-h728j   1/1     Running   0          12h   &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef,type&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;vulnerable
cyberchef-npql7   1/1     Running   0          12h   &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl label pod cyberchef-h728j &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;testing &lt;span class=&quot;nt&quot;&gt;--overwrite&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; app

NAME              READY   STATUS    RESTARTS   AGE     APP
cyberchef-gbrgh   1/1     Running   0          13h     cyberchef
cyberchef-h728j   1/1     Running   0          13h     testing
cyberchef-kwn7r   1/1     Running   0          2m44s   cyberchef
cyberchef-npql7   1/1     Running   0          13h     cyberchef

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get rc
NAME        DESIRED   CURRENT   READY   AGE
cyberchef   3         3         3       13h
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;18-changing-pod-template&quot;&gt;1.8 Changing pod template&lt;/h3&gt;

&lt;p&gt;A ReplicationController’s pod template can be modified at any time. Changing the pod
template is like replacing a cookie cutter with another one. It will only affect the cookies you cut out afterward and will have no effect on the ones you’ve already cut. To modify the old pods, you’d need to delete them and let the ReplicationController replace them with new ones based on the new template.&lt;/p&gt;

&lt;p&gt;Editing a ReplicationController like this to change the container image in the pod
template, deleting the existing pods, and letting them be replaced with new ones from
the new template could be used for upgrading pods&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# This will open the ReplicationController’s YAML definition in your default text editor.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Find the pod template section and add an additional label to the metadata&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#  After you save your changes and exit the editor, kubectl will update the ReplicationController and print the following message:  replicationcontroller/cyberchef edited&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl edit rc cyberchef
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;19-horizontally-scaling-pods&quot;&gt;1.9 Horizontally scaling pods&lt;/h3&gt;

&lt;p&gt;Scaling the number of pods up or down is as easy as changing the value of the rep- licas field in the ReplicationController resource. After the change, the Replication- Controller will either see too many pods exist (when scaling down) and delete part of them, or see too few of them (when scaling up) and create additional pods.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# By kubectl command&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl scale rc cyberchef &lt;span class=&quot;nt&quot;&gt;--replicas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;4

&lt;span class=&quot;c&quot;&gt;# By changing Yaml&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubeclt edit rc cyberchef
&lt;span class=&quot;c&quot;&gt;# Edit replicas section&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get rc
NAME        DESIRED   CURRENT   READY   AGE
cyberchef   4         4         4       2m
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;110-deleting-a-replication-controller&quot;&gt;1.10 Deleting a Replication Controller&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# This will delete rc as well all the pods&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete rc cyberchef

&lt;span class=&quot;c&quot;&gt;# If you want to keep running your pods and delete the replication controller&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete rc cyberchef &lt;span class=&quot;nt&quot;&gt;--cascade&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Now pods are running their own&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;20-replicasets&quot;&gt;2.0 ReplicaSets&lt;/h2&gt;

&lt;p&gt;A ReplicaSet acts as a cluster-wide Pod manager, ensuring that the right types and number of Pods are running at all times.&lt;/p&gt;

&lt;p&gt;Initially, ReplicationControllers were the only Kubernetes component for replicating
pods and rescheduling them when nodes failed. Later, a similar resource called a
ReplicaSet was introduced. It’s a new generation of ReplicationController and
replaces it completely (ReplicationControllers will eventually be deprecated). ReplicaSet also use same policy as replicationController, ReplicaSets create and manage Pods, they do not own the Pods they create. ReplicaSets use label queries to identify the set of Pods they should be managing.&lt;/p&gt;

&lt;p&gt;If we going in deep detail, Despite the value placed on declarative configuration of software, there are times when it is easier to build something up imperatively. In particular, early on you may be simply deploying a single Pod with a container image without a ReplicaSet manag‐ ing it. But at some point you may want to expand your singleton container into a replicated service and create and manage an array of similar containers. You may have even defined a load balancer that is serving traffic to that single Pod. If ReplicaSets owned the Pods they created, then the only way to start replicating your Pod would be to delete it and then relaunch it via a ReplicaSet. This might be disruptive, as there would be a moment in time when there would be no copies of your container running. However, because ReplicaSets are decoupled from the Pods they manage, you can simply create a ReplicaSet that will “adopt” the existing Pod, and scale out additional copies of those containers. In this way, you can seamlessly move from a single imperative Pod to a replicated set of Pods managed by a ReplicaSet.&lt;/p&gt;

&lt;h3 id=&quot;21-comparing-a-replicaset-to-a-replicationcontroller&quot;&gt;2.1 Comparing a ReplicaSet to a ReplicationController&lt;/h3&gt;

&lt;p&gt;A ReplicaSet behaves exactly like a ReplicationController, but it has more expressive pod selectors. Whereas a ReplicationController’s label selector only allows matching pods that include a certain label, a ReplicaSet’s selector also allows matching pods that lack a certain label or pods that include a certain label key, regardless of its value. Also, for example, a single ReplicationController can’t match pods with the label env=production and those with the label env=devel at the same time. It can only match either pods with the env=production label or pods with the env=devel label. But a sin- gle ReplicaSet can match both sets of pods and treat them as a single group. Similarly, a ReplicationController can’t match pods based merely on the presence of a label key, regardless of its value, whereas a ReplicaSet can. For example, a Replica- Set can match all pods that include a label with the key env, whatever its actual value is (you can think of it as env=*).&lt;/p&gt;

&lt;h3 id=&quot;22-reconciliation-loops&quot;&gt;2.2 Reconciliation Loops&lt;/h3&gt;

&lt;p&gt;The central concept behind a reconciliation loop is the notion of desired state versus observed or current state. Desired state is the state you want. With a ReplicaSet, it is the desired number of replicas and the definition of the Pod to replicate. For example, “the desired state is that there are three replicas of a Pod running the cyberchef”.&lt;/p&gt;

&lt;p&gt;In contrast, the current state is the currently observed state of the system. For example, “there are only two cyberchef Pods currently running.” The reconciliation loop is constantly running, observing the current state of the world and taking action to try to make the observed state match the desired state. For instance, with the previous examples, the reconciliation loop would create a new cyberchef Pod in an effort to make the observed state match the desired state of three replicas.&lt;/p&gt;

&lt;p&gt;There are many benefits to the reconciliation loop approach to managing state. It is
an inherently goal-driven, self-healing system, yet it can often be easily expressed in a few lines of code. As a concrete example of this, note that the reconciliation loop for ReplicaSets is a single loop, yet it handles user actions to scale up or scale down the ReplicaSet as well as node failures or nodes rejoining the cluster after being absent.&lt;/p&gt;

&lt;h3 id=&quot;23-quarantining-containers&quot;&gt;2.3 Quarantining Containers&lt;/h3&gt;

&lt;p&gt;Oftentimes, when a server misbehaves, Pod-level health checks will automatically restart that Pod. But if your health checks are incomplete, a Pod can be misbehaving but still be part of the replicated set. In these situations, while it would work to simply kill the Pod, that would leave your developers with only logs to debug the problem. Instead, you can modify the set of labels on the sick Pod. Doing so will disassociate it from the ReplicaSet (and service) so that you can debug the Pod. The ReplicaSet con‐ troller will notice that a Pod is missing and create a new copy, but because the Pod is still running it is available to developers for interactive debugging, which is significantly more valuable than debugging from logs.&lt;/p&gt;

&lt;h3 id=&quot;24-deepdive-with-replicaset&quot;&gt;2.4 DeepDive with ReplicaSet&lt;/h3&gt;

&lt;p&gt;You’re creating a resource of type ReplicaSet which has much the same contents as the ReplicationController you created earlier. The only difference is in the selector. Instead of listing labels the pods need to have directly under the selector property, you’re specifying them under selector .matchLabels. This is the simpler (and less expressive) way of defining label selectors in a ReplicaSet.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Creating a YAML for ReplicaSet:&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---cyberchef_rs&lt;/span&gt;.yml
apiVersion: apps/v1
kind: ReplicaSet
metadata:
  name: cyberchef
  labels:
    app: cyberchef
spec:
  replicas: 3
  selector:
    matchLabels:
      app: cyberchef
  template:
    metadata:
      labels:
        app: cyberchef
    spec:
      containers:
        - name: cyberchef
          image: mpepping/cyberchef:testing
          ports:
            - containerPort: 8000

&lt;span class=&quot;c&quot;&gt;# To submit the cyberChef replicaSet to the kubernetes API:&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./cyberchef_rs.yml

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get rs
NAME        DESIRED   CURRENT   READY   AGE
cyberchef   3         3         3       40m

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# To get further details:&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl describe rs
Name:         cyberchef
Namespace:    default
Selector:     &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef
Labels:       &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef
Annotations:  &amp;lt;none&amp;gt;
Replicas:     3 current / 3 desired
Pods Status:  3 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef
  Containers:
   cyberchef:
    Image:        mpepping/cyberchef:testing
    Port:         8000/TCP
    Host Port:    0/TCP
    Environment:  &amp;lt;none&amp;gt;
    Mounts:       &amp;lt;none&amp;gt;
  Volumes:        &amp;lt;none&amp;gt;
Events:
  Type    Reason            Age   From                   Message
  &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;    &lt;span class=&quot;nt&quot;&gt;------&lt;/span&gt;            &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;  &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;                   &lt;span class=&quot;nt&quot;&gt;-------&lt;/span&gt;
  Normal  SuccessfulCreate  41m   replicaset-controller  Created pod: cyberchef-l68b2
  Normal  SuccessfulCreate  41m   replicaset-controller  Created pod: cyberchef-8hg5p
  Normal  SuccessfulCreate  41m   replicaset-controller  Created pod: cyberchef-qfqrk

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Finding a replicaSet from a pod&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Method1:  Check for annotation : kubernetes.io/created-by&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods &amp;lt;pod-name&amp;gt; &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; yaml

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Method2: Check for the kind&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods cyberchef-qm28m &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; yaml | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;kind
kind: ReplicaSet

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Finding a Set of pods for a replicaSet&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# use the selector to filter out the pods that a replicaSet is using&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef
NAME              READY   STATUS    RESTARTS   AGE
cyberchef-qm28m   1/1     Running   0          15m
cyberchef-r6fff   1/1     Running   0          15m
cyberchef-scjlr   1/1     Running   0          15m

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Scaling ReplicaSets&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ReplicaSets are scaled up or down by updating the spec.replicas key on the ReplicaSet object stored in Kubernetes.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Easiest way to achieve this using scale command&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl scale replicasets cyberchef &lt;span class=&quot;nt&quot;&gt;--replicas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;4
&lt;span class=&quot;c&quot;&gt;# Note : change ReplicaSet configuration file count as best practices, so use this imperative scale option only on emergency sitautions otherwise update the replicas from the files.&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Autoscaling a ReplicaSet&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# If we understand this with a scenario, if the application uses the high cpu and memory kubernetes will automatically scale the application.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This scaling is based on responses of some custom application metrics.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Kubernetes can handle all of these scenarios via Horizontal Pod Autoscaling (HPA).&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## Autoscaling based on cpu&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl autoscale rs cyberchef &lt;span class=&quot;nt&quot;&gt;--min&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2 &lt;span class=&quot;nt&quot;&gt;--max&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5 &lt;span class=&quot;nt&quot;&gt;--cpu-percent&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;80
&lt;span class=&quot;c&quot;&gt;# This command creates an autoscaler that scales between two and five replicas with a CPU threshold of 80%.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# To view, modify, or delete this resource you can use the standard kubectl commands and the horizontalpodautoscalers resource.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# horizontalpodautoscalers is quite a bit to type, but it can be shortened to hpa:&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get hpa

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# To Delete ReplicaSets&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete rs cyberchef

&lt;span class=&quot;c&quot;&gt;# If you don’t want to delete the Pods that are being managed by the ReplicaSet, you can set the --cascade flag to false to ensure only the ReplicaSet object is deleted and not the Pods:&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete rs kuard &lt;span class=&quot;nt&quot;&gt;--cascade&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;25-using-the-replicaset-more-expressive-label-selectors&quot;&gt;2.5 Using the ReplicaSet more expressive label selectors&lt;/h3&gt;

&lt;p&gt;The main improvements of ReplicaSets over ReplicationControllers are their more expressive label selectors. You intentionally used the simpler matchLabels selector in the first ReplicaSet example to see that ReplicaSets are no different from ReplicationControllers. Now, you’ll rewrite the selector to use the more powerful matchExpressions property.&lt;/p&gt;

&lt;div class=&quot;language-yaml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;na&quot;&gt;selector&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
  &lt;span class=&quot;na&quot;&gt;matchExpressions&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
    &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;na&quot;&gt;key&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;app&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;operator&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;In&lt;/span&gt;
      &lt;span class=&quot;na&quot;&gt;values&lt;/span&gt;&lt;span class=&quot;pi&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;pi&quot;&gt;-&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;cyberchef&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can add additional expressions to the selector. As in the example, each expression
must contain a key, an operator, and possibly (depending on the operator) a list of
values. You’ll see four valid operators:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In—Label’s value must match one of the specified values.&lt;/li&gt;
  &lt;li&gt;NotIn—Label’s value must not match any of the specified values.&lt;/li&gt;
  &lt;li&gt;Exists—Pod must include a label with the specified key (the value isn’t important). When using this operator, you shouldn’t specify the values field.&lt;/li&gt;
  &lt;li&gt;DoesNotExist—Pod must not include a label with the specified key. The values
property must not be specified.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If you specify multiple expressions, all those expressions must evaluate to true for the selector to match a pod. If you specify both matchLabels and matchExpressions, all the labels must match and all the expressions must evaluate to true for the pod to match the selector.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;30-daemonsets&quot;&gt;3.0 DaemonSets&lt;/h2&gt;

&lt;p&gt;DaemonSet:  Running exactly one pod on each node&lt;/p&gt;

&lt;p&gt;To run a pod on all cluster nodes, you create a DaemonSet object, which is much like a ReplicationController or a ReplicaSet, except that pods created by a DaemonSet already have a target node specified and skip the Kubernetes Scheduler. Whereas a ReplicaSet (or ReplicationController) makes sure that a desired number of pod replicas exist in the cluster, a DaemonSet doesn’t have any notion of a desired replica count. It doesn’t need it because its job is to ensure that a pod matching its pod selector is running on each node.&lt;/p&gt;

&lt;p&gt;A DaemonSet ensures a copy of a Pod is running across a set of nodes in a Kuber‐ netes cluster. DaemonSets are used to deploy system daemons such as log collectors and monitoring agents, which typically must run on every node. DaemonSets share similar functionality with ReplicaSets; both create Pods that are expected to be long-running services and ensure that the desired state and the observed state of the cluster match.&lt;/p&gt;

&lt;p&gt;If a node goes down, the DaemonSet doesn’t cause the pod to be created else-
where. But when a new node is added to the cluster, the DaemonSet immediately
deploys a new pod instance to it. It also does the same if someone inadvertently
deletes one of the pods, leaving the node without the DaemonSet’s pod. Like a Replica-
Set, a DaemonSet creates the pod from the pod template configured in it.&lt;/p&gt;

&lt;p&gt;You can use labels to run DaemonSet Pods on specific nodes; for example, you may want to run specialized intrusion-detection software on nodes that are exposed to the edge network.&lt;/p&gt;

&lt;h3 id=&quot;31-daemonset-scheduler&quot;&gt;3.1 DaemonSet Scheduler&lt;/h3&gt;

&lt;p&gt;By default a DaemonSet will create a copy of a Pod on every node unless a node selec‐
tor is used, which will limit eligible nodes to those with a matching set of labels. DaemonSets determine which node a Pod will run on at Pod creation time by specifying the nodeName field in the Pod spec. As a result, Pods created by DaemonSets are ignored by the Kubernetes scheduler.&lt;/p&gt;

&lt;p&gt;Like ReplicaSets, DaemonSets are managed by a reconciliation control loop that measures the desired state (a Pod is present on all nodes) with the observed state (is the Pod present on a particular node?). Given this information, the DaemonSet controller creates a Pod on each node that doesn’t currently have a matching Pod.&lt;/p&gt;

&lt;p&gt;If a new node is added to the cluster, then the DaemonSet controller notices that it is missing a Pod and adds the Pod to the new node.&lt;/p&gt;

&lt;h3 id=&quot;32-deepdive-with-daemonset&quot;&gt;3.2 DeepDive with DaemonSet&lt;/h3&gt;

&lt;p&gt;DaemonSets are created by submitting a DaemonSet configuration to the Kubernetes API server.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# we can only run these pods on selected labels, like if some nodes have ssd's and we want to run those pods only on ssd based nodes.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Like in below example we use node selector to choose only those pods that have ssd's.&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;--cyberchef_daemonSet&lt;/span&gt;.yml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: cyberchef
  labels:
    app: cyberchef
spec:
  selector:
    matchLabels:
      app: cyberchef
  template:
    metadata:
      labels:
        app: cyberchef
    spec:
      nodeSelector:
        disk: ssd
      containers:
        - name: cyberchef
          image: mpepping/cyberchef
          ports:
            - containerPort: 8000

&lt;span class=&quot;c&quot;&gt;# To Supply to Kubernetes API&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./cyberchef_daemonSet.yml

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Get the DaemonSet&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get ds
kubectl get daemonset
NAME        DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
cyberchef   2         2         0       2            0           &amp;lt;none&amp;gt;          4s

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Get the pods&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods
NAME              READY   STATUS    RESTARTS   AGE
cyberchef-dnrx9   1/1     Running   0          53s
cyberchef-gfhcl   1/1     Running   0          53s

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Get further details&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl describe daemonset cyberchef
Name:           cyberchef
Selector:       &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef
Node-Selector:  &amp;lt;none&amp;gt;
Labels:         &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef
Annotations:    deprecated.daemonset.template.generation: 1
Desired Number of Nodes Scheduled: 2
Current Number of Nodes Scheduled: 2
Number of Nodes Scheduled with Up-to-date Pods: 2
Number of Nodes Scheduled with Available Pods: 2
Number of Nodes Misscheduled: 0
Pods Status:  2 Running / 0 Waiting / 0 Succeeded / 0 Failed
Pod Template:
  Labels:  &lt;span class=&quot;nv&quot;&gt;app&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cyberchef
  Containers:
   cyberchef:
    Image:        mpepping/cyberchef
    Port:         8000/TCP
    Host Port:    0/TCP
    Environment:  &amp;lt;none&amp;gt;
    Mounts:       &amp;lt;none&amp;gt;
  Volumes:        &amp;lt;none&amp;gt;
Events:
  Type    Reason            Age   From                  Message
  &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;    &lt;span class=&quot;nt&quot;&gt;------&lt;/span&gt;            &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;  &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;                  &lt;span class=&quot;nt&quot;&gt;-------&lt;/span&gt;
  Normal  SuccessfulCreate  111s  daemonset-controller  Created pod: cyberchef-gfhcl
  Normal  SuccessfulCreate  111s  daemonset-controller  Created pod: cyberchef-dnrx9


&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Get the nodes&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get nodes
NAME           STATUS   ROLES    AGE     VERSION
minikube       Ready    master   2d17h   v1.19.2
minikube-m02   Ready    &amp;lt;none&amp;gt;   2d16h   v1.19.2
minikube-m03   Ready    &amp;lt;none&amp;gt;   5m      v1.19.2

&lt;span class=&quot;c&quot;&gt;# To check pod running on which node&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; wide
NAME              READY   STATUS    RESTARTS   AGE    IP           NODE           NOMINATED NODE   READINESS GATES
cyberchef-dnrx9   1/1     Running   0          4m5s   172.17.0.2   minikube-m02   &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;
cyberchef-gfhcl   1/1     Running   0          4m5s   172.17.0.5   minikube       &amp;lt;none&amp;gt;           &amp;lt;none&amp;gt;

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# To limiting daemonSets to specific nodes&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Change the labels of the node&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl label node minikube-m03 &lt;span class=&quot;nv&quot;&gt;disk&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;hdd &lt;span class=&quot;nt&quot;&gt;--overwrite&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Show Labels&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get nodes &lt;span class=&quot;nt&quot;&gt;--show-labels&lt;/span&gt;
NAME           STATUS   ROLES    AGE    VERSION   LABELS
minikube       Ready    master   2d17h   v1.19.2  beta.kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,beta.kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,disk&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ssd ...
minikube-m03   Ready    master   2d17h   v1.19.2   beta.kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,beta.kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,disk&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;hdd ...
minikube-m02   Ready    master   2d17h   v1.19.2   beta.kubernetes.io/arch&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;amd64,beta.kubernetes.io/os&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;linux,disk&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ssd ...
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;33-limiting-daemonsets-to-specific-nodes&quot;&gt;3.3 Limiting DaemonSets to Specific Nodes&lt;/h3&gt;

&lt;p&gt;The most common use case for DaemonSets is to run a Pod across every node in a Kubernetes cluster. However, there are some cases where you want to deploy a Pod to only a subset of nodes. For example, maybe you have a workload that requires a GPU or access to fast storage only available on a subset of nodes in your cluster. In cases like these, node labels can be used to tag specific nodes that meet workload requirements.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Adding Lables to Nodes&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;DaemonSets to specific nodes is to add the desired set of labels to a subset of nodes. This can be achieved using the kubectl label command.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;kubectl label nodes minikube &lt;span class=&quot;nv&quot;&gt;ssd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true

&lt;/span&gt;kubectl get nodes &lt;span class=&quot;nt&quot;&gt;--selector&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;ssd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;true
&lt;/span&gt;NAME           STATUS   AGE     VERSION
minikube       Ready    2d17h   v1.19.2
minikube-m02   Ready    2d17h   v1.19.2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;ul&gt;
  &lt;li&gt;NodeSelector&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Node selectors can be used to limit what nodes a Pod can run on in a given Kuber‐
netes cluster. Node selectors are defined as part of the Pod spec when creating a DaemonSet.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# I specified this on previous cyberchef daemon example&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---x&lt;/span&gt;.yml
apiVersion:
kind:
metadata:
spec:
  template:
    metadata:
    spec:
      nodeSelector:
        ssd: &lt;span class=&quot;s2&quot;&gt;&quot;true&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;34-updating-a-daemonset&quot;&gt;3.4 Updating a DaemonSet&lt;/h3&gt;

&lt;p&gt;DaemonSets are great for deploying services across an entire cluster, but what about
upgrades? Prior to Kubernetes 1.6, the only way to update Pods managed by a DaemonSet was to update the DaemonSet and then manually delete each Pod that was managed by the DaemonSet so that it would be re-created with the new configuration.&lt;/p&gt;

&lt;p&gt;DaemonSets can be rolled out using the same RollingUpdate strategy that deployments use. You can configure the update strategy using the spec.updateStrategy.type field, which should have the value RollingUpdate. When a DaemonSet has an update strategy of RollingUpdate, any change to the spec.template field (or subfields) in the DaemonSet will initiate a rolling update.&lt;/p&gt;

&lt;p&gt;The RollingUpdate strategy gradually updates members of a DaemonSet until all of the Pods are running the new configuration. There are two parameters that control the rolling update of a DaemonSet:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;spec.minReadySeconds, which determines how long a Pod must be “ready” before the rolling update proceeds to upgrade subsequent Pods&lt;/li&gt;
  &lt;li&gt;spec.updateStrategy.rollingUpdate.maxUnavailable, which indicates how many Pods may be simultaneously updated by the rolling update&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Once a rolling update has started, you can use the kubectl rollout commands to
see the current status of a DaemonSet rollout. For example, kubectl rollout status daemonSets cyberchef will show the current rollout status of a DaemonSet named cyberchef.&lt;/p&gt;

&lt;h3 id=&quot;35-deleting-a-daemonset&quot;&gt;3.5 Deleting a DaemonSet&lt;/h3&gt;

&lt;p&gt;Deleting a DaemonSet is pretty straightforward using the kubectl delete command. Just be sure to supply the correct name of the DaemonSet you would like to delete:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Delete using specification&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./cyberchef_daemon.yaml

&lt;span class=&quot;c&quot;&gt;# Delete using daemonSet&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete ds cyberchef
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;40-running-pods-that-perform-a-single-completable-task&quot;&gt;4.0 Running pods that perform a single completable task&lt;/h2&gt;

&lt;p&gt;ReplicationControllers, ReplicaSets, and DaemonSets run continuous tasks that are never considered completed. Processes in such pods are restarted when they exit. But in a completable task, after its process terminates, it should not be restarted again.&lt;/p&gt;

&lt;p&gt;In the event of a node failure, the pods on that node that are managed by a Job will be rescheduled to other nodes the way ReplicaSet pods are. In the event of a failure of the process itself (when the process returns an error exit code), the Job can be configured to either restart the container or not.&lt;/p&gt;

&lt;p&gt;For example, Jobs are useful for ad hoc tasks, where it’s crucial that the task finishes properly. You could run the task in an unmanaged pod and wait for it to finish, but in the event of a node failing or the pod being evicted from the node while it is performing its task, you’d need to manually recreate it. Doing this manually doesn’t make sense—especially if the job takes hours to complete.&lt;/p&gt;

&lt;p&gt;An example&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;nt&quot;&gt;---batch_job&lt;/span&gt;.yml
apiVersion: batch/v1
kind: Job
metadata:
  name: batch-job
spec:
  template:
    metadata:
      labels:
        app: batch-job
    spec:
      restartPolicy: Never
      containers:
      - name: pi
        image: perl
        &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;perl&quot;&lt;/span&gt;,  &lt;span class=&quot;s2&quot;&gt;&quot;-Mbignum=bpi&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;-wle&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;print bpi(50)&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./batch_job.yml

&lt;span class=&quot;c&quot;&gt;# Get the jobs&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get &lt;span class=&quot;nb&quot;&gt;jobs
&lt;/span&gt;NAME        COMPLETIONS   DURATION   AGE
batch-job   1/1           44s        46s

&lt;span class=&quot;c&quot;&gt;# Get the pods&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods
NAME              READY   STATUS      RESTARTS   AGE
batch-job-8tb2j   1/1     Running     0          1m

&lt;span class=&quot;c&quot;&gt;# Showing the running job&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl logs batch-job-8tb2j
3.1415926535897932384626433832795028841971693993751

&lt;span class=&quot;c&quot;&gt;# After completing the job&lt;/span&gt;
NAME              READY   STATUS      RESTARTS   AGE
batch-job-8tb2j   0/1     Completed   0          2m

&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## If you want to run multiple pods instances in a job&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Jobs may be configured to create more than one pod instance and run them in parallel or sequentially.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This is done by setting the completions and the parallelism properties in the Job spec&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: 5
  template:
    &amp;lt;template is the same as &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;listing 4.11&amp;gt;

&lt;span class=&quot;c&quot;&gt;# This Job will run five pods one after the other. It initially creates one pod, and when the pod’s container finishes, it creates the second pod, and so on, until five pods complete successfully. If one of the pods fails, the Job creates a new pod, so the Job may create more than five pods overall.&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Instead of running single Job pods one after the other, you can also make the Job run multiple pods in parallel. You specify how many pods are allowed to run in parallel with the parallelism Job spec property, as shown in the following listing.&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
apiVersion: batch/v1
kind: Job
metadata:
  name: multi-completion-batch-job
spec:
  completions: 5
  parallelism: 2
  template:
    &amp;lt;same as &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;listing 4.11&amp;gt;

&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Scaling a Job : You can even change a Job’s parallelism property while the Job is running.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This is similar to scaling a ReplicaSet or ReplicationController, and can be done with the kubectl scale command:&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl scale job batch-job &lt;span class=&quot;nt&quot;&gt;--replicas&lt;/span&gt; 3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;50--scheduling-jobs-to-run-periodically-or-once-in-the-future&quot;&gt;5.0  Scheduling Jobs to run periodically or once in the future&lt;/h2&gt;

&lt;p&gt;Job resources run their pods immediately when you create the Job resource. But many
batch jobs need to be run at a specific time in the future or repeatedly in the specified interval. In Linux and UNIX-like operating systems, these jobs are better known as cron jobs. Kubernetes supports them, too.&lt;/p&gt;

&lt;p&gt;A cron job in Kubernetes is configured by creating a CronJob resource. The
schedule for running the job is specified in the well-known cron format, so if you’re
familiar with regular cron jobs, you’ll understand Kubernetes’ CronJobs in a matter
of seconds.&lt;/p&gt;

&lt;p&gt;At the configured time, Kubernetes will create a Job resource according to the Job template configured in the CronJob object. When the Job resource is created, one or more pod replicas will be created and started according to the Job’s pod template, as you learned in the previous section. There’s nothing more to it.&lt;/p&gt;

&lt;h3 id=&quot;51-creating-a-cronjob&quot;&gt;5.1 Creating a Cronjob&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;nt&quot;&gt;---cronjob&lt;/span&gt;.yml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: &lt;span class=&quot;s2&quot;&gt;&quot;*/1 * * * *&quot;&lt;/span&gt;
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: hello
            image: busybox
            args:
            - /bin/sh
            - &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt;
            - &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;Hello from the Kubernetes cluster
          restartPolicy: OnFailure
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;52-understanding-how-scheduled-jobs-are-run&quot;&gt;5.2 Understanding how scheduled jobs are run&lt;/h3&gt;

&lt;p&gt;Job resources will be created from the CronJob resource at approximately the scheduled time. The Job then creates the pods.&lt;/p&gt;

&lt;p&gt;It may happen that the Job or pod is created and run relatively late. You may have a hard requirement for the job to not be started too far over the scheduled time. In that case, you can specify a deadline by specifying the startingDeadlineSeconds field in the CronJob.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# In thi example one of the times the job is supposed to run is 10:30:00.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# If it doesn’t start by 10:30:15 for whatever reason, the job will not run and will be shown as Failed.&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
apiVersion: batch/v1beta1
kind: CronJob
spec:
  schedule: &lt;span class=&quot;s2&quot;&gt;&quot;0,15,30,45 ****&quot;&lt;/span&gt;
  startingDeadlineSeconds: 15
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Kubernetes" /><summary type="html"></summary></entry><entry><title type="html">Kubernetes Pods Deep Dive</title><link href="https://hacstac.github.io/Notes/kubernetes/2020/09/23/Kubernetes-Pods.html" rel="alternate" type="text/html" title="Kubernetes Pods Deep Dive" /><published>2020-09-23T00:00:00-05:00</published><updated>2020-09-23T00:00:00-05:00</updated><id>https://hacstac.github.io/Notes/kubernetes/2020/09/23/Kubernetes-Pods</id><content type="html" xml:base="https://hacstac.github.io/Notes/kubernetes/2020/09/23/Kubernetes-Pods.html">&lt;hr /&gt;

&lt;h2 id=&quot;10-pods&quot;&gt;1.0 Pods&lt;/h2&gt;

&lt;p&gt;POD: Kubernetes groups multiple containers into a single atomic unit called a Pod&lt;/p&gt;

&lt;p&gt;A pod is a co-located group of containers and represents the basic building block in Kubernetes. Instead of deploying containers individually, we always deploy and operate on a pod of containers. We’re not implying that a pod always includes more than one container—it’s common for pods to contain only a single container. The key thing about pods is that when a pod does have multiple containers, all of them are always run on a single worker node—it never spans multiple worker nodes. A Pod represents a collection of application containers and volumes running in the same execution environment. Pods, not containers, are the smallest deployable arti‐ fact in a Kubernetes cluster. This means all of the containers in a Pod always land on the same machine.&lt;/p&gt;

&lt;p&gt;Applications running in the same Pod share the same IP address and port space (net‐ work namespace), have the same hostname (UTS namespace), and can communicate using native interprocess communication channels over System V IPC or POSIX message queues (IPC namespace). However, applications in different Pods are isolated from each other; they have different IP addresses, different hostnames, and more. Containers in different Pods running on the same node might as well be on different servers.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Kubernetes/Pod1.png&quot; alt=&quot;Pods&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;11-some-general-faqs-regarding-pods&quot;&gt;1.1 Some general FAQs regarding pods&lt;/h3&gt;

&lt;h4 id=&quot;111-why-we-need-pods&quot;&gt;1.1.1 Why we need pods&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Understanding why multiple containers are better than one container running multiple processes&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Imagine an app consisting of multiple processes that either communicate through
IPC (Inter-Process Communication) or through locally stored files requires them to run on the same machine. Because in Kubernetes, we always run processes in containers, and each container is much like an isolated machine. Containers are designed to run only a single process per container (unless the process itself spawns child processes). If you run multiple unrelated operations in a single container, it is your responsibility to keep all those processes running, manage their logs, and so on. For example, you’d have to include a mechanism for automatically restarting individual processes if they crash. Also, all those processes would log to the same standard output, so you’d have difficulty figuring out what method logged. That’s why we need to run each process in its container, and because of this problem, we need a higher-level construct that will allow us to bind containers together and manage them as a single unit. This is the reasoning behind pods.&lt;/p&gt;

&lt;p&gt;A pod of containers allows us to run near related processes together and provide them with (almost) the same environment as if they were all running in a single
container, while keeping them somewhat isolated. This way, we can get the best of both
worlds. We can take advantage of all the features containers provide, while at the
same time giving the processes the illusion of running together.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Understanding the partial isolation between containers of the same pod&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We want containers inside each group to share certain resources, although not all, so that they’re not fully isolated. Kubernetes achieves this by config- uring Docker to have all containers of a pod share the same set of Linux namespaces instead of each container having its own set. Because all containers of a pod run under the same Network and UTS namespaces (we’re talking about Linux namespaces here), they all share the same hostname and network interfaces. Similarly, all containers of a pod run under the same IPC namespace and can communicate through IPC. In the latest Kubernetes and Docker versions, they can also share the same PID namespace, but that feature isn’t enabled by default. But when it comes to the filesystem, things are a little different. Because most of the container’s filesystem comes from the container image, by default, the filesystem of each container is fully isolated from other containers.&lt;/p&gt;

&lt;h4 id=&quot;112-how-containers-share-the-same-ip-and-port-space-in-pods&quot;&gt;1.1.2 How containers share the same IP and Port space in pods&lt;/h4&gt;

&lt;p&gt;One thing to stress here is that because containers in a pod run in the same Network namespace, they share the same IP address and port space. This means processes running in containers of the same pod need to take care not to bind to the same port numbers or they’ll run into port conflicts. But this only concerns containers in the same pod. Containers of different pods can never run into port conflicts, because each pod has a separate port space. All the containers in a pod also have the same loopback network interface, so a container can communicate with other containers in the same pod through localhost. All pods in a Kubernetes cluster reside in a single flat, shared, network-address space, which means every pod can access every other pod at the other pod’s IP address. No NAT (Network Address Translation) gateways exist between them. When two pods send network packets between each other, they’ll each see the actual IP address of the other as the source IP in the packet.&lt;/p&gt;

&lt;p&gt;Consequently, communication between pods is always simple. It doesn’t matter if two pods are scheduled onto a single or onto different worker nodes; in both cases the containers inside those pods can communicate with each other across the flat NAT- less network, much like computers on a local area network (LAN), regardless of the actual inter-node network topology. Like a computer on a LAN, each pod gets its own IP address and is accessible from all other pods through this network established specifically for pods. This is usually achieved through an additional software-defined net- work layered on top of the actual network.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Kubernetes/Pod_Network.png&quot; alt=&quot;Pod Network&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;113-how-to-organizing-containers-across-pods-properly&quot;&gt;1.1.3 How to organizing containers across pods properly&lt;/h4&gt;

&lt;p&gt;Pods are relatively lightweight, we can have as many as we need without incurring almost any overhead. Instead of stuffing everything into a single pod, we should organize apps into multiple pods, where each one contains only tightly related components or processes. For Ex: a multi-tier application consisting of a frontend
application server and a backend database should be configured as a single pod or as
two pods? Although nothing is stopping us from running both the frontend server and the database in a single pod with two containers, it isn’t the most appropriate way. We’ve said that all containers of the same pod always run co-located, but do the web server and the database really need to run on the same machine? The answer is obviously no.&lt;/p&gt;

&lt;p&gt;If both the frontend and backend are in the same pod, then both will always be run on the same machine. If you have a two-node Kubernetes cluster and only this single pod, you’ll only be using a single worker node and not taking advantage of the computational resources (CPU and memory) you have at your disposal on the second node. Splitting the pod into two would allow Kubernetes to schedule the frontend to one node and the backend to the other node, thereby improving the utilization of your infrastructure.&lt;/p&gt;

&lt;p&gt;Another reason why you shouldn’t put them both into a single pod is scaling. A pod is also the basic unit of scaling. Kubernetes can’t horizontally scale individual containers; instead, it scales whole pods. If your pod consists of a frontend and a backend container, when you scale up the number of instances of the pod to, let’s say, two, you end up with two frontend containers and two backend containers. Usually, frontend components have completely different scaling requirements
than the backends, so we tend to scale them individually. Not to mention the fact that
backends such as databases are usually much harder to scale compared to (stateless)
frontend web servers.&lt;/p&gt;

&lt;h4 id=&quot;114-when-to-use-multiple-containers-in-a-pod&quot;&gt;1.1.4 When to use multiple containers in a pod&lt;/h4&gt;

&lt;p&gt;The main reason to put multiple containers into a single pod is when the application
consists of one main process and one or more complementary processes. For example, the main container in a pod could be a web server that serves files from a certain file directory, while an additional container (a sidecar container) periodically downloads content from an external source and stores it in the web server’s directory. But always remember you need a good reason to place a two containers in a single pod. ( like another container for gathering logs or to be used for more specific need )&lt;/p&gt;

&lt;h4 id=&quot;115-what-is-pod-manifest&quot;&gt;1.1.5 What is pod manifest&lt;/h4&gt;

&lt;p&gt;Pods are described in a Pod manifest. The Pod manifest is just a text-file representation of the Kubernetes API object. Kubernetes strongly believes in declarative configu‐ ration. Declarative configuration means that you write down the desired state of the world in a configuration and then submit that configuration to a service that takes actions to ensure the desired state becomes the actual state.&lt;/p&gt;

&lt;p&gt;The Kubernetes API server accepts and processes Pod manifests before storing them in persistent storage (etcd). The scheduler also uses the Kubernetes API to find Pods that haven’t been scheduled to a node. The scheduler then places the Pods onto nodes depending on the resources and other constraints expressed in the Pod manifests. Multiple Pods can be placed on the same machine as long as there are sufficient resources. However, scheduling multiple replicas of the same application onto the same machine is worse for reliability, since the machine is a single failure domain. Consequently, the Kubernetes scheduler tries to ensure that Pods from the same application are distributed onto different machines for reliability in the presence of such failures. Once scheduled to a node, Pods don’t move and must be explicitly destroyed and rescheduled.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;12-let-get-into-the-pods&quot;&gt;1.2 Let get into the pods&lt;/h3&gt;

&lt;p&gt;Pods and other Kubernetes resources are usually created by posting a JSON or YAML
manifest to the Kubernetes REST API endpoint. Also, we can use other, simpler ways
of creating resources, such as the kubectl run command, but this usually allow us to configure a limited set of properties. Additionally, defining all your Kubernetes objects from YAML files makes it possible to store them in a version control system, with all the benefits it brings.&lt;/p&gt;

&lt;h4 id=&quot;121-main-parts-of-pod-definition&quot;&gt;1.2.1 Main parts of pod definition&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;The three important sections of any pod defination is:
    &lt;ul&gt;
      &lt;li&gt;Metadata includes the name, namespace, labels, and other information about the pod.&lt;/li&gt;
      &lt;li&gt;Spec contains the actual description of the pod’s contents, such as the pod’s containers, volumes, and other data.&lt;/li&gt;
      &lt;li&gt;Status contains the current information about the running pod, such as what condition the pod is in, the description and status of each container, and the pod’s internal IP and other basic info. The status part contains read-only runtime data that shows the state of the resource at a given moment. When creating a new pod, you never need to provide the status part.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;EX: let’s look at what a YAML definition for one of those pods looks like: I know this looks complicated, but it becomes simple once you understand the basics and know how to distinguish between the important parts and the minor details. Also, you can take comfort in the fact that when creating a new pod, the YAML you need to write is much shorter&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;---linkding&lt;/span&gt;.yml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    cattle.io/timestamp: &lt;span class=&quot;s2&quot;&gt;&quot;2020-09-19T07:58:22Z&quot;&lt;/span&gt;
    cni.projectcalico.org/podIP: 10.42.0.66/32
    cni.projectcalico.org/podIPs: 10.42.0.66/32
    field.cattle.io/ports: &lt;span class=&quot;s1&quot;&gt;'[[{&quot;containerPort&quot;:9090,&quot;dnsName&quot;:&quot;linkding-hostport&quot;,&quot;hostPort&quot;:9090,&quot;kind&quot;:&quot;HostPort&quot;,&quot;name&quot;:&quot;port&quot;,&quot;protocol&quot;:&quot;TCP&quot;,&quot;sourcePort&quot;:9090}]]'&lt;/span&gt;
  creationTimestamp: null
  generateName: linkding-5c77d5f4cf-
  labels:
    pod-template-hash: 5c77d5f4cf
    workload.user.cattle.io/workloadselector: deployment-default-linkding
  managedFields:
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
          f:cattle.io/timestamp: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
          f:field.cattle.io/ports: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:generateName: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:labels:
          .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
          f:pod-template-hash: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
          f:workload.user.cattle.io/workloadselector: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:ownerReferences:
          .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
          k:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;uid&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;762057a3-c1d7-4fa1-91ca-01e961fbc72c&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;:
            .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:apiVersion: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:blockOwnerDeletion: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:controller: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:kind: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:name: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:uid: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
      f:spec:
        f:containers:
          k:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;linkding&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;:
            .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:image: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:imagePullPolicy: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:livenessProbe:
              .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:failureThreshold: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:initialDelaySeconds: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:periodSeconds: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:successThreshold: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:tcpSocket:
                .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
                f:port: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:timeoutSeconds: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:name: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:ports:
              .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              k:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;containerPort&quot;&lt;/span&gt;:9090,&lt;span class=&quot;s2&quot;&gt;&quot;protocol&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;TCP&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;:
                .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
                f:containerPort: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
                f:hostPort: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
                f:name: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
                f:protocol: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:readinessProbe:
              .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:failureThreshold: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:initialDelaySeconds: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:periodSeconds: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:successThreshold: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:tcpSocket:
                .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
                f:port: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:timeoutSeconds: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:resources: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:securityContext:
              .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:allowPrivilegeEscalation: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:capabilities: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:privileged: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:readOnlyRootFilesystem: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:runAsNonRoot: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:stdin: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:terminationMessagePath: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:terminationMessagePolicy: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:tty: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:volumeMounts:
              .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              k:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;mountPath&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;/etc/linkding/data&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;:
                .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
                f:mountPath: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
                f:name: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:dnsPolicy: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:enableServiceLinks: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:restartPolicy: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:schedulerName: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:securityContext: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:terminationGracePeriodSeconds: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:volumes:
          .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
          k:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;data&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;:
            .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:hostPath:
              .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:path: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
              f:type: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:name: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
    manager: kube-controller-manager
    operation: Update
    &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;2020-09-19T07:58:22Z&quot;&lt;/span&gt;
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:metadata:
        f:annotations:
          f:cni.projectcalico.org/podIP: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
          f:cni.projectcalico.org/podIPs: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
    manager: calico
    operation: Update
    &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;2020-09-19T07:58:26Z&quot;&lt;/span&gt;
  - apiVersion: v1
    fieldsType: FieldsV1
    fieldsV1:
      f:status:
        f:conditions:
          k:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;type&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;ContainersReady&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;:
            .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:lastProbeTime: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:lastTransitionTime: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:status: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:type: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
          k:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;type&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;Initialized&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;:
            .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:lastProbeTime: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:lastTransitionTime: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:status: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:type: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
          k:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;type&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;Ready&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;:
            .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:lastProbeTime: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:lastTransitionTime: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:status: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:type: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:containerStatuses: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:hostIP: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:phase: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:podIP: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:podIPs:
          .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
          k:&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;ip&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;10.42.0.66&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;:
            .: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
            f:ip: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
        f:startTime: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
    manager: kubelet
    operation: Update
    &lt;span class=&quot;nb&quot;&gt;time&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;2020-09-19T07:59:15Z&quot;&lt;/span&gt;
  ownerReferences:
  - apiVersion: apps/v1
    blockOwnerDeletion: &lt;span class=&quot;nb&quot;&gt;true
    &lt;/span&gt;controller: &lt;span class=&quot;nb&quot;&gt;true
    &lt;/span&gt;kind: ReplicaSet
    name: linkding-5c77d5f4cf
    uid: 762057a3-c1d7-4fa1-91ca-01e961fbc72c
  selfLink: /api/v1/namespaces/default/pods/linkding-5c77d5f4cf-zlz6h
spec:
  containers:
  - image: sissbruecker/linkding:latest
    imagePullPolicy: Always
    livenessProbe:
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 2
      successThreshold: 1
      tcpSocket:
        port: 9090
      timeoutSeconds: 2
    name: linkding
    ports:
    - containerPort: 9090
      hostPort: 9090
      name: port
      protocol: TCP
    readinessProbe:
      failureThreshold: 3
      initialDelaySeconds: 10
      periodSeconds: 2
      successThreshold: 2
      tcpSocket:
        port: 9090
      timeoutSeconds: 2
    resources: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
    securityContext:
      allowPrivilegeEscalation: &lt;span class=&quot;nb&quot;&gt;false
      &lt;/span&gt;capabilities: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
      privileged: &lt;span class=&quot;nb&quot;&gt;false
      &lt;/span&gt;readOnlyRootFilesystem: &lt;span class=&quot;nb&quot;&gt;false
      &lt;/span&gt;runAsNonRoot: &lt;span class=&quot;nb&quot;&gt;false
    &lt;/span&gt;stdin: &lt;span class=&quot;nb&quot;&gt;true
    &lt;/span&gt;terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    &lt;span class=&quot;nb&quot;&gt;tty&lt;/span&gt;: &lt;span class=&quot;nb&quot;&gt;true
    &lt;/span&gt;volumeMounts:
    - mountPath: /etc/linkding/data
      name: data
    - mountPath: /var/run/secrets/kubernetes.io/serviceaccount
      name: default-token-zh8ch
      readOnly: &lt;span class=&quot;nb&quot;&gt;true
  &lt;/span&gt;dnsPolicy: ClusterFirst
  enableServiceLinks: &lt;span class=&quot;nb&quot;&gt;true
  &lt;/span&gt;nodeName: rancherserver
  priority: 0
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext: &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt;
  serviceAccount: default
  serviceAccountName: default
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    key: node.kubernetes.io/not-ready
    operator: Exists
    tolerationSeconds: 300
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    operator: Exists
    tolerationSeconds: 300
  volumes:
  - hostPath:
      path: /data
      &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt;
    name: data
  - name: default-token-zh8ch
    secret:
      defaultMode: 420
      secretName: default-token-zh8ch
status:
  phase: Pending
  qosClass: BestEffort
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# This is what we actually write:&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# linkding.yaml&lt;/span&gt;
apiVersion: v1
kind: Pod
metadata:
  name: linkding
spec:
  containers:
  - image: sissbruecker/linkding:latest
    name: linkding
    ports:
    - containerPort: 9090
      protocol: TCP

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;122-lets-create-a-pod&quot;&gt;1.2.2 Lets create a pod&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# ---Creation----&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# The simplest way to create a pod is via the imperative kubectl run command.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl run linkding &lt;span class=&quot;nt&quot;&gt;--image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;sissbruecker/linkding:latest &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;9090

&lt;span class=&quot;nt&quot;&gt;--or--&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# To create a pod from file&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./linkding.yaml

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;123-get-details-of-a-pod&quot;&gt;1.2.3 Get details of a pod&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# ---Details---&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Listing pods&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pods
If you ran this &lt;span class=&quot;nb&quot;&gt;command &lt;/span&gt;immediately after the Pod was created, you might see:
NAME       READY   STATUS   RESTARTS   AGE
linkding   0/1     Pending   0         1s
&lt;span class=&quot;c&quot;&gt;# The Pending state indicates that the Pod has been submitted but hasn’t been scheduled yet.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Get small overview&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl explain pods
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl explain pod.spec

&lt;span class=&quot;c&quot;&gt;# Getting the whole defination&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get pod linkding &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; yaml

&lt;span class=&quot;c&quot;&gt;# To get pod details&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl describe pods linkding
Name:         linkding-5c77d5f4cf-zlz6h
Namespace:    default
Priority:     0
Node:         rancherserver/10.0.1.69
Start Time:   Sat, 19 Sep 2020 07:58:23 +0000
Labels:       pod-template-hash&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5c77d5f4cf
              workload.user.cattle.io/workloadselector&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;deployment-default-linkding
Annotations:  cattle.io/timestamp: 2020-09-19T07:58:22Z
              cni.projectcalico.org/podIP: 10.42.0.120/32
              cni.projectcalico.org/podIPs: 10.42.0.120/32
              field.cattle.io/ports:
                &lt;span class=&quot;o&quot;&gt;[[{&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;containerPort&quot;&lt;/span&gt;:9090,&lt;span class=&quot;s2&quot;&gt;&quot;dnsName&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;linkding-hostport&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;hostPort&quot;&lt;/span&gt;:9090,&lt;span class=&quot;s2&quot;&gt;&quot;kind&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;HostPort&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;name&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;port&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;protocol&quot;&lt;/span&gt;:&lt;span class=&quot;s2&quot;&gt;&quot;TCP&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;sourcePort&quot;&lt;/span&gt;:9090&lt;span class=&quot;o&quot;&gt;}]&lt;/span&gt;...
Status:       Running
IP:           10.42.0.120
IPs:
  IP:           10.42.0.120
Controlled By:  ReplicaSet/linkding-5c77d5f4cf
Containers:
  linkding:
    Container ID:   docker://5afb45048bae434f94571685f822ba8922d86e829d10c0dd4d067cb7f3889ac2
    Image:          sissbruecker/linkding:latest
    Image ID:       docker-pullable://sissbruecker/linkding@sha256:96089547c4829f6578d35ec2ba4dbfb7afced22c971b692c3a7ed0105ca59af8
    Port:           9090/TCP
    Host Port:      9090/TCP
    State:          Running
      Started:      Wed, 23 Sep 2020 08:22:02 +0000
    Last State:     Terminated
      Reason:       Error
      Exit Code:    137
      Started:      Wed, 23 Sep 2020 08:21:04 +0000
      Finished:     Wed, 23 Sep 2020 08:21:50 +0000
    Ready:          True
    Restart Count:  3
    Liveness:       tcp-socket :9090 &lt;span class=&quot;nv&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10s &lt;span class=&quot;nb&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2s &lt;span class=&quot;nv&quot;&gt;period&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2s &lt;span class=&quot;c&quot;&gt;#success=1 #failure=3&lt;/span&gt;
    Readiness:      tcp-socket :9090 &lt;span class=&quot;nv&quot;&gt;delay&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;10s &lt;span class=&quot;nb&quot;&gt;timeout&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2s &lt;span class=&quot;nv&quot;&gt;period&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2s &lt;span class=&quot;c&quot;&gt;#success=2 #failure=3&lt;/span&gt;
    Environment:    &amp;lt;none&amp;gt;
    Mounts:
      /etc/linkding/data from data &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;rw&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
      /var/run/secrets/kubernetes.io/serviceaccount from default-token-zh8ch &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ro&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Conditions:
  Type              Status
  Initialized       True
  Ready             True
  ContainersReady   True
  PodScheduled      True
Volumes:
  data:
    Type:          HostPath &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;bare host directory volume&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    Path:          /data
    HostPathType:
  default-token-zh8ch:
    Type:        Secret &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;a volume populated by a Secret&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
    SecretName:  default-token-zh8ch
    Optional:    &lt;span class=&quot;nb&quot;&gt;false
&lt;/span&gt;QoS Class:       BestEffort
Node-Selectors:  &amp;lt;none&amp;gt;
Tolerations:     node.kubernetes.io/not-ready:NoExecute &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;300s
                 node.kubernetes.io/unreachable:NoExecute &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;300s
Events:
  Type     Reason          Age                     From                    Message
  &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;     &lt;span class=&quot;nt&quot;&gt;------&lt;/span&gt;          &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;                    &lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;                    &lt;span class=&quot;nt&quot;&gt;-------&lt;/span&gt;
  Normal   SandboxChanged  9m1s                    kubelet, rancherserver  Pod sandbox changed, it will be killed and re-created.
  Normal   Pulling         7m12s                   kubelet, rancherserver  Pulling image &lt;span class=&quot;s2&quot;&gt;&quot;sissbruecker/linkding:latest&quot;&lt;/span&gt;
  Normal   Pulled          6m55s                   kubelet, rancherserver  Successfully pulled image &lt;span class=&quot;s2&quot;&gt;&quot;sissbruecker/linkding:latest&quot;&lt;/span&gt;
  Normal   Created         6m44s                   kubelet, rancherserver  Created container linkding
  Normal   Started         6m33s                   kubelet, rancherserver  Started container linkding

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;124-deletion-of-pods&quot;&gt;1.2.4 Deletion of pods&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## ---Deletion---&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# deleting pod by a name&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete pod linkding

&lt;span class=&quot;c&quot;&gt;# Using the yaml, which is used when creating a pod&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./linkding.yaml

&lt;span class=&quot;c&quot;&gt;# delete multiple pods by a name&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete pods pod1 pod2

&lt;span class=&quot;c&quot;&gt;# delete pods using the label selectors&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete pod &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;creation_method&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;manual

&lt;span class=&quot;c&quot;&gt;# deleting pods by deleting the whole namespace&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete ns custom-namespace

&lt;span class=&quot;c&quot;&gt;# delete all pods in a namespace&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete pods &lt;span class=&quot;nt&quot;&gt;--all&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# delete all resources in a namespace&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl delete all &lt;span class=&quot;nt&quot;&gt;--all&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;125-accessing-the-pods&quot;&gt;1.2.5 Accessing the pods&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# ---Accessing the pod---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Using the port forwarding&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl port-forward linkding 9090:9090
&lt;span class=&quot;c&quot;&gt;# A secure tunnel is created from your local machine, through the Kubernetes master, to the instance of the Pod running on one of the worker nodes. As long as the port-forward command is still running, you can access the Pod (in this case the kuard web interface) at http://localhost:9090.&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;# Get the logs&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# When your application needs debugging, it’s helpful to be able to dig deeper than describe to understand what the application is doing. Kubernetes provides two commands for debugging running containers. The kubectl logs command downloads the current logs from the running instance:&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl logs linkding

&lt;span class=&quot;c&quot;&gt;# If you are using multicontainer pods ( use container name )&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl logs linkding &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &amp;lt;container&amp;gt;

&lt;span class=&quot;c&quot;&gt;# Running commands in container with exec&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Sometimes logs are insufficient, and to truly determine what’s going on you need to execute commands in the context of the container itself.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; linkding &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;------------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ---Copying files to and from containers---&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# At times you may need to copy files from a remote container to a local machine for more in-depth exploration.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; &amp;lt;pod-name&amp;gt;:/website/main.js ./main.js
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/config.txt &amp;lt;pod-name&amp;gt;:/config.txt
&lt;span class=&quot;c&quot;&gt;# Copying files into a container is an anti-pattern.&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;126-health-checks&quot;&gt;1.2.6 Health Checks&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# ---Health Checks---&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# When you run your application as a container in Kubernetes, it is automatically kept alive for you using a process health check. This health check simply ensures that the main process of your application is always running. If it isn’t, Kubernetes restarts it.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Add healthcheck&lt;/span&gt;
apiVersion: v1
kind: Pod
metadata:
  name: linkding
spec:
  containers:
  - image: sissbruecker/linkding:latest
    name: linkding
    livenessProbe:
      httpGet:
        path: /healthy
        port: 9090
      initialDeleySeconds: 5
      timeoutSeconds: 1
      periodSeconds: 10
      failureThreshold: 3
    ports:
    - containerPort: 9090
      protocol: TCP

&lt;span class=&quot;c&quot;&gt;# The preceding Pod manifest uses an httpGet probe to perform an HTTP GET request against the /healthy endpoint on port 9090 of the kuard container.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# The probe sets an initialDelaySeconds of 5, and thus will not be called until 5 seconds after all the containers in the Pod are created. The probe must respond within the 1-second time‐out, and the HTTP status code must be equal to or greater than 200 and less than 400 to be considered successful.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Kubernetes will call the probe every 10 seconds. If more than three consecutive probes fail, the container will fail and restart.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Types of Health Checks&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# In addition to HTTP checks, Kubernetes also supports tcpSocket health checks that open a TCP socket; if the connection is successful, the probe succeeds. This style of probe is useful for non-HTTP applications; for example, databases or other non– HTTP-based APIs.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Finally, Kubernetes allows exec probes. These execute a script or program in the context of the container. Following typical convention, if this script returns a zero exit code, the probe succeeds; otherwise, it fails. exec scripts are often useful for custom application validation logic that doesn’t fit neatly into an HTTP call.&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;127-resource-management&quot;&gt;1.2.7 Resource Management&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# ---Resource Management---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## Resource Requests: Minimum Required Resources&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# A Pod requests the resources required to run its containers. Kubernetes guarantees that these resources are available to the Pod. The most commonly requested resources are CPU and memory, but Kubernetes has support for other resource types as well, such as GPUs and more&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## Resource limits&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Requests are used when scheduling Pods to nodes. The Kubernetes scheduler will ensure that the sum of all requests of all Pods on a node does not exceed the capacity of the node&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Therefore, a Pod is guaranteed to have at least the requested resources when running on the node. Importantly, “request” specifies a minimum. It does not specify a maximum cap on the resources a Pod may use&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
apiVersion: v1
kind: Pod
metadata:
  name: linkding
spec:
  containers:

- image: sissbruecker/linkding:latest
    name: linkding
    resources:
      requests:
        cpu: &lt;span class=&quot;s2&quot;&gt;&quot;500m&quot;&lt;/span&gt;
        memory: &lt;span class=&quot;s2&quot;&gt;&quot;128Mi&quot;&lt;/span&gt;
      limits:
        cpu: &lt;span class=&quot;s2&quot;&gt;&quot;1000m&quot;&lt;/span&gt;
        memory: &lt;span class=&quot;s2&quot;&gt;&quot;256Mi&quot;&lt;/span&gt;
    livenessProbe:
      httpGet:
        path: /healthy
        port: 9090
      initialDeleySeconds: 5
      timeoutSeconds: 1
      periodSeconds: 10
      failureThreshold: 3
    ports:
  - containerPort: 9090
      protocol: TCP
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;128-persistent-volumes-with-pods&quot;&gt;1.2.8 Persistent Volumes with Pods&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# ---Persisting Data with Volumes---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# When a Pod is deleted or a container restarts, any and all data in the container’s file‐system is also deleted&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# This is often a good thing, since you don’t want to leave around cruft that happened to be written by your stateless web application&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# In other cases, having access to persistent disk storage is an important part of a healthy application&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Kubernetes models such persistent storage&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
apiVersion: v1
kind: Pod
metadata:
  name: linkding
spec:
  containers:

- image: sissbruecker/linkding:latest
    name: linkding
    resources:
      requests:
        cpu: &lt;span class=&quot;s2&quot;&gt;&quot;500m&quot;&lt;/span&gt;
        memory: &lt;span class=&quot;s2&quot;&gt;&quot;128Mi&quot;&lt;/span&gt;
      limits:
        cpu: &lt;span class=&quot;s2&quot;&gt;&quot;1000m&quot;&lt;/span&gt;
        memory: &lt;span class=&quot;s2&quot;&gt;&quot;256Mi&quot;&lt;/span&gt;
    livenessProbe:
      httpGet:
        path: /healthy
        port: 9090
      initialDeleySeconds: 5
      timeoutSeconds: 1
      periodSeconds: 10
      failureThreshold: 3
    volumeMounts:
  - mountPath: &lt;span class=&quot;s2&quot;&gt;&quot;/data&quot;&lt;/span&gt;
        name: &lt;span class=&quot;s2&quot;&gt;&quot;linkding-data&quot;&lt;/span&gt;
    ports:
  - containerPort: 9090
      protocol: TCP

&lt;span class=&quot;c&quot;&gt;# Different ways of using volumes with pods&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## Communication/Synchronization&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# To achieve this, the Pod uses an emptyDir volume. Such a volume is scoped to the Pod’s lifespan, but it can be shared between two containers.&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;-----------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Cache&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# An application may use a volume that is valuable for performance, but not required for correct operation of the application&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# For example, perhaps the application keeps prerendered thumbnails of larger images. Of course, they can be reconstructed from the original images, but that makes serving the thumbnails more expensive&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# You want such a cache to survive a container restart due to a health-check failure, and thus emptyDir works well for the cache use case as well&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;-----------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Persitent Data&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Sometimes you will use a volume for truly persistent data—data that is independent of the lifespan of a particular Pod, and should move between nodes in the cluster if a node fails or a Pod moves to a different machine for some reason&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# To achieve this, Kubernetes supports a wide variety of remote network storage volumes, including widely supported protocols like NFS and iSCSI as well as cloud provider network storage like Amazon’s Elastic Block Store, Azure’s Files and Disk Storage, as well as Google’s Persistent Disk&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;-----------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Mounting the host filesystem&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Other applications don’t actually need a persistent volume, but they do need some access to the underlying host filesystem. For example, they may need access to the /dev filesystem in order to perform raw block-level access to a device on the system&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# For these cases, Kubernetes supports the hostPath volume, which can mount arbitrary locations on the worker node into the container. The previous example uses the hostPath volume type.&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;-----------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Persisting Data Using Remote Disks&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Oftentimes, you want the data a Pod is using to stay with the Pod, even if it is restarted on a different host machine&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# To achieve this, you can mount a remote network storage volume into your Pod. When using network-based storage, Kubernetes automatically mounts and unmounts the appropriate storage whenever a Pod using that volume is scheduled onto a particular machine&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# There are numerous methods for mounting volumes over the network&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Kubernetes includes support for standard protocols such as NFS and iSCSI as well as cloud provider–based storage APIs for the major cloud providers (both public and private)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# In many cases, the cloud providers will also create the disk for you if it doesn’t already exist&lt;/span&gt;

volumes:

- name: &lt;span class=&quot;s2&quot;&gt;&quot;linkding&quot;&lt;/span&gt;
    nfs:
      server: my.nfs.server.local
      path: &lt;span class=&quot;s2&quot;&gt;&quot;/exports&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Kubernetes" /><summary type="html"></summary></entry><entry><title type="html">Step-By-Step Process To Deploy Kubernetes On Your Raspberry PIs</title><link href="https://hacstac.github.io/Notes/kubernetes/2020/09/15/MicroK8s-Installation-Guide.html" rel="alternate" type="text/html" title="Step-By-Step Process To Deploy Kubernetes On Your Raspberry PIs" /><published>2020-09-15T00:00:00-05:00</published><updated>2020-09-15T00:00:00-05:00</updated><id>https://hacstac.github.io/Notes/kubernetes/2020/09/15/MicroK8s-Installation-Guide</id><content type="html" xml:base="https://hacstac.github.io/Notes/kubernetes/2020/09/15/MicroK8s-Installation-Guide.html">&lt;hr /&gt;

&lt;h2 id=&quot;overview&quot;&gt;Overview&lt;/h2&gt;

&lt;p&gt;Kubernetes is one of orchestration system which is getting popular for the last five years. Google initially makes it, but now it’s maintained by its vast community worldwide. That’s why it might become a new standard for container orchestration. But having a Kubernetes cluster might be a bit expensive. You have to run at least two or three nodes to run the cluster and try several demo projects on that cluster. That could be overkill for someone. But how if we have a Kubernetes that might run in our local computer? That should be an exciting thing.&lt;/p&gt;

&lt;h3 id=&quot;hardware-need--must-have-&quot;&gt;Hardware Need ( Must Have )&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Raspberry PI (3/4 Min 2GB RAM): Minimum 2 Nodes ( 1 Master Node, 1 Worker Node )&lt;/li&gt;
  &lt;li&gt;Power Supply Unit ( 15W Type-C for Rasp4 or You can use POE Hat )&lt;/li&gt;
  &lt;li&gt;Ethernet Cable ( Minimum CAT 5e/6 ( Rasp supports up to 1Gbps ))&lt;/li&gt;
  &lt;li&gt;Micro SD Card ( Min 8 GB, Recommended 32 GB )&lt;/li&gt;
  &lt;li&gt;Good Case With FAN ( Saves your rasp from dirt and excessive heat )&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;software-requirement&quot;&gt;Software Requirement&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Etcher or Raspberry PI Imager&lt;/li&gt;
  &lt;li&gt;Ubuntu 20.4 64Bit ARM ( OS )&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;setup-process&quot;&gt;Setup Process&lt;/h2&gt;

&lt;h3 id=&quot;part---1-setup-raspberrypis&quot;&gt;Part - 1: Setup RaspberryPIs&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Use Raspberry PI Imager (Recommended) or Etcher to boot ubuntu 20.4 64Bit on SD Card&lt;/li&gt;
  &lt;li&gt;When Imager flashes the OS in the SD Card, open the boot drive and create a file name &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ssh&lt;/code&gt; (Without any extension).&lt;/li&gt;
  &lt;li&gt;Boot your Rasps&lt;/li&gt;
  &lt;li&gt;Check the rasp IP from your router or use Nmap to network scan like this: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;nmap -sP 192.168.0.0/24&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;SSH to rasps ( Ex: ssh ubuntu@10.0.1.49 )&lt;/li&gt;
  &lt;li&gt;Default username : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubuntu&lt;/code&gt; &amp;amp; default password : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ubuntu&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Do all the basics things you always do with your rasps like setup static IP, setup dotfiles, and other stuff.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;part---2-setup-microk8s&quot;&gt;Part - 2: Setup MicroK8s&lt;/h3&gt;

&lt;p&gt;Microk8s provides a single command installation of the latest Kubernetes release on a local machine for development and testing. Setup is quick, fast (~30 sec), and supports many plugins, including Istio, with a single command. Since K8s is not the easiest thing to get started with, having a tool that would make it easy for you to get going is very desirable.&lt;/p&gt;

&lt;p&gt;microk8s is strictly for Linux. There is no VM involved. It is distributed and runs as a snap — a pre-packaged application (similar to a Docker container). Snaps can be used on all major Linux distributions, including Ubuntu, Linux Mint, Debian, and Fedora.&lt;/p&gt;

&lt;h4 id=&quot;a-setup-docker-and-do-some-other-tweaks-before-installing-kubernetes&quot;&gt;A. Setup Docker and Do Some Other Tweaks Before Installing Kubernetes&lt;/h4&gt;

&lt;p&gt;It is not a surprise we are going to use Docker Engine for the container runtime. Despite there are alternatives in rkt, cri-o, and others. However, at a closer look, we can see Kubernetes uses containers. We use docker because it is the most famous container system and relatively easy to deploy.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Do this on all of your nodes ( Rasps )&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Install Docker&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;-sSL&lt;/span&gt; https://get.docker.com | sh


&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Enable cgroups (Control Groups). Cgroups allow the Linux kernel to limit and isolate resources.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Practically speaking, this allows Kubernetes to manage better resources used by the containers it runs and increases security by isolating containers from one another.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Inspect Docker&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker info
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;...&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
 Cgroup Driver: cgroups
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;...&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# If docker info shows this you need to change Cgroup to systemd, To allow systemd to act as the cgroups manager.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Create a /etc/docker/daemon.json ( Add the content to that file )&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;vim /etc/docker/daemon.json
&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;s2&quot;&gt;&quot;exec-opts&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;native.cgroupdriver=systemd&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;,
  &lt;span class=&quot;s2&quot;&gt;&quot;log-driver&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;json-file&quot;&lt;/span&gt;,
  &lt;span class=&quot;s2&quot;&gt;&quot;log-opts&quot;&lt;/span&gt;: &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;s2&quot;&gt;&quot;max-size&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;100m&quot;&lt;/span&gt;
  &lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;,
  &lt;span class=&quot;s2&quot;&gt;&quot;storage-driver&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;overlay2&quot;&lt;/span&gt;,
  &lt;span class=&quot;s2&quot;&gt;&quot;insecure-registries&quot;&lt;/span&gt; : &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;localhost:32000&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Restart Docker and Inspect Again&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;systemctl restart docker

&lt;span class=&quot;c&quot;&gt;# Inspect&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker info
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;...&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
 Cgroup Driver: systemd
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;...&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Enable cgroups limit support&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Enable limit support, as shown by the warnings in the docker info output above. You need to modify the kernel command line to enable these options at boot.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# For the Raspberry Pi 4, add the following to the /boot/firmware/cmdline.txt file:&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;---Manually&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;cgroup_enable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;cpuset
&lt;span class=&quot;nv&quot;&gt;cgroup_enable&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;memory
&lt;span class=&quot;nv&quot;&gt;cgroup_memory&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nv&quot;&gt;swapaccount&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;--or--&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;---Automatically&lt;/span&gt; with &lt;span class=&quot;nb&quot;&gt;sed&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Note the space before &quot;cgroup_enable=cpuset&quot;, to add a space after the last existing item on the line&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo sed&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'$ s/$/ cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1/'&lt;/span&gt; /boot/firmware/cmdline.txt
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Allow Iptables to see bridged traffic&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# According to the documentation, Kubernetes needs iptables to be configured to see bridged network traffic.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Enable net.bridge.bridge-nf-call-iptables and -iptables6&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt; | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-ip6tables = 1
net.bridge.bridge-nf-call-iptables = 1
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF

&lt;/span&gt;&lt;span class=&quot;c&quot;&gt;# To check&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;sysctl &lt;span class=&quot;nt&quot;&gt;--system&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;b-setup-microk8s&quot;&gt;B. Setup MicroK8s&lt;/h4&gt;

&lt;p&gt;You may follow the installation instruction of MicroK8S in the official &lt;a href=&quot;https://microk8s.io/docs/&quot;&gt;documentation&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Start with installing MicroK8s using Snap which will take only a few seconds.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Do this on all the nodes ( Only the Installation step, Rest of the steps are for just master server )&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;## Install MicroK8s&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;snap &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;microk8s &lt;span class=&quot;nt&quot;&gt;--channel&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1.19 &lt;span class=&quot;nt&quot;&gt;--classic&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Add user to group microk8s &amp;amp; give user permission to ~/.kube&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;usermod &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-G&lt;/span&gt; microk8s user
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo chown&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-R&lt;/span&gt; user ~/.kube

&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Check MicroK8s is Running&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;microk8s.status
microk8s is running
high-availability: no
  datastore master nodes: 10.0.1.2:19001
  datastore standby nodes: none
addons:
  disabled:
    dashboard            &lt;span class=&quot;c&quot;&gt;# The Kubernetes dashboard&lt;/span&gt;
    dns                  &lt;span class=&quot;c&quot;&gt;# CoreDNS&lt;/span&gt;
    ha-cluster           &lt;span class=&quot;c&quot;&gt;# Configure high availability on the current node&lt;/span&gt;
    helm                 &lt;span class=&quot;c&quot;&gt;# Helm 2 - the package manager for Kubernetes&lt;/span&gt;
    metrics-server       &lt;span class=&quot;c&quot;&gt;# K8s Metrics Server for API access to service metrics&lt;/span&gt;
    storage              &lt;span class=&quot;c&quot;&gt;# Storage class; allocates storage from host directory&lt;/span&gt;
    helm3                &lt;span class=&quot;c&quot;&gt;# Helm 3 - Kubernetes package manager&lt;/span&gt;
    host-access          &lt;span class=&quot;c&quot;&gt;# Allow Pods connecting to Host services smoothly&lt;/span&gt;
    ingress              &lt;span class=&quot;c&quot;&gt;# Ingress controller for external access&lt;/span&gt;
    metallb              &lt;span class=&quot;c&quot;&gt;# Loadbalancer for your Kubernetes cluster&lt;/span&gt;
    rbac                 &lt;span class=&quot;c&quot;&gt;# Role-Based Access Control for authorisation&lt;/span&gt;
    registry             &lt;span class=&quot;c&quot;&gt;# Private image registry exposed on localhost:32000&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Microk8s comes with a set of tools:&lt;/span&gt;
microk8s.config
microk8s.docker
microk8s.inspect
microk8s.kubectl
microk8s.start
microk8s.stop
microk8s.disable
microk8s.enable
microk8s.istioctl
microk8s.reset
microk8s.status

&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Check the nodes ( shows only master node )&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;microk8s.kubectl get nodes
NAME     STATUS   ROLES    AGE   VERSION
master   Ready    &amp;lt;none&amp;gt;   33m   v1.19.0-34+09a4aa08bb9e93

&lt;span class=&quot;c&quot;&gt;## Add an alias for microk8s.kubectl to saving a ton of time&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;snap &lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;microk8s.kubectl kubectl

&lt;span class=&quot;c&quot;&gt;## Add AddOns&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;microk8s.enable dns dashboard ingress helm helm3 storage metrics-server prometheus

&lt;span class=&quot;c&quot;&gt;# To Check&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;microk8s status
microk8s is running
high-availability: no
  datastore master nodes: 10.0.1.2:19001
  datastore standby nodes: none
addons:
  enabled:
    dashboard            &lt;span class=&quot;c&quot;&gt;# The Kubernetes dashboard&lt;/span&gt;
    dns                  &lt;span class=&quot;c&quot;&gt;# CoreDNS&lt;/span&gt;
    ha-cluster           &lt;span class=&quot;c&quot;&gt;# Configure high availability on the current node&lt;/span&gt;
    helm                 &lt;span class=&quot;c&quot;&gt;# Helm 2 - the package manager for Kubernetes&lt;/span&gt;
    metrics-server       &lt;span class=&quot;c&quot;&gt;# K8s Metrics Server for API access to service metrics&lt;/span&gt;
    ingress              &lt;span class=&quot;c&quot;&gt;# Ingress controller for external access&lt;/span&gt;
    helm3                &lt;span class=&quot;c&quot;&gt;# Helm 3 - Kubernetes package manager&lt;/span&gt;
    storage              &lt;span class=&quot;c&quot;&gt;# Storage class; allocates storage from host directory&lt;/span&gt;
  disabled:
    host-access          &lt;span class=&quot;c&quot;&gt;# Allow Pods connecting to Host services smoothly&lt;/span&gt;
    metallb              &lt;span class=&quot;c&quot;&gt;# Loadbalancer for your Kubernetes cluster&lt;/span&gt;
    rbac                 &lt;span class=&quot;c&quot;&gt;# Role-Based Access Control for authorisation&lt;/span&gt;
    registry             &lt;span class=&quot;c&quot;&gt;# Private image registry exposed on localhost:32000&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# To get details of all of your namespaces ( this returns your all the running services, pods, deployements and namespaces etc)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;microk8s.kubectl get all &lt;span class=&quot;nt&quot;&gt;--all-namespaces&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;c-access-the-kubernetes-dashboard&quot;&gt;C. Access the Kubernetes Dashboard&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# We use the headless OS, So we have no option to access services other than exposing to the internal network.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# By default Kubernetes dashboard is not accessible on the local network, but if you were using raspbionOS ( GUI ), you could access the clusterIP in your Raspberry pi.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# --Exposing--&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system edit service kubernetes-dashboard
&lt;span class=&quot;c&quot;&gt;# Change type: ClusterIP to NodePort&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Get service port&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;microk8s.kubectl &lt;span class=&quot;nt&quot;&gt;--namespace&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;kube-system get service kubernetes-dashboard
NAME                   TYPE       CLUSTER-IP       EXTERNAL-IP   PORT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;         AGE
kubernetes-dashboard   NodePort   10.152.183.188   &amp;lt;none&amp;gt;        443:30355/TCP   22m
&lt;span class=&quot;c&quot;&gt;# Dashboard Access : https://MasterServer:Port ( Ex: https://10.0.1.86:30355 )&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Kubernetes dashboard needs authentication to access to the dashboard&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Method 1 : Generate Token&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ token&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;microk8s kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system get secret | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;default-token | &lt;span class=&quot;nb&quot;&gt;cut&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot; &quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f1&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;microk8s kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system describe secret &lt;span class=&quot;nv&quot;&gt;$token&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Copy Token and Paste on Dashboard Login&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# If you again need this token&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;microk8s.kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system get secret
&lt;span class=&quot;c&quot;&gt;# Look for something like this kubernetes-dashboard-token-r62xm&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;microk8s.kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system describe secret kubernetes-dashboard-token-r62xm
&lt;span class=&quot;c&quot;&gt;# Shows Secret on Terminal Window&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;--or--&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Method 2 : Setup a Proxy&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# To Setup Proxy&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;microk8s.kubectl proxy &lt;span class=&quot;nt&quot;&gt;--accept-hosts&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;.&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--address&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.0.0.0 &amp;amp;

&lt;span class=&quot;c&quot;&gt;# Edit Dashboard Yaml&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;microk8s.kubectl &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; kube-system edit deploy kubernetes-dashboard &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; yaml

&lt;span class=&quot;c&quot;&gt;# Add ( - --enable-skip-login )&lt;/span&gt;
spec:
  containers:
    - args:
      - &lt;span class=&quot;nt&quot;&gt;--enable-skip-login&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# when login to dashboard just use ( skip ) option&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Access the server ( Master Server IP: 10.0.1.86 )&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# http://10.0.1.86:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;d-add-nodes-to-the-cluster&quot;&gt;D. Add Nodes To The Cluster&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# On Master Server&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;microk8s add-node
&lt;span class=&quot;c&quot;&gt;# It give join command like this: microk8s join 10.0.1.86:25000/6cae23f7273dc6700f439f8c19abc7de&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# On Worker Nodes&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;microk8s &lt;span class=&quot;nb&quot;&gt;join &lt;/span&gt;10.0.1.86:25000/6cae23f7273dc6700f439f8c19abc7de

&lt;span class=&quot;c&quot;&gt;# To Check&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;microk8s kubectl get nodes
NAME     STATUS   ROLES    AGE     VERSION
master   Ready    &amp;lt;none&amp;gt;   64m     v1.19.0-34+09a4aa08bb9e93
node1    Ready    &amp;lt;none&amp;gt;   2m49s   v1.19.0-34+09a4aa08bb9e93
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;e-deploy-some-fun&quot;&gt;E. Deploy Some Fun&lt;/h4&gt;

&lt;h5 id=&quot;e1-install-portainer&quot;&gt;E1. Install Portainer&lt;/h5&gt;

&lt;p&gt;Portainer is a lightweight management UI that allows you to manage your different Docker environments easily. Portainer provides an easy and straightforward solution for managing Docker containers and Swarm services through a web interface. Portainer supports a wide range of features for managing the Docker containers, such as controlling the creation and deletion of Swarm services, user authentication, authorizations, connecting, executing commands in the console of running containers viewing containers’ logs.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## Install Portainer&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Using Arkade ( Works on arm &amp;amp; amd64 )&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# If Arkade shows cluster unreachable, use (kubectl config view --raw &amp;gt;~/.kube/config)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;-sLS&lt;/span&gt; https://dl.get-arkade.dev | &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;sh
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;arkade &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;portainer
&lt;span class=&quot;c&quot;&gt;# Access Portainer UI on http://http://10.0.1.86:30777 ( Master Server IP : 10.0.1.86 )&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# If you are using ( arm64 ( 64 bit OS on Rasps ))&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;-LO&lt;/span&gt; https://raw.githubusercontent.com/portainer/portainer-k8s/master/portainer-nodeport.yaml
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; portainer-nodeport.yaml
&lt;span class=&quot;c&quot;&gt;# If it says it not exposed&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# microk8s kubectl expose deployment portainer --type=NodePort&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h5 id=&quot;e2-install-linkding--bookmark-manager-&quot;&gt;E2. Install Linkding ( Bookmark Manager )&lt;/h5&gt;

&lt;p&gt;Linkding is a self-hosted bookmark service : &lt;a href=&quot;https://github.com/sissbruecker/linkding&quot;&gt;sissbruecker/linkding&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Kubernetes/Linkding.png&quot; alt=&quot;Linkding&quot; /&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# I m installing linkding with persistent storage ( To know about What persistent storage is, check out my Kubernetes 101 post )&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# YAML for Persistent Volume ( pv.yml )&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
apiVersion: v1
kind: PersistentVolume
metadata:
  name: data
spec:
  accessModes:
    - ReadWriteOnce
  capacity:
    storage: 8Gi
  hostPath:
    path: /home/user/data
  storageClassName: development
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# YAML for Persistent Volume Claim ( pvc.yml )&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: data
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 1Gi
  storageClassName: development
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# YAML for Linkding Container Deployment  ( Install Linkding With Persistent Storage )&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
apiVersion: apps/v1
kind: Deployment
metadata:
  name: linkding
  labels:
    application: frontend
spec:
  replicas: 1
  selector:
    matchLabels:
      application: frontend
  template:
    metadata:
      labels:
        application: frontend
    spec:
      containers:
      - name: linkding
        image: sissbruecker/linkding
        ports:
        - containerPort: 9090
        imagePullPolicy: IfNotPresent
        volumeMounts:
        - name: data
          mountPath: /etc/linkding/data
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: data
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;## Create Deployments&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./pv.yml
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./pvc.yml
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./linkding.yml

&lt;span class=&quot;c&quot;&gt;# Apply Deployments&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get deployments
NAME       READY   UP-TO-DATE   AVAILABLE   AGE
linkding   1/1      1            1          102s

&lt;span class=&quot;c&quot;&gt;# Expose to the local network&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;microk8s kubectl expose deployment linkding &lt;span class=&quot;nt&quot;&gt;--type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;NodePort

&lt;span class=&quot;c&quot;&gt;# Get the Port&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl get svc
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;S&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;          AGE
linkding     NodePort    10.152.183.77   &amp;lt;none&amp;gt;        9090:31071/TCP   30s

&lt;span class=&quot;c&quot;&gt;# Access the Linkding Bookmark Manager ( http://10.0.1.86:31071 )&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# To create username password for Linkding ( Go to Portainer -&amp;gt; Application -&amp;gt; Linkding -&amp;gt; Console Access&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# python manage.py createsuperuser --username=user --email=admin@example.com&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# user = user&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# password = you set in above command&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h5 id=&quot;e3-install-codeserver&quot;&gt;E3. Install CodeServer&lt;/h5&gt;

&lt;p&gt;CodeServer is nothing but VS Code on the browser ( To use on any machine anywhere and access it in the browser. ) : &lt;a href=&quot;https://github.com/codercom/code-server&quot;&gt;codercom/code-server&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;## Installing code-server with persistent storage, because this gives us the freedom to access code files from desired directories&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# We already create PersitentVolume, but we need another claim for VSCode&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# YAML for Persistent Volume Claim ( code-server-pvc.yml )&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: code-data
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 3Gi
  storageClassName: development
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# YAML for code-server container deployment&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;---code-server&lt;/span&gt;.yml
apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: code-server
  name: code-server
spec:
  selector:
    matchLabels:
      app: code-server
  replicas: 3
  template:
    metadata:
      labels:
        app: code-server
    spec:
      containers:
      - image: codercom/code-server:latest
        imagePullPolicy: IfNotPresent
        name: code-server
        &lt;span class=&quot;nb&quot;&gt;env&lt;/span&gt;:
        - name: PASSWORD
          value: &lt;span class=&quot;s2&quot;&gt;&quot;password&quot;&lt;/span&gt;
        volumeMounts:
        - name: data
          mountPath: /home/coder/project
      volumes:
        - name: data
          persistentVolumeClaim:
            claimName: code-data
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;--------------------------------------------------------------------------------------&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Apply Deployments&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./code-server-pvc.yml
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl apply &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; ./code-server.yml

&lt;span class=&quot;c&quot;&gt;# Expose to internal network&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl expose deploy code-server &lt;span class=&quot;nt&quot;&gt;--type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;NodePort &lt;span class=&quot;nt&quot;&gt;--port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;80 &lt;span class=&quot;nt&quot;&gt;--target-port&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;8080
&lt;span class=&quot;c&quot;&gt;# Access the server on http://10.0.1.86&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# To Scale the deployment&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;kubectl scale deployment code-server &lt;span class=&quot;nt&quot;&gt;--replicas&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;You should now have an operational Kubernetes master and several worker nodes ready to accept workloads.&lt;/p&gt;

&lt;p&gt;I hope MicroK8 will be a great help for newcomers into Kubernetes to try it out and learn Kubernetes by playing with it. If you gave it a shot &amp;amp; liked it, leave me a comment here!&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Kubernetes" /><summary type="html"></summary></entry><entry><title type="html">Kubernetes 101</title><link href="https://hacstac.github.io/Notes/kubernetes/2020/09/15/Kubernetes-Introduction.html" rel="alternate" type="text/html" title="Kubernetes 101" /><published>2020-09-15T00:00:00-05:00</published><updated>2020-09-15T00:00:00-05:00</updated><id>https://hacstac.github.io/Notes/kubernetes/2020/09/15/Kubernetes-Introduction</id><content type="html" xml:base="https://hacstac.github.io/Notes/kubernetes/2020/09/15/Kubernetes-Introduction.html">&lt;hr /&gt;

&lt;h2 id=&quot;10-what-is-kubernetes&quot;&gt;1.0 What is Kubernetes&lt;/h2&gt;

&lt;p&gt;Kubernetes is an open-source platform/tool created by Google. It is written in GO-Lang. So currently Kubernetes is an open-source project under Apache 2.0 license. Sometimes in the industry, Kubernetes is also known as “K8s”. With Kubernetes, you can run any Linux container across private, public, and hybrid cloud environments. Kubernetes provides some edge functions, such as Loadbalancer, Service discovery, and Roled Based Access Control(RBAC).&lt;/p&gt;

&lt;p&gt;Basically, kubernetes is a software that allows us to deploy, manage and scale applications. The applications will be packed in containers and kubernetes groups them into units. It allows us to span our application over thousands of servers while looking like one single unit.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Key Features of Kubernetes
    &lt;ul&gt;
      &lt;li&gt;Horizontal Scaling&lt;/li&gt;
      &lt;li&gt;Auto Scaling&lt;/li&gt;
      &lt;li&gt;Health check &amp;amp; Self-healing&lt;/li&gt;
      &lt;li&gt;Load Balancer&lt;/li&gt;
      &lt;li&gt;Service Discovery&lt;/li&gt;
      &lt;li&gt;Automated rollbacks &amp;amp; rollouts&lt;/li&gt;
      &lt;li&gt;Canary Deployment&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;20-why-we-need-kubernetes&quot;&gt;2.0 Why we need Kubernetes&lt;/h2&gt;

&lt;p&gt;First, we need to familier with few terms:&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Monolithic Applications:&lt;/strong&gt; Years ago, most software applications were big monoliths, running either as a single process or as a small number of processes spread across a handful of servers. These legacy systems are still widespread today. They have slow release cycles and are updated relatively infrequently. At the end of every release cycle, developers pack- age up the whole system and hand it over to the ops team, who then deploys and monitors it. In case of hardware failures, the ops team manually migrates it to the remaining healthy servers. So these components that are all tightly coupled together and have to be developed, deployed, and managed as one entity, because they all run as a single OS process.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Problems with Monolithic Applications: Change of one part of the application require a redeployment of the whole application. Requires powerful servers, Uses Vertical Scaling ( which is Very Expensive ) and If one components creates problem, the whole application becomes unscalable.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Microservices Applications:&lt;/strong&gt; Today, these big monolithic legacy applications are slowly being broken down into smaller, independently running components called microservices. Each microservice runs as an independent process and communicates with other microservices through simple, well defined interfaces ( APIs ). Microservices communicate through synchronous protocols such as HTTP, over which they usually expose RESTful (REpresentational State Transfer) APIs, or through asyn- chronous protocols such as AMQP (Advanced Message Queueing Protocol).&lt;/p&gt;

&lt;p&gt;Microservices are decoupled from each other, they can be developed, deployed, updated,
and scaled individually. This enables you to change components quickly and as often as
necessary to keep up with today’s rapidly changing business requirements.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Problems with Microservices Based Applications: The bigger numbers of deployable components and increasingly larger datacenters, it becomes increasingly difficult to configure, manage, and keep the whole system running smoothly. It’s much harder to figure out where to put each of those components to achieve high resource utilization and thereby keep the hardware costs down and the one of biggest problem with scaling microservices is multiple apps that are running on same host mey have conflicting dependencies. One solution of this is applications could run in the exact same environment during development and in production so they have the exact same operating system, libraries, system configuration, networking environment, and everything else but this will not solve the conflicting, versions of libraries or different environment requirements to solve this issue we have great technology called containers. ( You are also provide Dedicated VM to particular service but it is not a ideal solution because it increases hardware cost and management)&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Doing all this manually is hard work. We need automation, which includes automatic scheduling of those components to our servers, automatic configuration, supervision, and failure-handling. This is where Kubernetes comes in.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Kubernetes/MicroServices.png&quot; alt=&quot;Microservices&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example, Kubernetes can easily manage a canary deployment for your system.&lt;/p&gt;

&lt;p&gt;Before used Kubernetes, you need to prepare your infrastructure to deploy a new microservice. I believe it cost you a few days or weeks. Without Kubernetes, large teams would have to manually script the deployment workflows. With Kubernetes, you don’t need to create your deployment script manually and it will reduce the amount of time and resources spent on DevOps.&lt;/p&gt;

&lt;h3 id=&quot;21-what-kubernetes-provides&quot;&gt;2.1 What kubernetes provides&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Service discovery and load balancing: Kubernetes can expose a container using the DNS name or using their own IP address. If traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is stable.&lt;/li&gt;
  &lt;li&gt;Storage orchestration: Kubernetes allows you to automatically mount a storage system of your choice, such as local storages, public cloud providers, and more.
Automated rollouts and rollbacks You can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new containers for your deployment, remove existing containers and adopt all their resources to the new container.&lt;/li&gt;
  &lt;li&gt;Automatic bin packing: You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources.&lt;/li&gt;
  &lt;li&gt;Self-healing: Kubernetes restarts containers that fail, replaces containers, kills containers that don’t respond to your user-defined health check, and doesn’t advertise them to clients until they are ready to serve.&lt;/li&gt;
  &lt;li&gt;Secret and configuration management: Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your container images, and without exposing secrets in your stack configuration.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;22-what-kubernetes-not-provides&quot;&gt;2.2 What kubernetes not provides&lt;/h3&gt;

&lt;p&gt;Kubernetes is a lot of good things, I hope to have been clear exposing all Kubernetes benefits. The main problem from Kubernetes newbie is that they discover it is not a PaaS (Platform as a Service) system like they suppose. Kubernetes is a lot of things but not an “all included” service. It is great and reduces the amount of work, especially on the sysadmin side, but doesn’t offer you any apart from the infrastructure.&lt;/p&gt;

&lt;p&gt;Said that most of the things you are looking for in a fully-managed system are there: simplified deployments, scaling, load balancing, logging, and monitoring. Usually, you get a standard configuration from your hosting but you can theoretically customize it if you really need it.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It does not limit the types of applications supported. Everything is written into the container so every container application, no matter on technology, can be run. The counterpart is that you still have to define the container by hand.&lt;/li&gt;
  &lt;li&gt;It doesn’t offer an automated deployment. You just have to push to a docker repository your built images, no more. This is quite easy if you already work in a process with Continuous Integration, Delivery, and Deployment (CI/CD), but consider that without it will be quite tricky.&lt;/li&gt;
  &lt;li&gt;It does not provide any application-level services, just infrastructure. If you need a database, you have to buy a service or run it into a dedicated container. Of course, taking charges of backups and so on.&lt;/li&gt;
  &lt;li&gt;Most of the interactions with the system are by a command line that wraps API. That’s very good because it allows automating each setting. Commands syntax is very simple but, if you are looking to a system that is managed fully by a UI you are looking to the worst place.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;30-how-does-kubernetes-work&quot;&gt;3.0 How does Kubernetes work&lt;/h2&gt;

&lt;p&gt;The system is composed of a master node and any number of worker nodes. When the developer submits a list of apps to the master, Kubernetes deploys them to the cluster of worker nodes. What node a component lands on doesn’t (and shouldn’t) matter—neither to the developer nor to the system administrator.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Kubernetes/Kubernetes_Master.png&quot; alt=&quot;Kubernetes Master&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Kubernetes will run your containerized app somewhere in the cluster, provide information to its components on how to find each other, and keep all of them running.
Because your application doesn’t care which node it’s running on, Kubernetes can
relocate the app at any time, and by mixing and matching apps, achieve far better
resource utilization than is possible with manual scheduling.&lt;/p&gt;

&lt;h3 id=&quot;31-architecture-of-kubernetes-cluster&quot;&gt;3.1 Architecture of Kubernetes Cluster&lt;/h3&gt;

&lt;p&gt;We’ve seen a bird’s-eye view of Kubernetes’ architecture. Now let’s take a closer look at what a Kubernetes cluster is composed of. At the hardware level, a Kubernetes cluster is composed of many nodes, which can be split into two types:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;The master node, which hosts the Kubernetes Control Plane that controls and manages the whole Kubernetes system&lt;/li&gt;
  &lt;li&gt;Worker nodes that run the actual applications you deploy&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Kubernetes/Kubernetes_arch1.png&quot; alt=&quot;Kubernetes Architecture&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;311-master--control-plane-&quot;&gt;3.1.1 Master ( Control Plane )&lt;/h4&gt;

&lt;p&gt;The Control Plane is what controls the cluster and makes it function. It consists of
multiple components that can run on a single master node or be split across multiple
nodes and replicated to ensure high availability. These components are&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Kubernetes API Server&lt;/strong&gt;: The application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster ( which admin and other control plane communicated with ).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Scheduler&lt;/strong&gt;: Scheduler watches API server for new Pod requests. It communicates with Nodes to create new pods and to assign work to nodes while allocating resources or imposing constraints.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Controller Manager&lt;/strong&gt;: Component on the master that runs controllers. Includes Node controller, Endpoint Controller, Namespace Controller, etc. ( Performs cluster-level functions, such as replicating components, keeping track of worker nodes, handling node failures, and so on )&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;Node Controller: Responsible for noticing and responding when nodes go down.&lt;/li&gt;
      &lt;li&gt;Replication Controller: Responsible for maintaining the correct number of pods for every replication controller object in the system.&lt;/li&gt;
      &lt;li&gt;Endpoints Controller: Populates the Endpoints object (that is, it joins Services and Pods).&lt;/li&gt;
      &lt;li&gt;Service Account and Token Controllers: Create default accounts and API access tokens for new namespaces.&lt;/li&gt;
      &lt;li&gt;Cloud-Controller-Manager: Cloud-controller-manager runs controllers that interact with the underlying cloud providers. The cloud-controller-manager binary is an alpha feature introduced in Kubernetes release 1.6. Cloud-controller-manager runs cloud-provider-specific controller loops only. You must disable these controller loops in the Kube-controller-manager. You can disable the controller loops by setting the –cloud-provider flag to external when starting the Kube-controller-manager.&lt;/li&gt;
      &lt;li&gt;Node Controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding.&lt;/li&gt;
      &lt;li&gt;Route Controller: For setting up routes in the underlying cloud infrastructure.&lt;/li&gt;
      &lt;li&gt;Service Controller: For creating, updating, and deleting cloud provider load balancers.&lt;/li&gt;
      &lt;li&gt;Volume Controller: For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;etcd&lt;/strong&gt;: A reliable distributed data store that persistently stores the cluster
configuration.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The components of the Control Plane hold and control the state of the cluster, but
they don’t run your applications. This is done by the (worker) nodes.&lt;/p&gt;

&lt;h4 id=&quot;312-slave--worker-nodes-&quot;&gt;3.1.2 Slave ( Worker Nodes )&lt;/h4&gt;

&lt;p&gt;The worker nodes are the machines that run your containerized applications. The
task of running, monitoring, and providing services to your applications is done by
the following components:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Docker, rkt, or another container runtime, which runs your containers&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Kubelet&lt;/strong&gt;: Kubectl registering the nodes with the cluster, watches for work assignments from the scheduler, instantiate new Pods, report back to the master.
Container Engine: Responsible for managing containers, image pulling, stopping the container, starting the container, destroying the container, etc.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Kube Proxy&lt;/strong&gt;: Responsible for forwarding app user requests to the right pod (load-balances network traffic between application components).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The sequence of deployment: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;DevOps -&amp;gt; API Server -&amp;gt; Scheduler -&amp;gt; Cluster -&amp;gt;Nodes -&amp;gt; Kubelet -&amp;gt; Container Engine -&amp;gt; Spawn Container in Pod&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The sequence of App user request: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;App user -&amp;gt; Kube proxy -&amp;gt; Pod -&amp;gt; Container(Your app is run here)&lt;/code&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;32-the-six-layers-of-k8s&quot;&gt;3.2 The Six Layers of K8s&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Kubernetes/Kubernetes_Abstraction.png&quot; alt=&quot;Kubernetes Abstrction&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Deployments create and manage ReplicaSets, which create and manage Pods, which run on Nodes, which have a container runtime, which run the app code you put in your Docker image.&lt;/p&gt;

&lt;p&gt;The levels shaded blue are higher-level K8s abstractions. The green levels represent Nodes and Node subprocess that you should be aware of, but may not touch.&lt;/p&gt;

&lt;h4 id=&quot;321-deployments&quot;&gt;3.2.1 Deployments&lt;/h4&gt;

&lt;p&gt;Although pods are the basic unit of computation in Kubernetes, they are not typically directly launched on a cluster. Instead, pods are usually managed by one more layer of abstraction: the deployment. A deployment’s primary purpose is to declare how many replicas of a pod should be running at a time. When a deployment is added to the cluster, it will automatically spin up the requested number of pods, and then monitor them. If a pod dies, the deployment will automatically re-create it. Using a deployment, you don’t have to deal with pods manually. You can just declare the desired state of the system, and it will be managed for you automatically.&lt;/p&gt;

&lt;h4 id=&quot;322-replicaset&quot;&gt;3.2.2 ReplicaSet&lt;/h4&gt;

&lt;p&gt;The Deployment creates a ReplicaSet that will ensure your app has the desired number of Pods. ReplicaSets will create and scale Pods based on the triggers you specify in your Deployment. Replication Controllers perform the same function as ReplicaSets, but Replication Controllers are old school. ReplicaSets are the smart way to manage replicated Pods in 2019.&lt;/p&gt;

&lt;h4 id=&quot;323-pods&quot;&gt;3.2.3 Pods&lt;/h4&gt;

&lt;p&gt;All containers will run in a pod. Pods abstract the network and storage away from the underlying containers. Your app will run here. Each pod has one unique IP address assigned which means one pod can communicate with each other like a traditional container in a docker environment. Each container inside the pod can reach all other pods into the virtual network, but cannot deep to the other container on other pods. That’s important to guarantee the pod abstraction: nobody has to know how the Pods are composed internally. Moreover, the IP assigned is volatile so you must use always the service name (resolved to the right IP directly). Pods are used as the unit of replication in Kubernetes. If your application becomes too popular and a single pod instance can’t carry the load, Kubernetes can be configured to deploy new replicas of your pod to the cluster as necessary. Even when not under heavy load, it is standard to have multiple copies of a pod running at any time in a production system to allow load balancing and failure resistance. Pods handle Volumes, Secrets, and configuration for containers. Pods are ephemeral. They are intended to be restarted automatically when they die.&lt;/p&gt;

&lt;p&gt;Note: Worker Node is already explained in above section&lt;/p&gt;

&lt;h3 id=&quot;33-key-terms-used-with-kubernetes&quot;&gt;3.3 Key Terms used with Kubernetes&lt;/h3&gt;

&lt;h4 id=&quot;331-services&quot;&gt;3.3.1 Services&lt;/h4&gt;

&lt;p&gt;The name “service” in informatics science is overused. In Kubernetes scope think to a service like something you want to serve. A Kubernetes service involves a set of pods and may offer a complex feature or just expose a single Pod with a single container. So you can have a service that provides a CMS feature, with database and web server inside, or two different services, one for the database and one for the webserver. That’s up to you. Basically it is abstraction layer on top of a set of ephemeral pods (think of this as the ‘face’ of a set of pods)&lt;/p&gt;

&lt;h4 id=&quot;332-ingress&quot;&gt;3.3.2 Ingress&lt;/h4&gt;

&lt;p&gt;By default, Kubernetes provides isolation between pods and the outside world. If you want to communicate with a service running in a pod, you have to open up a channel for communication. This is referred to as ingress. To do this there is an ingress controller that does something similar to a load balancer. It virtually forwards traffic from outside to the services. During this step, based on the ingress implementation you have chosen, you can add HTTPS encryption, route traffic based on the hostname or URL segments. The only service can be linked to the ingress controller, not Pods. There are multiple ways to add ingress to your cluster. The most common ways are by adding either an Ingress controller, or a LoadBalancer.&lt;/p&gt;

&lt;h4 id=&quot;333-volume&quot;&gt;3.3.3 Volume&lt;/h4&gt;

&lt;p&gt;By default Pod storage is volatile. This is important to know because at the first restart you will lose everything. Kubernetes volumes allow mapping some part of the hard drive of containers to a safe place. That space can be shared between containers. The mount point can be any part of the container, but a volume cannot be mount into another one.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;PersistentVolumes and PersistentVolumeClaims&lt;/strong&gt;: To help abstract away infrastructure specifics, K8s developed PersistentVolumes and PersistentVolumeClaims. Unfortunately the names are a bit misleading, because vanilla Volumes can have persistent storage, as well. PersisententVolumes (PV) and PersisentVolumeClaims (PVC) add complexity compared to using Volumes alone. However, PVs are useful for managing storage resources for large projects. With PVs, a K8s user still ends up using a Volume, but two steps are required first.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;A PersistentVolume is provisioned by a Cluster Administrator (or it’s provisioned dynamically).&lt;/li&gt;
  &lt;li&gt;An individual Cluster user who needs storage for a Pod creates PersistentVolumeClaim manifest. It specifies how much and what type of storage they need. K8s then finds and reserves the storage needed.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The user then creates a Pod with a Volume that uses the PVC. PersistentVolumes have lifecycles independent of any Pod. In fact, the Pod doesn’t even know about the PV, just the PVC. PVCs consume PV resources, analogously to how Pods consume Node resources.&lt;/p&gt;

&lt;h4 id=&quot;334-namespaces&quot;&gt;3.3.4 Namespaces&lt;/h4&gt;

&lt;p&gt;Think to namespace like the feature that makes Kubernetes multitenant. The namespace is the tenant level. Each namespace can partitioning resources to isolate services, ingress, and many other things. This feature is good to have a strong separation between application, delegate safely to different teams, and have separated environments in a single infrastructure. It is a virtual cluster on top of an underlying physical cluster&lt;/p&gt;

&lt;h4 id=&quot;335-labels-and-selectors&quot;&gt;3.3.5 Labels and Selectors&lt;/h4&gt;

&lt;p&gt;Labels are key-value pairs used to tag objects. Objects are the items that you create in a Kubernetes cluster (pods, deployments, replica sets, services, volumes etc). Selectors are used to collect objects based on tags.&lt;/p&gt;

&lt;h4 id=&quot;336-statefulsets&quot;&gt;3.3.6 StatefulSets&lt;/h4&gt;

&lt;p&gt;As we know, a ReplicaSet creates and manages Pods. If a Pod shuts down because a Node fails, a ReplicaSet can automatically replace the Pod on another Node. You should generally create a ReplicaSet through a Deployment rather than creating it directly, because it’s easier to update your app with a Deployment.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Kubernetes/Kubernetes_States.png&quot; alt=&quot;States&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Sometimes your app will need to keep information about its state. You can think of state as the current status of your user’s interaction with your app. So in a video game it’s all the unique aspects of the user’s character at a point in time. What do you do when your app has state you need to keep track of? Use a StatefulSet.&lt;/p&gt;

&lt;p&gt;Like a ReplicaSet, a StatefulSet manages deployment and scaling of a group of Pods based on a container spec. Unlike a Deployment, a StatefulSet’s Pods are not interchangeable. Each Pod has a unique, persistent identifier that the controller maintains over any rescheduling. StatefulSets for good for persistent, stateful backends like databases. The state information for the Pod is held in a Volume associated with the StatefulSet.&lt;/p&gt;

&lt;h4 id=&quot;337-daemonsets&quot;&gt;3.3.7 DaemonSets&lt;/h4&gt;

&lt;p&gt;DaemonSets are for continuous process. They run one Pod per Node. Each new Node added to the cluster automatically gets a Pod started by the DaemonSet. DaemonSets are useful for ongoing background tasks such as monitoring and log collection.
StatefulSets and DaemonSets are not controlled by a Deployment. Although they are at the same level of abstraction as a ReplicaSet, there is not a higher level of abstraction for them in the current API.&lt;/p&gt;

&lt;h4 id=&quot;338-etcd&quot;&gt;3.3.8 ETCD&lt;/h4&gt;

&lt;p&gt;It stores the configuration information which can be used by each of the nodes in the cluster. It is a high availability key-value store that can be distributed among multiple nodes. It is accessible only by Kubernetes API server as it may have some sensitive information. It is a distributed key-value store which is accessible to all.
ETCD is a distributed reliable key-value store used by Kubernetes to store all data used to manage the cluster. Think of it this way, when you have multiple nodes and multiple masters in your cluster, etcd stores all that information on all the nodes in the cluster in a distributed manner. ETCD is responsible for implementing locks within the cluster to ensure there are no conflicts between the Masters.&lt;/p&gt;

&lt;h4 id=&quot;339-cluster-ip&quot;&gt;3.3.9 Cluster IP&lt;/h4&gt;

&lt;p&gt;Only has a Virtual IP (also called Cluster IP). This service can be used within a cluster only Acts like a traffic router to your Pods inside the cluster. Each port exposed by a pod will need a service if you want a client to talk to it via that port. By default, the service port is the same as the port exposed by the Pod. Different methods to access this service: From any node inside the cluster: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;cluster IP&amp;gt;:&amp;lt;service port&amp;gt;&lt;/code&gt; From the node where a replica of the Pod is running: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;pod IP&amp;gt;:&amp;lt;pod port&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;3310-node-port&quot;&gt;3.3.10 Node Port&lt;/h4&gt;

&lt;p&gt;For this service, a physical port on the node is mapped to the service port and the service connects to the Pod via the pod port. Remember, this will also have a Virtual IP (i.e. cluster IP). Different methods to access this service: From outside the cluster, if any node in the cluster has a public IP: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;node public IP&amp;gt;:&amp;lt;node port&amp;gt;&lt;/code&gt; From any node inside the cluster: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;cluster IP&amp;gt;:&amp;lt;service port&amp;gt;&lt;/code&gt; From the node where a replica of the Pod is running: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;pod IP&amp;gt;:&amp;lt;pod port&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;3311-load-balancer&quot;&gt;3.3.11 Load Balancer&lt;/h4&gt;

&lt;p&gt;Giving public access to a node in your cluster is not a recommended method, When you want to give public access to a service, create a load balancer service (not getting into ingress at this stage). For each service that you want to give public access, you need to create a load balancer service, This type of service will generally work only in a cloud environment. Remember, this will also have a Virtual IP (i.e. cluster IP) This service will have a node port too Different methods to access this service: From outside the cluster: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;load balancer dns&amp;gt;:&amp;lt;service port&amp;gt;&lt;/code&gt; From outside the cluster, if any node in the cluster has a public IP: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;node public IP&amp;gt;:&amp;lt;node port&amp;gt;&lt;/code&gt; From any node inside the cluster: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;cluster IP&amp;gt;:&amp;lt;service port&amp;gt;&lt;/code&gt; From the node where a replica of the Pod is running: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;&amp;lt;pod IP&amp;gt;:&amp;lt;pod port&amp;gt;&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;3312-external-name&quot;&gt;3.3.12 External Name&lt;/h4&gt;

&lt;p&gt;This maps a service to endpoints completely outside of the cluster.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;40-befits-of-using-kubernetes&quot;&gt;4.0 Befits of using kubernetes&lt;/h2&gt;

&lt;h3 id=&quot;41-kubernetes-will-keeping-your-containers-running&quot;&gt;4.1 Kubernetes will keeping your containers running&lt;/h3&gt;

&lt;p&gt;Once the application is running, Kubernetes continuously makes sure that the deployed
state of the application always matches the description you provided. For example, if
you specify that you always want five instances of a web server running, Kubernetes will always keep exactly five instances running. If one of those instances stops working properly, like when its process crashes or when it stops responding, Kubernetes will restart it automatically. Similarly, if a whole worker node dies or becomes inaccessible, Kubernetes will select new nodes for all the containers that were running on the node and run them on the newly selected nodes.&lt;/p&gt;

&lt;h3 id=&quot;42-scaling-the-number-of-copies&quot;&gt;4.2 Scaling the number of copies&lt;/h3&gt;

&lt;p&gt;While the application is running, you can decide you want to increase or decrease the number of copies, and Kubernetes will spin up additional ones or stop the excess ones, respectively. You can even leave the job of deciding the optimal number of cop- ies to Kubernetes. It can automatically keep adjusting the number, based on real-time metrics, such as CPU load, memory consumption, queries per second, or any other metric your app exposes.&lt;/p&gt;

&lt;h3 id=&quot;43-hitting-a-moving-target&quot;&gt;4.3 Hitting a moving target&lt;/h3&gt;

&lt;p&gt;We’ve said that Kubernetes may need to move your containers around the cluster This can occur when the node they were running on has failed or because they were evicted from a node to make room for other containers. If the container is providing a service to external clients or other containers running in the cluster, how can they use the container properly if it’s constantly moving around the cluster? And how can clients connect to containers providing a service when those containers are replicated and spread across the whole cluster? To allow clients to easily find containers that provide a specific service, you can tell Kubernetes which containers provide the same service and Kubernetes will expose all of them at a single static IP address and expose that address to all applications run- ning in the cluster. This is done through environment variables, but clients can also look up the service IP through good old DNS. The kube-proxy will make sure connec- tions to the service are load balanced across all the containers that provide the service. The IP address of the service stays constant, so clients can always connect to its con- tainers, even when they’re moved around the cluster.&lt;/p&gt;

&lt;h3 id=&quot;44-simplifying-application-deployment&quot;&gt;4.4 Simplifying application deployment&lt;/h3&gt;

&lt;p&gt;Kubernetes exposes all its worker nodes as a single deployment platform,
application developers can start deploying applications on their own and don’t need
to know anything about the servers that make up the cluster. In essence, all the nodes are now a single bunch of computational resources that are waiting for applications to consume them. A developer doesn’t usually care what kind of server the application is running on, as long as the server can provide the application with adequate system resources. Certain cases do exist where the developer does care what kind of hardware the application should run on. ( Like some app needs SSD deployment instead of HDD )&lt;/p&gt;

&lt;h3 id=&quot;45-achieving-better-utilization-of-hardware&quot;&gt;4.5 Achieving better utilization of hardware&lt;/h3&gt;

&lt;p&gt;By setting up Kubernetes on your servers and using it to run your apps instead of running them manually, you’ve decoupled your app from the infrastructure. When you tell Kubernetes to run your application, you’re letting it choose the most appropriate node to run your application on based on the description of the application’s resource requirements and the available resources on each node. By using containers and not tying the app down to a specific node in your cluster, you’re allowing the app to freely move around the cluster at any time, so the different app components running on the cluster can be mixed and matched to be packed tightly onto the cluster nodes. This ensures the node’s hardware resources are utilized as best as possible.&lt;/p&gt;

&lt;p&gt;The ability to move applications around the cluster at any time allows Kubernetes
to utilize the infrastructure much better than what you can achieve manually.&lt;/p&gt;

&lt;h3 id=&quot;46-health-checking-and-self-healing&quot;&gt;4.6 Health checking and self-healing&lt;/h3&gt;

&lt;p&gt;Having a system that allows moving an application across the cluster at any time is also valuable in the event of server failures. As your cluster size increases, you’ll deal with failing computer components ever more frequently. Kubernetes monitors your app components and the nodes they run on and auto- matically reschedules them to other nodes in the event of a node failure. This frees the ops team from having to migrate app components manually and allows the team to immediately focus on fixing the node itself and returning it to the pool of available hardware resources instead of focusing on relocating the app.&lt;/p&gt;

&lt;h3 id=&quot;47-automatic-scaling&quot;&gt;4.7 Automatic scaling&lt;/h3&gt;

&lt;p&gt;Using Kubernetes to manage your deployed applications also means the ops team
doesn’t need to constantly monitor the load of individual applications to react to sudden load spikes. As previously mentioned, Kubernetes can be told to monitor the resources used by each application and to keep adjusting the number of running instances of each application. If Kubernetes is running on cloud infrastructure, where adding additional nodes is as easy as requesting them through the cloud provider’s API, Kubernetes can even automatically scale the whole cluster size up or down based on the needs of the deployed applications.&lt;/p&gt;

&lt;h3 id=&quot;48-simplifying-application-development&quot;&gt;4.8 Simplifying application development&lt;/h3&gt;

&lt;p&gt;Then there’s the fact that developers don’t need to implement features that they would usually implement. This includes discovery of services and/or peers in a clustered application. Kubernetes does this instead of the app. Usually, the app only needs to look up certain environment variables or perform a DNS lookup. If that’s not enough, the application can query the Kubernetes API server directly to get that and/or other information. Querying the Kubernetes API server like that can even save developers from having to implement complicated mechanisms such as leader election.&lt;/p&gt;

&lt;h3 id=&quot;49-storage-orchestration&quot;&gt;4.9 Storage orchestration&lt;/h3&gt;

&lt;p&gt;Consequently, mount local or public cloud or network storage.&lt;/p&gt;

&lt;h3 id=&quot;410-secret-and-configuration-management&quot;&gt;4.10 Secret and configuration management&lt;/h3&gt;

&lt;p&gt;Create and update secrets and configs without rebuilding your image&lt;/p&gt;

&lt;h3 id=&quot;411-horizontal-scaling&quot;&gt;4.11 Horizontal Scaling&lt;/h3&gt;

&lt;p&gt;first, we need to know what are the horizontal and vertical scalings are:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Vertically Scaling : Adding more CPUs, memory, and other server components&lt;/li&gt;
  &lt;li&gt;Horizontal Scaling : Setting up additional Servers and running multiple copies&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Kubernetes uses horizontal scaling, which provides greater flexibilities to the companies&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;50-ways-to-spin-up-kubernetes-cluster&quot;&gt;5.0 Ways to spin up kubernetes cluster&lt;/h2&gt;

&lt;h3 id=&quot;51-kubeadm&quot;&gt;5.1 Kubeadm&lt;/h3&gt;

&lt;p&gt;The official CNCF tool for provisioning Kubernetes clusters in a variety of shapes and forms (e.g. single-node, multi-node, HA, self-hosted)&lt;/p&gt;

&lt;h3 id=&quot;52-minikube&quot;&gt;5.2 Minikube&lt;/h3&gt;

&lt;p&gt;Minikube can run on Windows and MacOS, because it relies on virtualization (e.g. Virtualbox) to deploy a kubernetes cluster in a Linux VM. You can also run minikube directly on linux with or without virtualization. It also has some developer-friendly features, like add-ons.&lt;/p&gt;

&lt;p&gt;From a user perspective minikube is a very beginner friendly tool. You start the cluster using &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;minikube start&lt;/code&gt;, wait a few minutes and your &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;kubectl&lt;/code&gt; is ready to go. To specify a Kubernetes version you can use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--kubernetes-version&lt;/code&gt; flag.&lt;/p&gt;

&lt;p&gt;If you are new to Kubernetes the first class support for its dashboard that minikube offers may help you. With a simple minikube dashboard the application will open up giving you a nice overview of everything that is going on in your cluster. This is being achieved by minikube’s addon system that helps you integrating things like, Helm, Nvidia GPUs and an image registry with your cluster.&lt;/p&gt;

&lt;h3 id=&quot;53-microk8s&quot;&gt;5.3 MicroK8s&lt;/h3&gt;

&lt;p&gt;MicroK8s is a Kubernetes distribution from Canonical that is designed for fast and simple deployment, which makes it a good choice to run Kubernetes locally.&lt;/p&gt;

&lt;p&gt;Microk8s is similar to minikube in that it spins up a single-node Kubernetes cluster with its own set of add-ons. If MicroK8s runs on Linux, it also offers the advantage of not requiring VMs. On Windows and macOS, MicroK8s uses a VM framework called Multipass to create VMs for the Kubernetes cluster.&lt;/p&gt;

&lt;p&gt;Like minikube, microk8s is limited to a single-node Kubernetes cluster, with the added limitation of only running on Linux and only on Linux where snap is installed.&lt;/p&gt;

&lt;h3 id=&quot;54-k3s&quot;&gt;5.4 K3s&lt;/h3&gt;

&lt;p&gt;K3s is a minified version of Kubernetes developed by Rancher Labs. By removing dispensable features (legacy, alpha, non-default, in-tree plugins) and using lightweight components (e.g. sqlite3 instead of etcd3) they achieved a significant downsizing. This results in a single binary with a size of around 60 MB.&lt;/p&gt;

&lt;p&gt;K3s runs on any Linux distribution without any additional external dependencies or tools. It is marketed by Rancher as a lightweight Kubernetes offering suitable for edge environments, IoT devices, CI pipelines, and even ARM devices, like Raspberry Pi’s. K3s achieves its lightweight goal by stripping a bunch of features out of the Kubernetes binaries (e.g. legacy, alpha, and cloud-provider-specific features), replacing docker with containerd, and using sqlite3 as the default DB (instead of etcd). As a result, this lightweight Kubernetes only consumes 512 MB of RAM and 200 MB of disk space. K3s has some nice features, like Helm Chart support out-of-the-box.&lt;/p&gt;

&lt;p&gt;Unlike the previous two offerings, K3s can do multiple node Kubernetes cluster. However, due to technical limitations of SQLite, K3s currently does not support High Availability (HA), as in running multiple master nodes.&lt;/p&gt;

&lt;p&gt;One feature that stands out is called auto deployment. It allows you to deploy your Kubernetes manifests and Helm charts by putting them in a specific directory. K3s watches for changes and takes care of applying them without any further interaction. This is especially useful for CI pipelines and IoT devices (both target use cases of K3s). Just create/update your configuration and K3s makes sure to keep your deployments up to date.&lt;/p&gt;

&lt;h3 id=&quot;55-kind&quot;&gt;5.5 Kind&lt;/h3&gt;

&lt;p&gt;Kind (Kubernetes-in-Docker), as the name implies, runs Kubernetes clusters in Docker containers. This is the official tool used by Kubernetes maintainers for Kubernetes v1.11+ conformance testing. It supports multi-node clusters as well as HA clusters. Because it runs K8s in Docker, kind can run on Windows, Mac, and Linux.&lt;/p&gt;

&lt;p&gt;Kind is optimized first and foremost for CI pipelines, so it may not have some of the developer-friendly features of other offerings. Kind leads to a significantly faster startup speed compared to spawning VM.&lt;/p&gt;

&lt;p&gt;Creating a cluster is very similar to minikube’s approach. Executing kind create cluster, playing the waiting game and afterwards you are good to go. By using different names (–name) kind allows you to create multiple instances in parallel.&lt;/p&gt;

&lt;p&gt;If you are looking for a way to programmatically create a Kubernetes cluster, kind kindly (you have been for waiting for this, don’t you ) publishes its Go packages that are used under the hood.&lt;/p&gt;

&lt;h3 id=&quot;56-k3d&quot;&gt;5.6 K3d&lt;/h3&gt;

&lt;p&gt;A new project that aims to bring K3s-in-Docker (similar to kind).&lt;/p&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Kubernetes" /><summary type="html"></summary></entry><entry><title type="html">Docker Hard Parts [Part - 2]</title><link href="https://hacstac.github.io/Notes/docker/2020/09/01/Docker-Hard-Parts.html" rel="alternate" type="text/html" title="Docker Hard Parts [Part - 2]" /><published>2020-09-01T00:00:00-05:00</published><updated>2020-09-01T00:00:00-05:00</updated><id>https://hacstac.github.io/Notes/docker/2020/09/01/Docker-Hard-Parts</id><content type="html" xml:base="https://hacstac.github.io/Notes/docker/2020/09/01/Docker-Hard-Parts.html">&lt;h2 id=&quot;70-building-images-automatically-with-dockerfiles&quot;&gt;7.0 Building Images automatically with DockerFiles&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;71-instructions&quot;&gt;7.1 Instructions&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.dockerignore&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;FROM&lt;/code&gt; Sets the Base Image for subsequent instructions.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;MAINTAINER&lt;/code&gt; (deprecated - use LABEL instead) Set the Author field of the generated images.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;RUN&lt;/code&gt; execute any commands in a new layer on top of the current image and commit the results.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;CMD&lt;/code&gt; provide defaults for an executing container.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;EXPOSE&lt;/code&gt; informs Docker that the container listens on the specified network ports at runtime. NOTE: does not actually make ports accessible.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ENV&lt;/code&gt; sets environment variable.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ADD&lt;/code&gt; copies new files, directories or remote file to container. Invalidates caches. Avoid ADD and use COPY instead.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;COPY&lt;/code&gt; copies new files or directories to container. By default this copies as root regardless of the USER/WORKDIR settings. Use &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--chown=&amp;lt;user&amp;gt;:&amp;lt;group&amp;gt;&lt;/code&gt; to give ownership to another user/group. (Same for ADD.)&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ENTRYPOINT&lt;/code&gt; configures a container that will run as an executable.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;VOLUME&lt;/code&gt; creates a mount point for externally mounted volumes or other containers.*   USER sets the user name for following RUN / CMD / ENTRYPOINT commands.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;WORKDIR&lt;/code&gt; sets the working directory.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ARG&lt;/code&gt; defines a build-time variable.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;ONBUILD&lt;/code&gt; adds a trigger instruction when the image is used as the base for another build.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;STOPSIGNAL&lt;/code&gt; sets the system call signal that will be sent to the container to exit.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;LABEL&lt;/code&gt; apply key/value metadata to your images, containers, or daemons.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; : If your running dangerous commands ( add a logic that it fails if your shell is outside docker )&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#  Shell script fails if it’s run outside a container&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; /.dockerenv &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;then
    &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'Not in a Docker container, exiting.'&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;1
&lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;72-getting-practical&quot;&gt;7.2 Getting Practical&lt;/h3&gt;

&lt;h4 id=&quot;721-packaging-git-with-dockerfile&quot;&gt;7.2.1 Packaging Git with Dockerfile&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Create a file name Dockerfile&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
FROM ubuntu:latest
LABEL &lt;span class=&quot;nv&quot;&gt;maintainer&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;dia@allingeek.com&quot;&lt;/span&gt;
RUN apt-get update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; git
ENTRYPOINT &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;git&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Instantly Create Dockerfile Images&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; htop - &lt;span class=&quot;o&quot;&gt;&amp;lt;&amp;lt;&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;EOF&lt;/span&gt;&lt;span class=&quot;sh&quot;&gt;
FROM alpine
RUN apk --no-cache add htop
&lt;/span&gt;&lt;span class=&quot;no&quot;&gt;EOF

&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker image build &lt;span class=&quot;nt&quot;&gt;--tage&lt;/span&gt; ubuntu-git:auto  &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
FROM ubuntu:latest — Tells Docker to start from the latest Ubuntu image just as
you did when creating the image manually.

LABEL maintainer — Sets the maintainer name and email &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;the image. Provid-
ing this information helps people know whom to contact &lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;there’s a problem
with the image. This was accomplished earlier when you invoked
commit.

RUN apt-get update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; git — Tells the builder to run the
provided commands to &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;Git.

ENTRYPOINT &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;git&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; — Sets the entrypoint &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;the image to  git.
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# --file or -f will read from diff file like 'BuildScript or anything'&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# --quiet or -q will run in quiet mode&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;722-dockerignore&quot;&gt;7.2.2 Dockerignore&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.dockerignore&lt;/code&gt; file will help us to exclude some files to add to the image during the build&lt;/li&gt;
  &lt;li&gt;CLI modifies the context to exclude files and directories that match patterns in it. This helps to avoid unnecessarily sending large or sensitive files and directories to the daemon and potentially adding them to images using ADD or COPY.&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;723-file-system-instructions&quot;&gt;7.2.3 File System Instructions&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- COPY : Will copy files from the filesystem where the image is being built.
- VOLUME : Same as &lt;span class=&quot;nt&quot;&gt;--volume&lt;/span&gt; flag &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; bound mount volume &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
- CMD : This is a closely related to ENTRYPOINT
- ADD : This operates similarly to the COPY instruction with two imp differences:
           - fetch remote sources files &lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;a Url is specified
           - Extract the files of any &lt;span class=&quot;nb&quot;&gt;source &lt;/span&gt;determined to be an archive file
- ONBUILD : This instruction &lt;span class=&quot;nb&quot;&gt;let &lt;/span&gt;the other instructions to execute &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;resulting image is used as base img &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;another build &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;# Example&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# ADD : We can ADD large no of files to a container without any problem&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Docker will unpack tarfiles of most standard types (.gz, .bz2, .xz, .tar).&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# some.tar&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
FROM Debian
RUN &lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /opt/libeatmydata
ADD some.tar.gz /opt/libeatmydata/
RUN &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-lRt&lt;/span&gt; /opt/libeatmydata
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# ONBUILD : Use the ONBUILD command to automate and encapsulate the building of an image.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# GO Example  ( Outyet : Simple Go App )&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git clone https://github.com/golang/example
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;example/outyet
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; outyet &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--publish&lt;/span&gt; 8080:8080 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; outyet1 &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; outyet
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

With ONBUILD
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
FROM golang:onbuild
EXPOSE 8080
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

golang:onbuild Dockerfile
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
FROM golang:1.7
RUN &lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; /go/src/app
WORKDIR /go/src/app
CMD &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;go-wrapper&quot;&lt;/span&gt;, &lt;span class=&quot;s2&quot;&gt;&quot;run&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
ONBUILD COPY &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; /go/src/app
ONBUILD RUN go-wrapper download
ONBUILD RUN go-wrapper &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
The result of this technique is that you have an easy way to build an image that only contains  the  code  required  to  run  it, and no more.
There are also other examples of ONBUILD exists : node:onbuild , python:onbuild


&lt;span class=&quot;c&quot;&gt;# ENTRYPOINT : Sets the entrypoint for the image&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Basic Shell Script for clean logs&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#!/bin/bash&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Cleaning logs over &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt; days old&quot;&lt;/span&gt;
find /log_dir &lt;span class=&quot;nt&quot;&gt;-ctime&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$1&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-name&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'*log'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-exec&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\;&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Create a DockerFile ( Create a container with clean_log script )&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
FROM ubuntu:17.04
ADD clean_log /usr/bin/clean_log
RUN &lt;span class=&quot;nb&quot;&gt;chmod&lt;/span&gt; +x /usr/bin/clean_log
ENTRYPOINT &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/usr/bin/clean_log&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
CMD &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;7&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; log-cleaner &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /var/log/myapplogs:/log_dir log-cleaner 365
&lt;span class=&quot;c&quot;&gt;# Clean The logs of over a year ( default 7 days if no arg given )&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;724-create-a-maintainable-dockerfiles&quot;&gt;7.2.4 Create a Maintainable Dockerfiles&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- ARG : arg defines a variable thta &lt;span class=&quot;nb&quot;&gt;users &lt;/span&gt;can provide to docker when building and a image.
Ex: Dockerfile
ARG &lt;span class=&quot;nv&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;unknown
ENV &lt;span class=&quot;nv&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
LABEL base.version&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ version&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0.6&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; docker image build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; dockerinaction/mailer-base:&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; mailer-base.df &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--build-arg&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;VERSION&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;version&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker image inspect &lt;span class=&quot;nt&quot;&gt;--format&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
dockerinaction/mailer-base:0.6

&lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;  &lt;span class=&quot;s2&quot;&gt;&quot;base.name&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;Mailer Archetype&quot;&lt;/span&gt;,
   &lt;span class=&quot;s2&quot;&gt;&quot;base.version&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;0.6&quot;&lt;/span&gt;,
   &lt;span class=&quot;s2&quot;&gt;&quot;maintainer&quot;&lt;/span&gt;: &lt;span class=&quot;s2&quot;&gt;&quot;dia@allingeek.com&quot;&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;725-init-system-for-docker&quot;&gt;7.2.5 Init System for Docker&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Most Popular init systems are : runit, tini, BusyBox init, Supervisord, and DAEMON&lt;/li&gt;
  &lt;li&gt;By Default docker comes with tini init system&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--init&lt;/span&gt; alpine:3.6 nc &lt;span class=&quot;nt&quot;&gt;-l&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 3000
&lt;span class=&quot;c&quot;&gt;# Docker ran /dev/init -- nc -l -p 3000 inside the container instead of just nc&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;726-health-check-in-docker&quot;&gt;7.2.6 Health Check In Docker&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- There are two ways to specify the health check &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;:
1. Use a HEALTHCHECK instruction when defining the image
2. On the command-line when running a container

1. This is a 1st Mothed : It is used when we define an Image
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;FROM nginx:1.13-alpine
HEALTHCHECK &lt;span class=&quot;nt&quot;&gt;--interval&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5s &lt;span class=&quot;nt&quot;&gt;--retries&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;2 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 CMD nc &lt;span class=&quot;nt&quot;&gt;-vz&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-w&lt;/span&gt; 2 localhost 80 &lt;span class=&quot;o&quot;&gt;||&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;1

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker ps &lt;span class=&quot;nt&quot;&gt;--format&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'table \t\t'&lt;/span&gt;
NAMES            IMAGE                       STATUS
healthcheck_ex   dockerinaction/healthcheck  Up 3 minutes &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;healthy&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;


&lt;span class=&quot;c&quot;&gt;#  Exit Status Codes&lt;/span&gt;
- 0: success—The container is healthy and ready &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;use.
- 1: unhealthy—The container is not working correctly.
- 2: reserved—Do not use this &lt;span class=&quot;nb&quot;&gt;exit &lt;/span&gt;code.

2. Command-line Method
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container run &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;healthcheck_ex &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--health-cmd&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'nc -vz -w 2 localhost 80 || exit 1'&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
nginx:1.13-alpine
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;727-hardening-application-images&quot;&gt;7.2.7 Hardening Application Images&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;- There are Three Methods to hardended the images
1. We can enforce that our images are built from a specific image.
2. we can make sure that regardless of how containers are built from our image, they will have a sensible default user.
3. we should eliminate a common path &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;root user escalation
from programs with setuid or setgid attributes set.


1. Content Addressable Images identifiers &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; CAIID &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Build Images using authentic digest : that digest is known as CAIID&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# So now how many images we build from these they are authentic.&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
docker pull debian:stable
stable: Pulling from library/debian
31c6765cabf1: Pull &lt;span class=&quot;nb&quot;&gt;complete
&lt;/span&gt;Digest: sha256:6aedee3ef827...
&lt;span class=&quot;c&quot;&gt;# Dockerfile:&lt;/span&gt;
FROM debian@sha256:6aedee3ef827...
...
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

2. Create a User &amp;amp; &lt;span class=&quot;nb&quot;&gt;groups&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;RUN groupadd &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; postgres &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; useradd &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-g&lt;/span&gt; postgres postgres

3. SUID &amp;amp; GUID
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;FROM ubuntu:latest
&lt;span class=&quot;c&quot;&gt;# Set the SUID bit on whoami&lt;/span&gt;
RUN &lt;span class=&quot;nb&quot;&gt;chmod &lt;/span&gt;u+s /usr/bin/whoami
&lt;span class=&quot;c&quot;&gt;# Create an example user and set it as the default&lt;/span&gt;
RUN adduser &lt;span class=&quot;nt&quot;&gt;--system&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--no-create-home&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--disabled-password&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--disabled-login&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--shell&lt;/span&gt; /bin/sh example
USER example
&lt;span class=&quot;c&quot;&gt;# Set the default to compare the container user and&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# the effective user for whoami&lt;/span&gt;
CMD &lt;span class=&quot;nb&quot;&gt;printf&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Container running as: %s&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nb&quot;&gt;printf&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Effectively running whoami as: %s&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\n&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;whoami&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
Output
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
Container running as: example
Effectively running &lt;span class=&quot;nb&quot;&gt;whoami &lt;/span&gt;as: root
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

The output of the default &lt;span class=&quot;nb&quot;&gt;command &lt;/span&gt;shows that even though you’ve executed the
&lt;span class=&quot;nb&quot;&gt;whoami command &lt;/span&gt;as the example user, it’s running from the context of the root user.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;728-complete-story-of-cache&quot;&gt;7.2.8 Complete Story of Cache&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# --no-cache will downlaod fresh containers from source. it will not use the cache files&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Example&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;--no-cache&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Busting the Cache&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# we need this because some time our image build takes so much time. so we need cache up to a certain point.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Method 1 (cheat) :&lt;/span&gt;
Add a benign comment after the &lt;span class=&quot;nb&quot;&gt;command &lt;/span&gt;to invalidate the cache. This works because Docker treats the non-whitespace change to theline as though it were a new &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;, so the cached layer is not re-used.
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
CMD &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;npm&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;start&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt; &lt;span class=&quot;c&quot;&gt;#bust the cache&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Method 2 ( using ARG )&lt;/span&gt;
Use the ARG directive &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;your Dockerfile to &lt;span class=&quot;nb&quot;&gt;enable &lt;/span&gt;surgical cache-busting.
If this ARG variable isset to a value never used before on your host, the cache will be busted from that point.
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
WORKDIR todo
ARG &lt;span class=&quot;nv&quot;&gt;CACHEBUST&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;no
RUN npm &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;--build-arg&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;CACHEBUST&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RANDOM&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RANDOM&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; 19856
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;RANDOM&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; 26429

&lt;span class=&quot;c&quot;&gt;# If not using Bash&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;--build-arg&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;CACHEBUST&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt; +%s&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Method 3 ( Using ADD )&lt;/span&gt;
There are two useful features of ADD that you can use to your advantage &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;this context: it caches the contents of the file it refers to, and it can take a network resource as an argument.

&lt;span class=&quot;c&quot;&gt;# Git Repo Example&lt;/span&gt;
It means &lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;repo is not changed it uses cache or &lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;git repo is changed it rebuild the image from scratch.
But it will vary from resources &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;to resource type.

we can take &lt;span class=&quot;nb&quot;&gt;help &lt;/span&gt;of Github API here : It  has  URLs  foreach repository that &lt;span class=&quot;k&quot;&gt;return &lt;/span&gt;JSON &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;the most recent commits. When a new commit ismade, the content of the response changes.

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
FROM ubuntu:16.04
ADD https://api.github.com/repos/nodejs/node/commits  /dev/null
RUN git clone https://github.com/nodejs/node
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;729-flattening-images&quot;&gt;7.2.9 Flattening Images&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# This is imp because images can reveal the imp information&lt;/span&gt;
Example :

&lt;span class=&quot;c&quot;&gt;# create a docker file : It has sensitive information&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
FROM debian
RUN &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;My Big Secret&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; /tmp/secret_key
RUN &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /tmp/secret_key
RUN &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; /tmp/secret_key
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; secret &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# But now problem arise&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;history &lt;/span&gt;secret
...
5e39caf7560f 3 days ago /bin/sh &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;My Big Secret&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; /tmp/se 14 B
...

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run 5b376ff3d7cd &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /tmp/secret_key
My Big Secret

But &lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;someone could download this image from public repo and insect the &lt;span class=&quot;nb&quot;&gt;history&lt;/span&gt; &amp;amp; run thi &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt; : It reveal the secret.

&lt;span class=&quot;c&quot;&gt;# To get rid of this type of problem : we need to remove intermediate layering&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# we need to export the image as a trivially run container&lt;/span&gt;
and &lt;span class=&quot;k&quot;&gt;then &lt;/span&gt;re-import and tag the resulting image:
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; secret /bin/true
- new_id

&lt;span class=&quot;c&quot;&gt;# Runs a docker export, taking a conatiner iD as arg and outgoing a tar file of fs contents.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This is piped to docker import which takes tar file and create a new image.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;new_id | docker import - new_secret
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;history &lt;/span&gt;new_secret
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;73-refer&quot;&gt;7.3 Refer&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.docker.com/engine/reference/builder/#dockerfile-examples&quot;&gt;Examples&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.docker.com/engine/userguide/eng-image/dockerfile_best-practices/&quot;&gt;Best practices for writing Dockerfiles&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://crosbymichael.com/&quot;&gt;Michael Crosby&lt;/a&gt; has some more &lt;a href=&quot;http://crosbymichael.com/dockerfile-best-practices.html&quot;&gt;Dockerfiles best practices&lt;/a&gt; / &lt;a href=&quot;http://crosbymichael.com/dockerfile-best-practices-take-2.html&quot;&gt;take 2&lt;/a&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;http://jonathan.bergknoff.com/journal/building-good-docker-images&quot;&gt;Building Good Docker Images&lt;/a&gt; / &lt;a href=&quot;http://jonathan.bergknoff.com/journal/building-better-docker-images&quot;&gt;Building Better Docker Images&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://speakerdeck.com/garethr/managing-container-configuration-with-metadata&quot;&gt;Managing Container Configuration with Metadata&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://rock-it.pl/how-to-write-excellent-dockerfiles/&quot;&gt;How to write excellent Dockerfiles&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;80-registry--repository&quot;&gt;8.0 Registry &amp;amp; Repository&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Docker/Docker_Hard_Parts/Repository.png&quot; alt=&quot;Registry.png&quot; /&gt;&lt;/p&gt;

&lt;p&gt;A repository is a &lt;em&gt;hosted&lt;/em&gt; collection of tagged images that together create the file system for a container.&lt;/p&gt;

&lt;p&gt;A registry is a &lt;em&gt;host&lt;/em&gt; – a server that stores repositories and provides an HTTP API for &lt;a href=&quot;https://docs.docker.com/engine/tutorials/dockerrepos/&quot;&gt;managing the uploading and downloading of repositories&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;81-enviornment&quot;&gt;8.1 Enviornment&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.docker.com/engine/reference/commandline/login&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker login&lt;/code&gt;&lt;/a&gt; to login to a registry.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.docker.com/engine/reference/commandline/logout&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker logout&lt;/code&gt;&lt;/a&gt; to logout from a registry.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.docker.com/engine/reference/commandline/search&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker search&lt;/code&gt;&lt;/a&gt; searches registry for image.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.docker.com/engine/reference/commandline/pull&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker pull&lt;/code&gt;&lt;/a&gt; pulls an image from registry to local machine.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://docs.docker.com/engine/reference/commandline/push&quot;&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker push&lt;/code&gt;&lt;/a&gt; pushes an image to the registry from local machine.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt; : Refer Chapter 9 of Docker in Action : Public and private
software distribution ( This Cover’s Every Basic detail about Registry &amp;amp; Repository )&lt;/p&gt;

&lt;h3 id=&quot;82-setup-local-docker-registry&quot;&gt;8.2 Setup Local Docker Registry&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# To start a registry on local Network&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 5000:5000 &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/registry:/var/lib/registry registry:2

&lt;span class=&quot;c&quot;&gt;# This  command  makes  the  registry  available  on  port  5000  of  the  Docker  host(-p  5000:5000).  With  the  -v  flag,  it  makes  the  registry folder on your host(/var/lib/registry)  available  in  the  container  as  $HOME/registry.  The  registry’s  fileswill therefore be stored on the host in the /var/lib/registry folder.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# HOSTNAME is the hostname or IP address of your new reg-istry server&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# we also use --insecure-registry ( We know our local network is secure :) [Docker will only allow you to pull from registries with a signedHTTPS certificate.] )&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Push the image to the registry&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker push HOSTNAME:5000/image:tag.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;90-docker-compose&quot;&gt;9.0 Docker Compose&lt;/h2&gt;

&lt;p&gt;Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration. To learn more about all the features of Compose, see the &lt;a href=&quot;https://docs.docker.com/compose/overview/#features&quot;&gt;list of features&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The standard filename for Compose files is &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker-compose.yml&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;91-yaml-basics&quot;&gt;9.1 YAML Basics&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;A YAML document can include a comment at the end of any line. Com￾ments are marked by a space followed by a hash sign ( #). Any characters that follow until the end of the line are ignored by the parser.&lt;/li&gt;
  &lt;li&gt;YAML uses three types of data and two styles of describing that data, block and flow.
    &lt;ul&gt;
      &lt;li&gt;Flow collections are specified similarly to collection literals in JavaScript and other
languages. For example, the following is a list of strings in the flow style: &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[&quot;PersonA&quot;,&quot;PersonB&quot;]&lt;/code&gt;&lt;/li&gt;
      &lt;li&gt;The block style is more common and will be used in this primer except where noted.
The three types of data are &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;maps&lt;/code&gt;, &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;lists&lt;/code&gt;, and &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;scalar values&lt;/code&gt;.
        &lt;ul&gt;
          &lt;li&gt;Maps are defined by a set of unique properties in the form of key/value pairs that
are delimited by a colon and space (: ).&lt;/li&gt;
          &lt;li&gt;Scaler Values : Scaler String &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;image: &quot;alpine&quot;&lt;/code&gt;
  , Scaler Command : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;command: echo hello world&lt;/code&gt;&lt;/li&gt;
          &lt;li&gt;Scaler Rules :
            &lt;ol&gt;
              &lt;li&gt;Must not be empty,&lt;/li&gt;
              &lt;li&gt;Must not contain leading or trailing whitespace characters&lt;/li&gt;
              &lt;li&gt;Must not begin with an indicator character (for example, - or :) in places where
doing so would cause an ambiguity.&lt;/li&gt;
              &lt;li&gt;Must never contain character combinations using a colon (:) and hash sign (#)&lt;/li&gt;
            &lt;/ol&gt;
          &lt;/li&gt;
          &lt;li&gt;Lists (or block sequences) are series of nodes in which each element is denoted by a
leading hyphen (-) indicator. For example:
      &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;- item 1&lt;/code&gt;
      &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;- item 2&lt;/code&gt;
      &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;- item 3&lt;/code&gt;
      &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;- # an empty item&lt;/code&gt;
      &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;- item 4&lt;/code&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Indentation Rules : YAML uses indentation to indicate content scope. Scope determines which
block each element belongs to. There are a few rules:
    &lt;ul&gt;
      &lt;li&gt;Only spaces can be used for indentation.&lt;/li&gt;
      &lt;li&gt;The amount of indentation does not matter as long as
  – All peer elements (in the same scope) have the same amount of indentation.
  – Any child elements are further indented.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;These documents are equivalent:&lt;/p&gt;

&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;top-level:
 second-level: # three spaces
 third-level: # two more spaces
 - &quot;list item&quot; # single additional indent on items in this list
 another-third-level: # a third-level peer with the same two spaces
 fourth-level: &quot;string scalar&quot; # 6 more spaces
 another-second-level: # a 2nd level peer with three spaces
 - a list item # list items in this scope have
 # 15 total leading spaces
 - a peer item # A peer list item with a gap in the list

---
# every scope level adds exactly 1 space
top-level:
 second-level:
 third-level:
 - &quot;list item&quot;
 another-third-level:
 fourth-level: &quot;string scalar&quot;
 another-second-level:
 - a list item
 - a peer item
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;92-docker-compose-basics&quot;&gt;9.2 Docker Compose Basics&lt;/h3&gt;

&lt;p&gt;By using the following command you can start up your application:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# first install docker-compose on your system (eg: Ubuntu )&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;docker-compose
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker-compose &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; &amp;lt;docker-compose-file&amp;gt; up
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can also run docker-compose in detached mode using -d flag, then you can stop it whenever needed by the following command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker-compose stop
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You can bring everything down, removing the containers entirely, with the down command. Pass &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--volumes&lt;/code&gt; to also remove the data volume.&lt;/p&gt;

&lt;p&gt;Let understand Docker compose with an Example :&lt;/p&gt;

&lt;h4 id=&quot;921-example-yml&quot;&gt;9.2.1 Example yml&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# wikijs.yml&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
version: &lt;span class=&quot;s1&quot;&gt;'2'&lt;/span&gt;
services:
  db:
    image: postgres:11-alpine
    environment:
      POSTGRES_DB: wiki
      POSTGRES_PASSWORD: wikijsrocks
      POSTGRES_USER: wikijs
    logging:
      driver: &lt;span class=&quot;s2&quot;&gt;&quot;none&quot;&lt;/span&gt;
    restart: unless-stopped
    volumes:
      - db-data:/var/lib/postgresql/data

  wiki:
    image: requarks/wiki:2
    depends_on:
      - db
    environment:
      DB_TYPE: postgres
      DB_HOST: db
      DB_PORT: 5432
      DB_USER: wikijs
      DB_PASS: wikijsrocks
      DB_NAME: wiki
    restart: unless-stopped
    ports:
      - &lt;span class=&quot;s2&quot;&gt;&quot;80:3000&quot;&lt;/span&gt;

volumes:
  db-data:
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# we can create a docker stack with this yml file ( it established a wikijs and postgresql db )&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker stack deploy &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; wikijs.yml wikijs

&lt;span class=&quot;c&quot;&gt;# if we use docker swarn or Kubernetes we can create a replicas of this images into 3 diff servers&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt;
deploy:
 replicas: 3
&lt;span class=&quot;nt&quot;&gt;--&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# add this to end of yml and again deploy it. It will update the container&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# To Check The State of stack&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker stack ps &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--format&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'\t'&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 wikijs

&lt;span class=&quot;c&quot;&gt;# To remove a service from stack ( like limit the replicas from 3 to 2 )&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker stack deploy &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; wikijs.yml &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--prune&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
wikijs


We Need To use Prue Here : Because without Prune it will not completely remove services which causes problmes &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; like &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;Instead of using postgressql we want to use a mysql so &lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;we updated our stack yml file and deploy it. it will add mysql db but doesnt remove postgres containers so to remove postgres we use prune &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

The &lt;span class=&quot;nt&quot;&gt;--prune&lt;/span&gt; flag will clean up any resource &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;the stack that isn’t explicitly referenced
&lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;the Compose file used &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;the deploy operation.

&lt;span class=&quot;c&quot;&gt;# Prune&lt;/span&gt;

The new &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;Data Management Commands]&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;https://github.com/docker/docker/pull/26108&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; have landed as of Docker 1.13:

&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;docker system prune&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;docker volume prune&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;docker network prune&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;docker container prune&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;docker image prune&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;


Note : There is a problem here everytime a container replaced docker will create a new volume space &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;container and its replicas. This would cause problems &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;a real-world system.
So, to get rid of this

EX:
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
volumes:
   pgdata: &lt;span class=&quot;c&quot;&gt;# empty definition uses volume defaults&lt;/span&gt;
services:
   postgres:
      image: dockerinaction/postgres:11-alpine
      volumes:
         - &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;: volume
         &lt;span class=&quot;nb&quot;&gt;source&lt;/span&gt;: pgdata &lt;span class=&quot;c&quot;&gt;# The named volume above&lt;/span&gt;
         target: /var/lib/postgresql/data
      environment:
         POSTGRES_PASSWORD: example
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

The file defines a volume named pgdata, and the postgres service
mounts that volume at /var/lib/postgresql/data. That location is where the Postgre￾SQL software will store any database schema or data.

Inspect
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker stack deploy &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; databases.yml &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--prune&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
my-databases

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker volume &lt;span class=&quot;nb&quot;&gt;ls
&lt;/span&gt;DRIVER  VOLUME NAME
&lt;span class=&quot;nb&quot;&gt;local   &lt;/span&gt;my-databases_pgdata

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker service remove my-databases_postgres

Then restore the service by using the Compose file:
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker stack deploy &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; databases.yml &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--prune&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
my-databases
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;100-devops-operations-with-docker&quot;&gt;10.0 DevOps Operations With Docker&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;101-convert-your-virtual-box-vm-to-docker-container&quot;&gt;10.1 Convert Your Virtual Box VM To Docker Container&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Process : VM FILE (.vdi or anything) =&amp;gt; TAR =&amp;gt; Import TAR as an Image in Docker.&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;qemu-utils

&lt;span class=&quot;c&quot;&gt;# Identify the path to your VM disk image. ( Stop the VM )&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Sets up a variable pointingto your VM disk image.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ VMDISK&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/VirtualBox VMs/myvm/myvm.vdi&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Initializes a kernelmodule requiredby qemu-nbd&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;modprobe nbd

&lt;span class=&quot;c&quot;&gt;# Connects the VM disk to a virtual device node&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;qemu-nbd &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; /dev/nbd0 &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$VMDISK3&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;CO1-3&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Lists the partition numbers available to mount on this disk&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; /dev/nbd0p&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; /dev/nbd0p1 /dev/nbd0p2

&lt;span class=&quot;c&quot;&gt;# Mounts the selected partition at /mnt with qemu-nbd&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;mount /dev/nbd0p2 /mnt

&lt;span class=&quot;c&quot;&gt;# Creates a TAR filecalled img.tar from /mnt&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo tar &lt;/span&gt;cf img.tar &lt;span class=&quot;nt&quot;&gt;-C&lt;/span&gt; /mnt &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Unmounts and cleans up after qemu-nbd&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;umount /mnt &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;qemu-nbd &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; /dev/nbd0

&lt;span class=&quot;c&quot;&gt;# Dockerfile&lt;/span&gt;
FROM scratch
ADD img.tar /

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;102-host-like-container&quot;&gt;10.2 Host Like Container&lt;/h3&gt;

&lt;p&gt;Containers are not virtual machines—there are significant differences—and pretending there aren’t can cause confusion and issues down the line.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Differences between VMs and Docker containers:
    &lt;ul&gt;
      &lt;li&gt;Docker is application-oriented, whereas VMs are operating-system oriented.&lt;/li&gt;
      &lt;li&gt;Docker  containers  share  an  operating  system  with  other  Docker  containers.  Incontrast, VMs each have their own operating system managed by a hypervisor.&lt;/li&gt;
      &lt;li&gt;Docker containers are designed to run one principal process, not manage mul-tiple sets of processes.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; phusion/baseimage &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; This Image designed to run multiple processes.]
docker &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-i&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; container_ID /bin/bash
ps &lt;span class=&quot;nt&quot;&gt;-ef&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; It starts &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;cron, sshd, and syslog&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt; It Much like a host. &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;103-running-gui-in-containers&quot;&gt;10.3 Running GUI in Containers&lt;/h3&gt;

&lt;p&gt;Process : Create  an  image  with  your  user  credentials  and  the  program,  and  bind  mount  your Xserver to it.
Note : another method is to setup VNC server on container&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Dockerfile for setting up firefox on container&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
FROM ubuntu:14.04
RUN apt-get update
RUN apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; firefox
RUN groupadd &lt;span class=&quot;nt&quot;&gt;-g&lt;/span&gt; GID USERNAME
RUN useradd &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; /home/USERNAME &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; /bin/bash &lt;span class=&quot;se&quot;&gt;\-&lt;/span&gt;m USERNAME &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; UID &lt;span class=&quot;nt&quot;&gt;-g&lt;/span&gt; GID
USER USERNAME
ENV HOME /home/USERNAME
CMD /usr/bin/firefox
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; gui &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /tmp/.X11-unix:/tmp/.X11-unix &lt;span class=&quot;nt&quot;&gt;-h&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOSTNAME&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$HOME&lt;/span&gt;/.Xauthority:/home/&lt;span class=&quot;nv&quot;&gt;$USER&lt;/span&gt;/.Xauthority &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;DISPLAY&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$DISPLAY&lt;/span&gt; gui

&lt;span class=&quot;c&quot;&gt;# It will popup firefox ( which runs on container )&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;104-using-docker-machine-to-provision-docker-hosts&quot;&gt;10.4 Using Docker Machine to Provision Docker Hosts&lt;/h3&gt;

&lt;p&gt;Docker-machine is a tool just like a vagrant. EX: we can setup VM with virtualBox with docker command.
walkthrough:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Install Docker Machine on Linux&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;-L&lt;/span&gt; https://github.com/docker/machine/releases/download/v0.16.2/docker-machine-&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;uname&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;-&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;uname&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt;/tmp/docker-machine &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;chmod&lt;/span&gt; +x /tmp/docker-machine &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;sudo cp&lt;/span&gt; /tmp/docker-machine /usr/local/bin/docker-machine

&lt;span class=&quot;c&quot;&gt;# Create a VM by docker daemon on Oracle Virtual Box&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker-machine create &lt;span class=&quot;nt&quot;&gt;--driver&lt;/span&gt; virtualbox host1

&lt;span class=&quot;c&quot;&gt;# Run this command to set the DOCKER_HOST environment variable, which sets the default host that Docker commands will be run on&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker-machine &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;host1
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;eval&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;docker-machine &lt;span class=&quot;nb&quot;&gt;env &lt;/span&gt;host1&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# we can direct to VM&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker-machine ssh host1

Commands of Docker-Machine:
create  : Creates a new Machine
&lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt;      : List Machines
stop    : Stop Machines
start   : Start Machines
restart : Restart Machines
&lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt;      : Destroys the machine
inspect : Returns a JSON representation of the machine’s metadata
config  : Return the config of machine
ip      : Returns the IP address of the machine
url     : Returns a URL &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;the Docker daemon on the machine
upgrade : Upgrades the Docker version on the host to the latest
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;105-build-images-using-a-chef-solo&quot;&gt;10.5 Build Images using a Chef Solo&lt;/h3&gt;

&lt;p&gt;Chef : It is a configuration Management Tool ( using this can reduce the amount of work required to configure Images )&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Here we setup hello world apache website ( Example ).&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Need a working code&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git clone https://github.com/docker-in-practice/docker-chef-solo-example.git
&lt;span class=&quot;c&quot;&gt;# All the working code in there&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;into it &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; there is a Dockerfile and some other files and folders &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; chef recepies etc &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; chef-example &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8080:80 chef-example
&lt;span class=&quot;c&quot;&gt;# This is a one time written code we can anywhere to deploy a website ( in such a small steps )&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;106-ci-operations-with-docker&quot;&gt;10.6 CI Operations With Docker&lt;/h3&gt;

&lt;p&gt;CI : Continious Integration&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Docker/Docker_Hard_Parts/CIoperations.png&quot; alt=&quot;Image_pipeline.png&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;1061-steps&quot;&gt;10.6.1 Steps&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Check out a clean copy of the source code defining the image and build scripts so the origin and process used to build the image is known.&lt;/li&gt;
  &lt;li&gt;Retrieve or generate artifacts that will be included in the image, such as the application package and runtime libraries.&lt;/li&gt;
  &lt;li&gt;Build the image by using a Dockerfile.&lt;/li&gt;
  &lt;li&gt;Verify that the image is structured and functions as intended.&lt;/li&gt;
  &lt;li&gt;(Optional) Verify that the image does not contain known vulnerabilities.&lt;/li&gt;
  &lt;li&gt;Tag the image so that it can be consumed easily.&lt;/li&gt;
  &lt;li&gt;Publish the image to a registry or another distribution channel.&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;1062-method-1--builds-a-image-using-dockerhub-workflow--test-and-push-images-&quot;&gt;10.6.2 Method 1 : Builds a Image Using DockerHub Workflow ( Test and Push Images )&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# For this you will Git Repo and docker Hub Repo&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Link Docker Hub to to git repo ( it take code from git repo and compile and create a desired Image ( like other ci tools do )&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# wait for the docker hub to build to complete&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Remember this is a basic solution&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;1063-method-2--setting-up-a-package-cache-for-faster-builds&quot;&gt;10.6.3 Method 2 : Setting up a package cache for faster Builds&lt;/h4&gt;

&lt;p&gt;While building the images it will take caches instead of download everytime from internet.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# We are using squid Proxy Here&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;squid-deb-proxy
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;check &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;port 8000
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;create a Docker File

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt; using a apt proxy
FROM debian
RUN apt-get update &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;net-tools
RUN &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Acquire::http::Proxy &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;http://&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
route &lt;span class=&quot;nt&quot;&gt;-n&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;awk&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'/^0.0.0.0/ {print $2}'&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:8000&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&quot;&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; /etc/apt/apt.conf.d/30proxy
RUN &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Acquire::http::Proxy::ppa.launchpad.net DIRECT;&quot;&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&amp;gt;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
/etc/apt/apt.conf.d/30proxy
CMD &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/bin/bash&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

This will cache all the webpages and apt package we downlaod after running this container : &lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;again download them they will be download &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;miliseconds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h5 id=&quot;1063-method-3--running-the-jenkins-master-withing-the-docker-container&quot;&gt;10.6.3 Method 3 : Running the Jenkins Master Withing the Docker Container&lt;/h5&gt;

&lt;p&gt;Portable Jenkins Server&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# download git clone https://github.com/docker-in-practice/jenkins.git.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Put your required plugins in jenkins_plugins.txt&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt; Dockerfile
FROM jenkins
COPY jenkins_plugins.txt /tmp/jenkins_plugins.txt
RUN /usr/local/bin/plugins.sh /tmp/jenkins_plugins.txt
USER root
RUN &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; /tmp/jenkins_plugins.txt
RUN groupadd &lt;span class=&quot;nt&quot;&gt;-g&lt;/span&gt; 999 docker
RUN addgroup &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; jenkins docker
USER jenkins
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; jenkins &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; jenkins &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8080:8080 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 50000:50000 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /var/run/docker.sock:/var/run/docker.sock &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /tmp:/var/jenkins_home &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
jenkins

&lt;span class=&quot;c&quot;&gt;# go to localhost:8080 : Copy master password from logs&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Reliably Upgrade a Jenkins Server&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt; Dockerfile
FROM docker
ADD jenkins_updater.sh /jenkins_updater.sh
RUN &lt;span class=&quot;nb&quot;&gt;chmod&lt;/span&gt; +x /jenkins_updater.sh
ENTRYPOINT /jenkins_updater.sh
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt; Shell script to backup and restart Jenkins
&lt;span class=&quot;c&quot;&gt;#!/bin/sh&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;set&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-x&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;!&lt;/span&gt; docker pull jenkins | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;up.to.date
&lt;span class=&quot;k&quot;&gt;then
&lt;/span&gt;docker stop jenkins
docker rename jenkins jenkins.bak.&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt; +%Y%m%d%H%M&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;cp&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; /var/docker/mounts/jenkins_home &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
/var/docker/mounts/jenkins_home.bak.&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt; +%Y%m%d%H%M&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;

docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--restart&lt;/span&gt; always &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /var/docker/mounts/jenkins_home:/var/jenkins_home &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; jenkins &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8080:8080 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
jenkins
&lt;span class=&quot;k&quot;&gt;fi&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# docker Command to run the Jenkins Updater&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /var/lib/docker:/var/lib/docker &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /var/run/docker.sock:/var/run/docker.sock &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt;/var/docker/mounts:/var/docker/mounts dockerinpractice/jenkins-updater

&lt;span class=&quot;c&quot;&gt;# to automate the process add this command to crontab&lt;/span&gt;
0 &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; dokcker_command
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;107-cd-operations-with-docker&quot;&gt;10.7 CD Operations with Docker&lt;/h3&gt;

&lt;p&gt;CD : Continious Delivery ( CI + Deployment )&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c&quot;&gt;# Copy an Image Between Two Registries&lt;/span&gt;
Process &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; Pulling the image from the registry -&amp;gt; retag -&amp;gt; pushing the new Image
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker tag &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$OLDREG&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;$MYIMAGE&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$NEWREG&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;$MYIMAGE&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker push &lt;span class=&quot;nv&quot;&gt;$NEWREG&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;$MYIMAGE&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker rmi &lt;span class=&quot;nv&quot;&gt;$OLDREG&lt;/span&gt;/&lt;span class=&quot;nv&quot;&gt;$MYIMAGE&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker image prune &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Copy an Image btw Two Machine With a very low-bandwidth Connection&lt;/span&gt;

Process &lt;span class=&quot;o&quot;&gt;=&amp;gt;&lt;/span&gt; Here we use backup tool called Bup &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; creates a bup data tool &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Example ( Not Exact data is used so please Don't judge me )&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# pull two images like Ubuntu:18.04 &amp;amp; 19.10 ( Both are example = 65 MB Each = 130 MB )&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir &lt;/span&gt;bup_pool
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;dbup&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;docker run --rm &lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;
-v &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/bup_pool:/pool -v /var/run/docker.sock:/var/run/docker.sock dockerinpractice/dbup&quot;&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;dbup save ubuntu:18.04
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;du&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-sh&lt;/span&gt; bup_pool &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; 74 MB &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;dbup save ubuntu:19.10
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;du&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-sh&lt;/span&gt; bup_pool &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; 96 MB &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; Saves 35 MB &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# On other machine ( rsync from host1 to host2&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;dbup load ubuntu:18.04

&lt;span class=&quot;c&quot;&gt;# Also copies files between host with TAR&lt;/span&gt;
Docker &lt;span class=&quot;nb&quot;&gt;export&lt;/span&gt; : creates Container To TAR
Docker Import : TAR to Image
Docker save   : Image To TAR
Docker load   : TAR to Docker Image

Example : Transfer docker Image directory over ssh
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;export&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt;  debian:7.3 &lt;span class=&quot;nb&quot;&gt;true&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt; | ssh user@host docker import
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;108-cordination-between-containers&quot;&gt;10.8 Cordination Between Containers&lt;/h3&gt;

&lt;p&gt;We need coordination between containers : Like if we take an example of one server and one python based echo client.
P1: If we start the client container first : It will lead to failure
P2 : forgetting to remove the containers will result in
problems when you try to restart
P3 :  Naming containers incorrectly will result
in failure.
So how to get rid of this types of problem : we need a solution,where we can run the container without any problem.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;c&quot;&gt;# Solution : create a compose file&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
version: &lt;span class=&quot;s2&quot;&gt;&quot;3&quot;&lt;/span&gt;
   services:
     echo-server:
       image: server
       expose:
         - &lt;span class=&quot;s2&quot;&gt;&quot;2000&quot;&lt;/span&gt;
     client:
       image: client
       links:
- echo-server:talkto
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# with this we can start container in correct order &amp;amp; also we call rebuild the container anywhere&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker-compose up
Attaching to dockercompose_server_1, dockercompose_client_1
client_1 | Received: Hello, world
client_1 |
client_1 | Received: Hello, world
client_1 |

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;110-security-with-docker&quot;&gt;11.0 Security With Docker&lt;/h2&gt;

&lt;hr /&gt;
&lt;p&gt;This is where security tips about Docker go. The Docker &lt;a href=&quot;https://docs.docker.com/engine/security/security/&quot;&gt;security&lt;/a&gt; page goes into more detail.&lt;/p&gt;

&lt;p&gt;First things first: Docker runs as root. If you are in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker&lt;/code&gt; group, you effectively &lt;a href=&quot;https://web.archive.org/web/20161226211755/http://reventlov.com/advisories/using-the-docker-command-to-root-the-host&quot;&gt;have root access&lt;/a&gt;. If you expose the docker unix socket to a container, you are giving the container &lt;a href=&quot;https://www.lvh.io/posts/dont-expose-the-docker-socket-not-even-to-a-container/&quot;&gt;root access to the host&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Docker should not be your only defense. You should secure and harden it.&lt;/p&gt;

&lt;p&gt;For an understanding of what containers leave exposed, you should read &lt;a href=&quot;https://www.nccgroup.trust/globalassets/our-research/us/whitepapers/2016/april/ncc_group_understanding_hardening_linux_containers-1-1.pdf&quot;&gt;Understanding and Hardening Linux Containers&lt;/a&gt; by &lt;a href=&quot;https://twitter.com/dyn___&quot;&gt;Aaron Grattafiori&lt;/a&gt;. This is a complete and comprehensive guide to the issues involved with containers, with a plethora of links and footnotes leading on to yet more useful content. The security tips following are useful if you’ve already hardened containers in the past, but are not a substitute for understanding.&lt;/p&gt;

&lt;h3 id=&quot;111-security-tips&quot;&gt;11.1 Security Tips&lt;/h3&gt;

&lt;p&gt;For greatest security, you want to run Docker inside a virtual machine. This is straight from the Docker Security Team Lead – &lt;a href=&quot;http://www.slideshare.net/jpetazzo/linux-containers-lxc-docker-and-security&quot;&gt;slides&lt;/a&gt; / &lt;a href=&quot;http://www.projectatomic.io/blog/2014/08/is-it-safe-a-look-at-docker-and-security-from-linuxcon/&quot;&gt;notes&lt;/a&gt;. Then, run with AppArmor / seccomp / SELinux / grsec etc to &lt;a href=&quot;http://linux-audit.com/docker-security-best-practices-for-your-vessel-and-containers/&quot;&gt;limit the container permissions&lt;/a&gt;. See the &lt;a href=&quot;https://blog.docker.com/2016/02/docker-engine-1-10-security/&quot;&gt;Docker 1.10 security features&lt;/a&gt; for more details.&lt;/p&gt;

&lt;p&gt;Docker image ids are &lt;a href=&quot;https://medium.com/@quayio/your-docker-image-ids-are-secrets-and-its-time-you-treated-them-that-way-f55e9f14c1a4&quot;&gt;sensitive information&lt;/a&gt; and should not be exposed to the outside world. Treat them like passwords.&lt;/p&gt;

&lt;p&gt;See the &lt;a href=&quot;https://github.com/konstruktoid/Docker/blob/master/Security/CheatSheet.adoc&quot;&gt;Docker Security Cheat Sheet&lt;/a&gt; by &lt;a href=&quot;https://github.com/konstruktoid&quot;&gt;Thomas Sjögren&lt;/a&gt;: some good stuff about container hardening in there.&lt;/p&gt;

&lt;p&gt;Check out the &lt;a href=&quot;https://github.com/docker/docker-bench-security&quot;&gt;docker bench security script&lt;/a&gt;, download the &lt;a href=&quot;https://blog.docker.com/2015/05/understanding-docker-security-and-best-practices/&quot;&gt;white papers&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Snyk’s &lt;a href=&quot;https://snyk.io/blog/10-docker-image-security-best-practices/&quot;&gt;10 Docker Image Security Best Practices cheat sheet&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;You should start off by using a kernel with unstable patches for grsecurity / pax compiled in, such as &lt;a href=&quot;https://en.wikipedia.org/wiki/Alpine_Linux&quot;&gt;Alpine Linux&lt;/a&gt;. If you are using grsecurity in production, you should spring for &lt;a href=&quot;https://grsecurity.net/business_support.php&quot;&gt;commercial support&lt;/a&gt; for the &lt;a href=&quot;https://grsecurity.net/announce.php&quot;&gt;stable patches&lt;/a&gt;, same as you would do for RedHat. It’s $200 a month, which is nothing to your devops budget.&lt;/p&gt;

&lt;p&gt;Since docker 1.11 you can easily limit the number of active processes running inside a container to prevent fork bombs. This requires a linux kernel &amp;gt;= 4.3 with CGROUP_PIDS=y to be in the kernel configuration.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run &lt;span class=&quot;nt&quot;&gt;--pids-limit&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;64
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Also available since docker 1.11 is the ability to prevent processes from gaining new privileges. This feature have been in the linux kernel since version 3.5. You can read more about it in &lt;a href=&quot;http://www.projectatomic.io/blog/2016/03/no-new-privs-docker/&quot;&gt;this&lt;/a&gt; blog post.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run &lt;span class=&quot;nt&quot;&gt;--security-opt&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;no-new-privileges
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;From the &lt;a href=&quot;http://container-solutions.com/content/uploads/2015/06/15.06.15_DockerCheatSheet_A2.pdf&quot;&gt;Docker Security Cheat Sheet&lt;/a&gt; (it’s in PDF which makes it hard to use, so copying below) by &lt;a href=&quot;http://container-solutions.com/is-docker-safe-for-production/&quot;&gt;Container Solutions&lt;/a&gt;:&lt;/p&gt;

&lt;p&gt;Turn off interprocess communication with:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--icc&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;false&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--iptables&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Set the container to be read-only:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run &lt;span class=&quot;nt&quot;&gt;--read-only&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Verify images with a hashsum:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker pull debian@sha256:a25306f3850e1bd44541976aa7b5fd0a29be
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Set volumes to be read only:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;/secrets:/secrets:ro debian
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Define and run a user in your Dockerfile so you don’t run as root inside the container:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;RUN groupadd &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; user &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; useradd &lt;span class=&quot;nt&quot;&gt;-r&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-g&lt;/span&gt; user user
USER user
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;112-security-videos&quot;&gt;11.2 Security Videos&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/04LOuMgNj9U&quot;&gt;Using Docker Safely&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/KmxOXmPhZbk&quot;&gt;Securing your applications using Docker&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://youtu.be/a9lE9Urr6AQ&quot;&gt;Container security: Do containers actually contain?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=iN6QbszB1R8&quot;&gt;Linux Containers: Future or Fantasy?&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;113-security-roadmap&quot;&gt;11.3 Security Roadmap&lt;/h3&gt;

&lt;p&gt;The Docker roadmap talks about &lt;a href=&quot;https://github.com/docker/docker/blob/master/ROADMAP.md#11-security&quot;&gt;seccomp support&lt;/a&gt;.
There is an AppArmor policy generator called &lt;a href=&quot;https://github.com/jfrazelle/bane&quot;&gt;bane&lt;/a&gt;, and they’re working on &lt;a href=&quot;https://github.com/docker/docker/issues/17142&quot;&gt;security profiles&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Till 21 Aprill&lt;/p&gt;

&lt;h3 id=&quot;path-to-complete&quot;&gt;Path to Complete&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Docker access&lt;/li&gt;
  &lt;li&gt;Security Measures In Docker&lt;/li&gt;
  &lt;li&gt;Securing Access to Docker&lt;/li&gt;
  &lt;li&gt;Security from Outside docker&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;120-monitoring-docker&quot;&gt;12.0 Monitoring Docker&lt;/h2&gt;

&lt;h3 id=&quot;121-monitoring&quot;&gt;12.1 Monitoring&lt;/h3&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;122-resource-control&quot;&gt;12.2 Resource Control&lt;/h3&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;123-some-advance-approches&quot;&gt;12.3 Some Advance Approches&lt;/h3&gt;</content><author><name></name></author><category term="Docker" /><summary type="html">7.0 Building Images automatically with DockerFiles</summary></entry><entry><title type="html">Docker Easy Parts [Part - 1]</title><link href="https://hacstac.github.io/Notes/docker/2020/08/29/Docker-Easy-Parts.html" rel="alternate" type="text/html" title="Docker Easy Parts [Part - 1]" /><published>2020-08-29T00:00:00-05:00</published><updated>2020-08-29T00:00:00-05:00</updated><id>https://hacstac.github.io/Notes/docker/2020/08/29/Docker-Easy-Parts</id><content type="html" xml:base="https://hacstac.github.io/Notes/docker/2020/08/29/Docker-Easy-Parts.html">&lt;h2 id=&quot;10-what-is-a-docker&quot;&gt;1.0 What is a Docker&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;11-introduction&quot;&gt;1.1 Introduction&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Defination&lt;/strong&gt; : Docker is a tool designed to make it easier to create, deploy, and run applications by using containers. Containers allow a developer to package up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package.&lt;/p&gt;

&lt;p&gt;Docker container technology was launched in 2013 as an open source Docker Engine. Containers encapsulate an application as a single executable package of software that bundles application code together with all of the related configuration files, libraries, and dependencies required for it to run.&lt;/p&gt;

&lt;p&gt;Containerized applications are “isolated” in that they do not bundle in a copy of the operating system. Instead, an open source Docker engine is installed on the host’s operating system and becomes the conduit for containers to share an operating system with other containers on the same computing system.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Docker/Docker_easy_parts/docker.png&quot; alt=&quot;Docker&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Docker/Docker_easy_parts/life_with_docker.png&quot; alt=&quot;Life With Docker&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;12-below-is-a-list-of-common-docker-terms&quot;&gt;1.2 Below is a list of common Docker terms&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Docker Engine&lt;/strong&gt; is a client-server application with 3 major components - a server which is a type of long-running program called a daemon process; a REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do; a command line interface (CLI) client.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Docker daemon&lt;/strong&gt; (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Docker client&lt;/strong&gt; (docker) is the primary way that many Docker users interact with Docker. When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Image&lt;/strong&gt; is a read-only template with instructions for creating a Docker container. You might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Docker registry&lt;/strong&gt; stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Container&lt;/strong&gt; is a runnable instance of an image. Containers are made possible by operating system (OS) process isolation and virtualization, which enable multiple application components to share the resources of a single instance of an OS kernel.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;13-architecture-of-docker&quot;&gt;1.3 Architecture of docker&lt;/h3&gt;

&lt;p&gt;Docker on your host machine is (at the time of writing) split into two parts—a daemon with a RESTful API and a client that talks to the daemon. You invoke the Docker client to get information from or give instructions to the
daemon; the daemon is a server that receives requests and returns responses from the client using the HTTP protocol. In turn, it will make requests to other services to send and receive images, also using the HTTP protocol. The server will accept requests from the command-line client or anyone else authorized to connect. The daemon is also responsible for taking care of your images and containers behind the scenes, whereasthe client acts as the intermediary between you and the RESTful API.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Docker/Docker_easy_parts/Docker_architecture.png&quot; alt=&quot;Docker Architecture&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Docker/Docker_easy_parts/working_docker_run.png&quot; alt=&quot;working_docker_run&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;20-installation&quot;&gt;2.0 Installation&lt;/h2&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Docker&lt;/strong&gt; can operate on most of the Operating Systems In Industries : Windows, MacOS, Most Of flavours of Linux ( Ubuntu, RHEL, Arch)&lt;/li&gt;
  &lt;li&gt;Docker Operates on both AMD64 and ARM Based Systems&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Simple script to install docker on Linux&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl &lt;span class=&quot;nt&quot;&gt;-sSL&lt;/span&gt; https://get.docker.com/ | sh

&lt;span class=&quot;c&quot;&gt;# Check Docker Verison&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker version &lt;span class=&quot;nt&quot;&gt;--format&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt;
19.03.8

&lt;span class=&quot;c&quot;&gt;# Dump Raw JSON DATA : Like Kernel, Architecture , details , build time etc&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker version &lt;span class=&quot;nt&quot;&gt;--format&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;''&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Running Docker without sudo&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;usermod &lt;span class=&quot;nt&quot;&gt;-aG&lt;/span&gt; docker username

or

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;addgroup &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; username docker
&lt;span class=&quot;c&quot;&gt;# restart docker&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;30-docker-basics&quot;&gt;3.0 Docker Basics&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;31-day-to-day-docker-commands&quot;&gt;3.1 Day To Day Docker Commands&lt;/h3&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker create&lt;/code&gt; creates a container but does not start it.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker rename&lt;/code&gt; allows the container to be renamed.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker run&lt;/code&gt; creates and starts a container in one operation.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker rm&lt;/code&gt; deletes a container.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker update&lt;/code&gt; updates a container’s resource limits.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker images&lt;/code&gt; shows all images.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker cp&lt;/code&gt; copies files or folders between a container and the local filesystem.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker build&lt;/code&gt; creates image from Dockerfile.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker commit&lt;/code&gt; creates image from a container, pausing it temporarily if it is running.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker rmi&lt;/code&gt; removes an image.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker start&lt;/code&gt; starts a container so it is running.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker stop&lt;/code&gt; stops a running container.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker restart&lt;/code&gt; stops and starts a container.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker pause&lt;/code&gt; pauses a running container, “freezing” it in place.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker unpause&lt;/code&gt; will unpause a running container.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker wait&lt;/code&gt; blocks until running container stops.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker kill&lt;/code&gt; sends a SIGKILL to a running container.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker attach&lt;/code&gt; will connect to a running container.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker ps&lt;/code&gt; shows running containers.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker logs&lt;/code&gt; gets logs from container. (You can use a custom log driver, but logs is only available for json-file and journald in 1.10).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker inspect&lt;/code&gt; looks at all the info on a container (including IP address).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker events&lt;/code&gt; gets events from container.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker port&lt;/code&gt; shows public facing port of container.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker top&lt;/code&gt; shows running processes in container.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker stats&lt;/code&gt; shows containers’ resource usage statistics.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker diff&lt;/code&gt; shows changed files in the container’s FS.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker history&lt;/code&gt; shows history of image.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker tag&lt;/code&gt; tags an image to a name (local or registry).&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;32-getting-practical&quot;&gt;3.2 Getting Practical&lt;/h3&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;321-list-images&quot;&gt;3.2.1 List Images&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker images // show images
docker ps &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt;
docker ps // shows started containers &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; all containers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;322-startstoprestart&quot;&gt;3.2.2 Start/Stop/Restart&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker stop/start/restart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; : If you want to detach from a running container, use Ctrl + P, Ctrl + Q.
If you want to integrate a container with a host process manager, start the daemon with -r=false then use docker start -a.&lt;/p&gt;

&lt;h4 id=&quot;323-logs&quot;&gt;3.2.3 Logs&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker logs &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;Name_of_container&lt;span class=&quot;o&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;324-rename&quot;&gt;3.2.4 Rename&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker rename new_name current_name // rename container
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;325-create-container&quot;&gt;3.2.5 Create container&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker create nginx // will only create a container
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;326-example-run&quot;&gt;3.2.6 Example Run&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--interactive&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--tty&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--link&lt;/span&gt; web:web &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; web_test &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 busybox:1.29 /bin/sh

 &lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; detach automatically the container &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;run container &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;background and print container ID&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--interactive&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;--tty&lt;/span&gt; or &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; that will allocate a pseudo-TTY session
 &lt;span class=&quot;nt&quot;&gt;--link&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;link &lt;/span&gt;to other container
 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; name of docker container
 &lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;web_test ps // show extra process run with this containers
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;327-docker-run-with-shell-variables&quot;&gt;3.2.7 Docker Run with SHELL Variables&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
 &lt;span class=&quot;nv&quot;&gt;CID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;docker create nginx:latest&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$CID&lt;/span&gt; // assigns to a Shell variable
 &lt;span class=&quot;nv&quot;&gt;MAILER_CID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; dockerinaction/ch2_mailer&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
 &lt;span class=&quot;nv&quot;&gt;WEB_CID&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;docker create nginx&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker start &lt;span class=&quot;nv&quot;&gt;$AGENT_CID&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker start &lt;span class=&quot;nv&quot;&gt;$WEB_CID&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;328-env-variables&quot;&gt;3.2.8 Env Variables&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; wpdb &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;MYSQL_ROOT_PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;ch2demo &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 mysql:5.7

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--env&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;MY_ENVIRONMENT_VAR&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;this is a test&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt;busybox:1.29 &lt;span class=&quot;se&quot;&gt;\ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;env&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# --env flag, or -e for short, can be used to inject any environment variable.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;329-with-read-only-option&quot;&gt;3.2.9 With Read Only Option&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; wp &lt;span class=&quot;nt&quot;&gt;--read-only&lt;/span&gt; wordpress:5.0.0-php7.2-apache  // create a container with only &lt;span class=&quot;nb&quot;&gt;readonly &lt;/span&gt;options
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3210-inspect&quot;&gt;3.2.10 Inspect&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;#  The docker inspect command will display all the metadata(JSON)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker inspect &lt;span class=&quot;nt&quot;&gt;--format&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt; wp // Prints &lt;span class=&quot;nb&quot;&gt;true &lt;/span&gt;&lt;span class=&quot;k&quot;&gt;if &lt;/span&gt;container is running
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker inspect &lt;span class=&quot;nt&quot;&gt;--format&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt; Id/name // Show ip of container
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3211-diff--filesystem-check-&quot;&gt;3.2.11 Diff ( Filesystem check )&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; wp_writable wordpress:5.0.0-php7.2-apache

&lt;span class=&quot;c&quot;&gt;# let’s check where Apache changed the container’s filesystem with the docker&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container diff wp_writable

A - A file or directory was added
D - A file or directory was deleted
C - A file or directory was changed

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
C /run
C /run/apache2
A /run/apache2/apache2.pid
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3212-clean-up&quot;&gt;3.2.12 Clean Up&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{&lt;/span&gt;container_ID&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
docker &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-vf&lt;/span&gt; &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;docker ps &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-q&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;
docker rmi &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;name] // Remove an image
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3213-executing-commands&quot;&gt;3.2.13 Executing Commands&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# docker exec to execute a command in container.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# To enter a running container, attach a new shell process to a running container called foo, use:&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; foo /bin/bash.

&lt;span class=&quot;c&quot;&gt;# exec Modes :&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 1 Basic : Runs the command in the container synchronously on the command line&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;name_of_container &lt;span class=&quot;nb&quot;&gt;echo&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hello User&quot;&lt;/span&gt;
Hello User

&lt;span class=&quot;c&quot;&gt;# 2 Daemon : Runs the command in the background on the container&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; name_C &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
find / &lt;span class=&quot;nt&quot;&gt;-ctime&lt;/span&gt; 7 &lt;span class=&quot;nt&quot;&gt;-name&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'*log'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-exec&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;{}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 3 Interactive : Runs the command and allows the user to interact with it&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;exec&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; ubuntu /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3215-linking-containers-for-port-isolation&quot;&gt;3.2.15 Linking containers for port isolation&lt;/h4&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; : This is an older method of declaring container communication—Docker’s link flag. This isn’t the recommended way of working anymore.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Example&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;#  This will allow us communication between containers without using user-defined networks.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; wp-mysql &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;MYSQL_ROOT_PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;yoursecretpassword &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; mysql
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; wordpress &lt;span class=&quot;nt&quot;&gt;--link&lt;/span&gt; wp-mysql:mysql &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 10003:80 &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt;  wordpress
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3216-search-a-docker-image&quot;&gt;3.2.16 Search a Docker Image&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker search node
docker pull node // Pull the Image by Name &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; on Hub &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; node /bin/bash &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; Start Node Container &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3217-cleanly-kill-containers&quot;&gt;3.2.17 Cleanly Kill Containers&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Always use docker stop ( it actually stops the containers ).&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Docker kill will send immediate signal which will kill process while running ( so they can create temp files )&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;nb&quot;&gt;kill           &lt;/span&gt;Term     15
docker &lt;span class=&quot;nb&quot;&gt;kill    &lt;/span&gt;Kill      9
docker stop    Term     15
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3218-docker-prune&quot;&gt;3.2.18 Docker Prune&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Prune commands&lt;/span&gt;

    docker system prune
    docker volume prune
    docker network prune
    docker container prune
    docker image prune

&lt;span class=&quot;c&quot;&gt;# Example&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Nuclear Option ( if you want to remove all containers of your host machine ) [Removes all : Runnig &amp;amp; exited]&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker ps &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-q&lt;/span&gt; | xargs &lt;span class=&quot;nt&quot;&gt;--no-run-if-empty&lt;/span&gt; docker &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# To keep running containers&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker ps &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-q&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--filter&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;exited | xargs &lt;span class=&quot;nt&quot;&gt;--no-run-if-empty&lt;/span&gt; docker &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# To list out all exited &amp;amp; failed Containers&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;comm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-3&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&amp;lt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;docker ps &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-q&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;status&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;exited | &lt;span class=&quot;nb&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&amp;lt;&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;docker ps &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-q&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--filter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;exited&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 | &lt;span class=&quot;nb&quot;&gt;sort&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; | &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
xargs &lt;span class=&quot;nt&quot;&gt;--no-run-if-empty&lt;/span&gt; docker inspect &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; error_containers

&lt;span class=&quot;c&quot;&gt;# Prune Volumes&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# List out all docker voluems&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker volume &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Delete Unused Volumes&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker volume prune
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3219-space-occupied-by-docker-system&quot;&gt;3.2.19 Space Occupied By docker System&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker system &lt;span class=&quot;nb&quot;&gt;df
&lt;/span&gt;TYPE                TOTAL               ACTIVE              SIZE                RECLAIMABLE
Images              7                   1                   1.963GB             1.885GB &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;95%&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Containers          1                   1                   0B                  0B
Local Volumes       2                   1                   242.4MB             242.3MB &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;99%&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
Build Cache         0                   0                   0B                  0B
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3220-container-stats&quot;&gt;3.2.20 Container Stats&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker stats ID
CONTAINER ID        NAME                CPU %               MEM USAGE / LIMIT    MEM %               NET I/O             BLOCK I/O           PIDS
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3221-tag&quot;&gt;3.2.21 Tag&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker image tag ubuntu-git:latest ubuntu-git:2.7
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3222-commit&quot;&gt;3.2.22 Commit&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container commit &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;@dockerinaction&quot;&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-m&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Added git&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  image-dev ubuntu-git
&lt;span class=&quot;c&quot;&gt;# Outputs a new unique image identifier like:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# bbf1d5d430cdf541a72ad74dfa54f6faec41d2c1e4200778e9d4302035e5d143&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Build a New Image For Commited Image&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; ubuntu-git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;3223-set-an-entrypoint&quot;&gt;3.2.23 Set an EntryPoint&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;docker container run &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; cmd-git &lt;span class=&quot;nt&quot;&gt;--entrypoint&lt;/span&gt; git ubuntu-git
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h4 id=&quot;3224-versioning-best-practice&quot;&gt;3.2.24 Versioning Best Practice&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Docker official Repo's are the best example of tagging an image&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Example for go lang&lt;/span&gt;
 1.x
 1.9
 1.9.6
 1.9-stretch
 1.10-alpine
 latest
 &lt;span class=&quot;c&quot;&gt;# this is an example of tags to build that don't confuse the end user&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;33-states-of-docker&quot;&gt;3.3 States of Docker&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Docker container can be in one of six states:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Created&lt;/li&gt;
  &lt;li&gt;Running&lt;/li&gt;
  &lt;li&gt;Restarting&lt;/li&gt;
  &lt;li&gt;Paused&lt;/li&gt;
  &lt;li&gt;Removing&lt;/li&gt;
  &lt;li&gt;Exited (also used if the container has never been started)&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Created&lt;/strong&gt; : A container that has been created (e.g. with docker create) but not started&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Running&lt;/strong&gt; : A currently running container&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Paused&lt;/strong&gt; : A container whose processes have been paused&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Exited&lt;/strong&gt; : A container that ran and completed (“stopped” in other contexts, although a created container is technically also “stopped”)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dead&lt;/strong&gt; : A container that the daemon tried and failed to stop (usually due to a busy device or resource used by the container)&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Restarting&lt;/strong&gt; : A container that is in the process of being restarted&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;331-restart-state&quot;&gt;3.3.1 Restart State&lt;/h4&gt;

&lt;p&gt;Using the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;--restart&lt;/code&gt; flag at container-creation time, you can tell Docker to do any of the following:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Never restart (default)&lt;/li&gt;
  &lt;li&gt;Attempt to restart when a failure is detected&lt;/li&gt;
  &lt;li&gt;Attempt for some predetermined time to restart when a failure is detected&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Always restart the container regardless of the condition&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;strong&gt;Methods&lt;/strong&gt;&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;no = = Don’t restart when the container exits&lt;/li&gt;
      &lt;li&gt;always == Always restart when the container exits&lt;/li&gt;
      &lt;li&gt;unless-stopped == Always restart, but remember explicitly stopping&lt;/li&gt;
      &lt;li&gt;on-failure[:max-retry] == Restart only on failure&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Example Run&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; backoff-detector &lt;span class=&quot;nt&quot;&gt;--restart&lt;/span&gt; always busybox:1.29 &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker logs &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; backoff-detector
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;34-init--pid-systems-for-docker&quot;&gt;3.4 Init &amp;amp; PID Systems for Docker&lt;/h3&gt;

&lt;p&gt;Several such init systems could be used inside a container. The most popular include &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;runit, Yelp/dumb-init, tini, supervisord, and tianon/gosu&lt;/code&gt; .&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 80:80 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; lamp-test tutum/lamp
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker top lamp-test
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;lamp-test ps
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;exec &lt;/span&gt;lamp-test &lt;span class=&quot;nb&quot;&gt;kill&lt;/span&gt; &amp;lt;PID&amp;gt; // &lt;span class=&quot;nb&quot;&gt;kill &lt;/span&gt;a process

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--entrypoint&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;cat&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 wordpress:5.0.0-php7.2-apache &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
 /usr/local/bin/docker-entrypoint.sh

&lt;span class=&quot;c&quot;&gt;# If you run through the displayed script, you’ll see how it validates the environment variables against the dependencies of the software and sets default values. Once the script has validated that WordPress can execute&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;35-software-installation-simplified&quot;&gt;3.5 Software Installation Simplified&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;Three main ways to install Docker images:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Using Docker registries&lt;/li&gt;
  &lt;li&gt;Using image files with docker save and docker load&lt;/li&gt;
  &lt;li&gt;Building images with Dockerfiles&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;we can install software in three other ways:&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;You can use alternative repository registries or run your own registry.&lt;/li&gt;
  &lt;li&gt;You can manually load images from a file.&lt;/li&gt;
  &lt;li&gt;You can download a project from some other source and build an image by using a provided Dockerfile.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note : - Keep in mind for Tags with images [latest, stable, alpha, Beta]&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Download image from another regestry instead of docker hub : &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker pull quay.io/dockerinaction/ch3_hello_registry:latest&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;[REGISTRYHOST:PORT/][USERNAME/]NAME[:TAG]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;351-installing-images-using-dockerfile&quot;&gt;3.5.1 Installing Images using dockerfile&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/dockerinaction/ch3_dockerfile.git
docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; dia_ch3/dockerfile:latest ch3_dockerfile
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;36-backup--restore-docker-images&quot;&gt;3.6 Backup &amp;amp; Restore Docker Images&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker export&lt;/code&gt; turns container filesystem into tarball archive stream to STDOUT.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker import&lt;/code&gt; creates an image from a tarball.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker load&lt;/code&gt; loads an image from a tar archive as STDIN, including images and tags (as of 0.7).&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker save&lt;/code&gt; saves an image to a tar archive stream to STDOUT with all parent layers, tags &amp;amp; versions (as of 0.7).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;361-comparison&quot;&gt;3.6.1 Comparison&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Docker &lt;span class=&quot;nb&quot;&gt;export&lt;/span&gt; : Container To TAR
Docker Import : TAR to Image
Docker save   : Image To TAR
Docker load   : TAR to Image
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;362-getting-practical&quot;&gt;3.6.2 Getting Practical&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker login/logout // get access to private repo on docker hub
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker save &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;image_name]
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker save &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; myfile.tar image_name:latest // saves &lt;span class=&quot;nb&quot;&gt;tar &lt;/span&gt;file &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;current directory
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker load –i myfile.tar

&lt;span class=&quot;c&quot;&gt;# Load/Save image&lt;/span&gt;
- Load an image from file:
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker load &amp;lt; my_image.tar.gz

&lt;span class=&quot;c&quot;&gt;# Save an existing image:&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker save my_image:my_tag | &lt;span class=&quot;nb&quot;&gt;gzip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; my_image.tar.gz

&lt;span class=&quot;c&quot;&gt;# Import/Export container&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Import a container as an image from file:&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cat &lt;/span&gt;my_container.tar.gz | docker import - my_image:my_tag

&lt;span class=&quot;c&quot;&gt;# Export an existing container:&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;my_container | &lt;span class=&quot;nb&quot;&gt;gzip&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; my_container.tar.gz

&lt;span class=&quot;c&quot;&gt;# Difference between loading a saved image and importing an exported container as an image&lt;/span&gt;
Loading an image using the load &lt;span class=&quot;nb&quot;&gt;command &lt;/span&gt;creates a new image including its history.
Importing a container as an image using the import &lt;span class=&quot;nb&quot;&gt;command &lt;/span&gt;creates a new image excluding the &lt;span class=&quot;nb&quot;&gt;history &lt;/span&gt;which results &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;a smaller image size compared to loading an image.

&lt;span class=&quot;c&quot;&gt;# Save the State of Docker Image:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# we can save the state of image by commiting ( like we do in source control )&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker commit my_container
&lt;span class=&quot;c&quot;&gt;# Creates a new Image ID&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Restore State of Conatiner&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;options] New_Image_ID

&lt;span class=&quot;c&quot;&gt;# There is problem here. Docker Images ID's are 256Bit long, So there is no way to remember. what stuff we commit ( solution : is Tagging )&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Tag&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker tag ID_OF_IMAGE imagename
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run imagename &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; instead of 256Bit long ID &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# We can also Reffer to a specific image in builds&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# mention ID of Specific Build of Image ( like we did in previous steps ) and use it docker file.&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Remember : This is image is locally available ( docker is not looking this on Docker HUB )&lt;/span&gt;

FROM 8eaa4ff06b53

&lt;span class=&quot;c&quot;&gt;##  Walkthrough of Saving States&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Install 2048 Game ( for this we need VNC viewer ( TigerVNC )&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 5901:5901 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 6080:6080 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; win2048 imiell/win2048
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;vncviewer localhost:1 &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;:1 If you have no X display on host &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# connect to port 5901 &amp;amp; default password for vnc viewer is 'vncpass'&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Save a Game ( Commit Container )&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker commit win2048 1&lt;span class=&quot;o&quot;&gt;((&lt;/span&gt;co14-1&lt;span class=&quot;o&quot;&gt;))&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker tag ID 2048tag:&lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt; +%s&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Return To the Save Game&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 5901:5901 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 6080:6080 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; win2048 my2048tag:&lt;span class=&quot;nv&quot;&gt;$mytag&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;37-generating-dependency-graph-of-docker-image&quot;&gt;3.7 Generating Dependency graph of Docker Image&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# genrate a tree of dependecies of image&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;git clone https://github.com/docker-in-practice/docker-image-graph
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;docker-image-graph
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; dockerinpractice/docker-image-graph
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /var/run/docker.sock:/var/run/docker.sock dockerinpractice/docker-image-graph &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; docker_images.png
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;38-tricks-for-making-an-image-smaller&quot;&gt;3.8 Tricks for Making an Image Smaller&lt;/h3&gt;

&lt;h4 id=&quot;381-method-1--reduce-the-size-of-third-party-image&quot;&gt;3.8.1 Method 1 : Reduce the size of Third Party Image&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Step 1 : Remove Unnecessary file&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Step 2 : Flatten the Image ( describe in this book )&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Step 3 : Check Which Packages we dont need ( $ dpkg -l | awk '{print $2}'&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Step 4 : Remove Packages ( apt-get purge -y package name )&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Step 5 : Clean the cache ( apt-get autoremove, apt-get clean )&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Step 6 : Remove all the man pages and other doc files :&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; /usr/share/doc/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; /usr/share/man/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; /usr/share/info/&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Step 7 : Clean the Temp data &amp;amp; logs in (/var)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;find /var | &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'\.log$'&lt;/span&gt; | xargs &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Step 8 : Commit The Image ( These Steps Creates a Much Smaller Image )&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;382-method-2--tiny-docker-images-with-busybox-and-alpine&quot;&gt;3.8.2 Method 2 : Tiny Docker Images with BusyBox and Alpine&lt;/h4&gt;

&lt;p&gt;Small, usable OSs that can be embedded onto a low-power or cheap computer have existed since Linux began.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# BusyBox ( Weight of BusyBox 2.5 MB )&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# BusyBox is so small ( so it can't uses the bash. It uses ash )&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; busybox /bin/ash

&lt;span class=&quot;c&quot;&gt;# problem is that busybox don't uses any package manager : so for installing packages&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; progrium/busybox /bin/ash &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;Size: 5 MB &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# its uses opkg package manager&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;opkg-install bash &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; /dev/null
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;bash &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; Size of contianer is 6 MB with Bash Shell &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; get ready to play with bash &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Alpine ( 36 MB )&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt; Package Manager : APK
FROM gliderlabs/alpine:3.6
RUN apk-install mysql-client
ENTRYPOINT &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;mysql&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

list of packages : https://pkgs.alpinelinux.org/packages
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;383-method-3--the-go-model-of-minimal-containers&quot;&gt;3.8.3 Method 3 : The GO model of minimal containers&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# we can minimal Web server with go [ 5 MB Web Server ]&lt;/span&gt;
https://github.com/docker-in-practice/go-web-server

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt; Dockerfile
FROM golang:1.4.2
RUN &lt;span class=&quot;nv&quot;&gt;CGO_ENABLED&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0 go get &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-ldflags&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'-s'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-installsuffix&lt;/span&gt; cgo &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
github.com/docker-in-practice/go-web-server
CMD &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;cat&quot;&lt;/span&gt;,&lt;span class=&quot;s2&quot;&gt;&quot;/go/bin/go-web-server&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; go-webserver &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; go-web-server &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;go-web-server
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run go-webserver &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; go-web-server
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;chmod&lt;/span&gt; +x go-web-server
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;echo &lt;/span&gt;Hi &lt;span class=&quot;o&quot;&gt;&amp;gt;&lt;/span&gt; page.html

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
FROM scratch
ADD go-web-server /go-web-server
ADD page.html /page.html
ENTRYPOINT &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;/go-web-server&quot;&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; go-web-server &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker images | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;go-web-server
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8080:8080 go-web-server &lt;span class=&quot;nt&quot;&gt;-port&lt;/span&gt; 8080
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; : Remember One Large image is much efficient than some small images : Because it saves space on your HDD and save network bandwidth also &amp;amp; Easy to maintainable.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt; : One Ubuntu Image with Node, Python, Nginx and other services ( around 1GB) (assigns only 1 IP)
Many small container are request internet ( so they consume bandwidth more &amp;amp; also they will consume more space than 1 GB )&lt;/p&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;40-storage-volumes&quot;&gt;4.0 Storage Volumes&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;41-day-to-day-volumes-command&quot;&gt;4.1 Day To Day Volumes Command&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker volume create&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker volume rm&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker volume ls&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker volume inspect&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;42-types-of-volumes&quot;&gt;4.2 Types of Volumes&lt;/h3&gt;

&lt;p&gt;The three most common types of storage mounted into containers:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bind mounts&lt;/li&gt;
  &lt;li&gt;In-memory storage&lt;/li&gt;
  &lt;li&gt;Docker volumes&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;421-bind-mounts&quot;&gt;4.2.1 Bind Mounts&lt;/h4&gt;

&lt;p&gt;Bind mounts are mount points used to remount parts of a filesystem tree onto other locations. When working with containers, bind mounts attach a user-specified location on the host filesystem to a specific point in a container file tree.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CONF_SRC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;~/example.conf&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;CONF_DST&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/etc/nginx/conf.d/default.conf&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;&lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;LOG_SRC&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;~/example.log&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;LOG_DST&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/var/log/nginx/custom.host.access.log&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; diaweb &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--mount&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;bind&lt;/span&gt;,src&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CONF_SRC&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;,dst&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;CONF_DST&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--mount&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;bind&lt;/span&gt;,src&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;LOG_SRC&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt;,dst&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;${&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;LOG_DST&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 80:80 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
nginx:latest

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;422-in-memory-storage&quot;&gt;4.2.2 In-Memory Storage&lt;/h4&gt;

&lt;p&gt;Most service software and web applications use private key files, database passwords,
API key files, or other sensitive configuration files, and need upload buffering space.
In these cases, it is important that you never include those types of files in an image or
write them to disk. Instead, you should use in-memory storage. You can add in-memory
storage to containers with a special type of mount.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# 1777 permissions in octal&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# tmpfs-size=16k&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
memory-based filesystem into a containerdocker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--mount&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;tmpfs,dst&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/tmp,tmpfs-size&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;16k,tmpfs-mode&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;1770 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--entrypoint&lt;/span&gt; mount &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
alpine:latest &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;423-docker-volumes&quot;&gt;4.2.3 Docker Volumes&lt;/h4&gt;

&lt;p&gt;Docker volumes are named filesystem trees managed by Docker. They can be implemented with disk storage on the host filesystem, or another more exotic backend such as cloud storage. All operations on Docker volumes can be accomplished using the docker volume subcommand set.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
docker volume create &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--driver&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;local&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--label&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;example&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;location &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
location-example
docker volume inspect &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--format&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;&quot;&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
location-example
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;43-moving-docker-to-a-different-partition&quot;&gt;4.3 Moving Docker to a different Partition&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Stop Docker Daemon ( service docker stop )&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ $ &lt;/span&gt;dockerd &lt;span class=&quot;nt&quot;&gt;-g&lt;/span&gt; /home/dockeruser/mydocker
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt; : This will wipe all the containers and images from your previous Docker daemon.&lt;/p&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;44-access-filesystem-from-docker-container&quot;&gt;4.4 Access Filesystem from Docker Container&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# This will mount /dotfiles folder to container&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /home/hacstac/dotfiles:~/dotfiles &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; debian bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;45-share-volumes-across-the-internet&quot;&gt;4.5 Share Volumes Across the internet&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# In this we use a technology called Resilio&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Example ( 2 Machines ) : Setup Resilio on both Machine - That synchronized a volume ( Connected through a Secret Key )&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Machine 1&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8888:8888 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 55555:55555 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; resilio ctlc/btsync

&lt;span class=&quot;c&quot;&gt;# docker logs resilio Or ( Lazy ) -&amp;gt; Use Portainer ( Logs )&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# copy secret key&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--volumes-from&lt;/span&gt; resilio ubuntu /bin/bash
&lt;span class=&quot;c&quot;&gt;# create Data in ubuntu : touch /data/shared_from_server&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Machine 2&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# setup resilio client&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; resilio-client &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8888:8888 &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 55555:55555 ctlc/btsync key_of_server

&lt;span class=&quot;c&quot;&gt;# Setup ubuntu&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--volumes-from&lt;/span&gt; resilio-client ubuntu /bin/bash
&lt;span class=&quot;c&quot;&gt;# our data folder is now available in this &amp;amp; if you create a file in here then it will synchronized to Machine 1 also.&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;46-using-a-centralized-data-volumes-for-containers&quot;&gt;4.6 Using a Centralized Data Volumes For Containers&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Create a volume with docker which store a data which you need in other docker containers&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /codebase &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; codebase busybox

&lt;span class=&quot;c&quot;&gt;# access the codebase&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--volumes-from&lt;/span&gt; codebase ubuntu /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;47-mounting-the-remote-file-systems&quot;&gt;4.7 Mounting the remote file systems&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# This will need FUSE Kernel Module to be loaded on Host OS ( filesystem and userspace )&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Required Root Access ( Danger )&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 4.7.1. SSHFS&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Local Host&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--privileged&lt;/span&gt; debian /bin/bash
&lt;span class=&quot;c&quot;&gt;# Inside a Container&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;apt-get update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;sshfs
&lt;span class=&quot;nv&quot;&gt;$ LOCALPATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/path/to/directory/
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$LOCALPATH&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;sshfs &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; nonempty user@host:/path/to/directory &lt;span class=&quot;nv&quot;&gt;$LOCALPATH&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# to unmount fusermount -u /path/to/local/directory&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Now the remote folder is mount on LOCALPATH&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# 4.7.2 NFS&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# Install a NFS on host ( because docker doesn't support NFS )&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;apt-get &lt;span class=&quot;nb&quot;&gt;install &lt;/span&gt;nfs-kernel-server
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;mkdir&lt;/span&gt; /export
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;chmod &lt;/span&gt;777 /export
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;mount &lt;span class=&quot;nt&quot;&gt;--bind&lt;/span&gt; /opt/test/db /export

&lt;span class=&quot;c&quot;&gt;# add this to fstab ( if you want to persist over reboot )&lt;/span&gt;
/etc/fstab file: /opt/test/db /export none &lt;span class=&quot;nb&quot;&gt;bind &lt;/span&gt;0 0

&lt;span class=&quot;c&quot;&gt;# add this to /etc/exports&lt;/span&gt;
/export   127.0.0.1&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt;ro,fsid&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;0,insecure,no_subtree_check,async&lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# to Read/Write : change ro to rw &amp;amp; add no_root_squash (After async)&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# To open to the internet replace localhost to *&lt;/span&gt;
&lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; Danger : Think about it &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;mount &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; nfs 127.0.0.1:/export /mnt
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;exportfs &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;service nfs-kernel-server restart

&lt;span class=&quot;c&quot;&gt;# Run a container&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; nfs-client &lt;span class=&quot;nt&quot;&gt;--privileged&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /mnt:/mnt busybox /bin/true

&lt;span class=&quot;c&quot;&gt;# Mount on other container&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--volumes-from&lt;/span&gt; nfs-client debian /bin/bash
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;50-networks-in-docker&quot;&gt;5.0 Networks in Docker&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;51-day-to-day-network-commands&quot;&gt;5.1 Day To Day Network Commands&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker network create&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker network rm&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker network ls&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker network inspect&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker network connect&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;docker network disconnect&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;52-examples&quot;&gt;5.2 Examples&lt;/h3&gt;

&lt;h4 id=&quot;521-to-list-all-networks&quot;&gt;5.2.1 To list all networks&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker network &lt;span class=&quot;nb&quot;&gt;ls

&lt;/span&gt;NETWORK ID          NAME                DRIVER              SCOPE
f32f6d51e8c8        bridge              bridge              &lt;span class=&quot;nb&quot;&gt;local
&lt;/span&gt;366d3d1f4719        hacstac_default     bridge              &lt;span class=&quot;nb&quot;&gt;local
&lt;/span&gt;6c08bddba2c4        host                host                &lt;span class=&quot;nb&quot;&gt;local
&lt;/span&gt;b726554d155b        none                null                &lt;span class=&quot;nb&quot;&gt;local&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;522-to-create-a-new-network&quot;&gt;5.2.2 To Create a New Network&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker network create &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--driver&lt;/span&gt; bridge &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--label&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;project&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;dockerinaction &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--label&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;chapter&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;5 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--attachable&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--scope&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;local&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--subnet&lt;/span&gt; 10.0.42.0/24 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--ip-range&lt;/span&gt; 10.0.42.128/25 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  user-network

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--network&lt;/span&gt; user-network &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; network-explorer &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  alpine:3.8 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    sh

&lt;span class=&quot;c&quot;&gt;# CTRL-P + CTRL-Q Detech&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# docker attach network-explorer&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# walkthrough&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker network create my_network &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; Create a Network &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker network connect my_network blog1 &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; blog1 container connect to a network my_network &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--network&lt;/span&gt; my_network ubuntu:16.04 bash &lt;span class=&quot;o&quot;&gt;[&lt;/span&gt; Now this ubuntu container have access to blog1 Container &lt;span class=&quot;o&quot;&gt;]&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;ip &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; inet &lt;span class=&quot;nt&quot;&gt;-4&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-o&lt;/span&gt; addr // this will list loopback and assign ip subnet address
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;523-create-a-another-bridge-network&quot;&gt;5.2.3 Create a another bridge network&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker network create &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--driver&lt;/span&gt; bridge &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--attachable&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--scope&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;local&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--subnet&lt;/span&gt; 10.0.43.0/24 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  &lt;span class=&quot;nt&quot;&gt;--ip-range&lt;/span&gt; 10.0.43.128/25 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  user-network2


&lt;span class=&quot;c&quot;&gt;# Attach priviously created container attach to this network&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker network connect &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  user-network2 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
  network-explorer

&lt;span class=&quot;c&quot;&gt;# then this container lists two ethernet addresses&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# scan with nmap&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;nmap &lt;span class=&quot;nt&quot;&gt;-sn&lt;/span&gt; 10.0.42.&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-sn&lt;/span&gt; 10.0.43.&lt;span class=&quot;k&quot;&gt;*&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-oG&lt;/span&gt; /dev/stdout | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;Status
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;524-with-network---none--means-container-with-no-external-excess&quot;&gt;5.2.4 With Network - none : Means container with no external excess&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--network&lt;/span&gt; none &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    alpine:3.8 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    ping &lt;span class=&quot;nt&quot;&gt;-w&lt;/span&gt; 2 1.1.1.1
&lt;span class=&quot;c&quot;&gt;# it will try to ping 1.1.1.1 but it failed because this container hace no external network access&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# NodePort Publishing&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# 8080:8000 will denote 8080 port of host machine and 8000 port of container&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-p&lt;/span&gt; 8080 &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; listener alpine:3.8
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker port listener // To view port of running container
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;525-dns-with-docker&quot;&gt;5.2.5 DNS with docker&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Feature 1 : --hostname will add hostname : so we open with DN&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--hostname&lt;/span&gt; barker &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    alpine:3.8 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    nslookup barker

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
Server:    10.0.2.3
Address 1: 10.0.2.3

Name:      barker
Address 1: 172.17.0.22 barker
&lt;span class=&quot;nt&quot;&gt;----&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Feature 2 : --dns : set dns server on container&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--dns&lt;/span&gt; 8.8.8.8 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    alpine:3.8 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    nslookup docker.com

&lt;span class=&quot;c&quot;&gt;# Feature 3 : --dns-search : allows us to specify a DNS searchdomain,  which  is  like  a  default  hostname  suffix&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--dns-search&lt;/span&gt; docker.com &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--dns&lt;/span&gt; 1.1.1.1 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    alpine:3.8 &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /etc/resolv.conf
&lt;span class=&quot;c&quot;&gt;# Will display contents that look like:&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# search docker.com&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# nameserver 1.1.1.1&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Feature 4 : --add-host&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--hostname&lt;/span&gt; mycontainer &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--add-host&lt;/span&gt; docker.com:127.0.0.1 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--add-host&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt;:10.10.10.2 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    alpine:3.8 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;cat&lt;/span&gt; /etc/hosts

&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
172.17.0.45  mycontainer
127.0.0.1    localhost
::1          localhost ip6-localhost ip6-loopbackfe00
::0          ip6-localnetff00::0      ip6-mcastprefixff02
::1          ip6-allnodesff02::2      ip6-allrouters
10.10.10.2   &lt;span class=&quot;nb&quot;&gt;test
&lt;/span&gt;127.0.0.1   docker.com
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;526-you-can-specify-a-specific-ip-address-for-a-container&quot;&gt;5.2.6 You can specify a specific IP address for a container&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# create a new bridge network with your subnet and gateway for your ip block&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker network create &lt;span class=&quot;nt&quot;&gt;--subnet&lt;/span&gt; 203.0.113.0/24 &lt;span class=&quot;nt&quot;&gt;--gateway&lt;/span&gt; 203.0.113.254 iptastic

&lt;span class=&quot;c&quot;&gt;# run a nginx container with a specific ip in that block&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--net&lt;/span&gt; iptastic &lt;span class=&quot;nt&quot;&gt;--ip&lt;/span&gt; 203.0.113.2 nginx

&lt;span class=&quot;c&quot;&gt;# curl the ip from any other place (assuming this is a public ip block duh)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;curl 203.0.113.2
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;527-open-a-docker-daemon-to-the-world&quot;&gt;5.2.7 Open a Docker Daemon to the World&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# first of stop the docker service&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;service docker stop / or
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;systemctl stop docker

&lt;span class=&quot;c&quot;&gt;# Checks for docker daemon ??&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt; ps &lt;span class=&quot;nt&quot;&gt;-ef&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-E&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'docker(d| -d| daemon)\b'&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Expose to local host : 2375&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;docker daemon &lt;span class=&quot;nt&quot;&gt;-H&lt;/span&gt; tcp://0.0.0.0:2375

&lt;span class=&quot;c&quot;&gt;# To connect&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nt&quot;&gt;-H&lt;/span&gt; tcp://&amp;lt;your host&lt;span class=&quot;s1&quot;&gt;'s ip&amp;gt;:2375 &amp;lt;subcommand&amp;gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;528-get-an-ip--ports-of-a-docker-container&quot;&gt;5.2.8 Get an IP &amp;amp; Ports of a Docker Container&lt;/h4&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;alias &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;dl&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'docker ps -l -q'&lt;/span&gt;  &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; latest container ID &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker inspect &lt;span class=&quot;si&quot;&gt;$(&lt;/span&gt;dl&lt;span class=&quot;si&quot;&gt;)&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;grep&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-wm1&lt;/span&gt; IPAddress | &lt;span class=&quot;nb&quot;&gt;cut&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'&quot;'&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; 4
Pass &lt;span class=&quot;o&quot;&gt;(&lt;/span&gt; ID of container Instead of dl &lt;span class=&quot;o&quot;&gt;)&lt;/span&gt; : this above &lt;span class=&quot;nb&quot;&gt;command &lt;/span&gt;gives ip of latest container

&lt;span class=&quot;c&quot;&gt;# get ports&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker inspect &lt;span class=&quot;nt&quot;&gt;-f&lt;/span&gt; &lt;span class=&quot;s1&quot;&gt;'  -&amp;gt;  '&lt;/span&gt; 274d2292a137 | name_of_container
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;60-limiting-risk-with-resource-controls&quot;&gt;6.0 Limiting Risk with Resource Controls&lt;/h2&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;61-memory-limits&quot;&gt;6.1 Memory Limits&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; ch6_mariadb &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--memory&lt;/span&gt; 256m &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--cpu-shares&lt;/span&gt; 1024 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--cap-drop&lt;/span&gt; net_raw &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;MYSQL_ROOT_PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    mariadb:5.5

&lt;span class=&quot;c&quot;&gt;# This container only uses the 256M memory&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;62-cpu-limits&quot;&gt;6.2 CPU Limits&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-P&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; ch6_wordpress &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--memory&lt;/span&gt; 512m &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--cpu-shares&lt;/span&gt; 512 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--cap-drop&lt;/span&gt; net_raw &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--link&lt;/span&gt; ch6_mariadb:mysql &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;WORDPRESS_DB_PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
wordpress:5.0.0-php7.2-apache
&lt;span class=&quot;c&quot;&gt;# if total cpu share is 1536 - 512 ( 33% ) : this container consume 33% of cpu shares&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-P&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; ch6_wordpress &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--memory&lt;/span&gt; 512m &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--cpus&lt;/span&gt; 0.75 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--cap-drop&lt;/span&gt; net_raw &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;--link&lt;/span&gt; ch6_mariadb:mysql &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;-e&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;WORDPRESS_DB_PASSWORD&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;test&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
wordpress:5.0.0-php7.2-apache
&lt;span class=&quot;c&quot;&gt;# This Container : consumed max 75% of cpu cores&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# also we can use {--cpuset-cpus 0-4} (cores)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;63-access-to-devices&quot;&gt;6.3 Access to devices&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--device&lt;/span&gt; /dev/video0:/dev/video0 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    ubuntu:16.04 &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-al&lt;/span&gt; /dev
&lt;span class=&quot;c&quot;&gt;# --device flag will mount external device to container&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;64-sharing-memory--ipc--interprocess-communication-&quot;&gt;6.4 Sharing Memory ( IPC : Interprocess Communication )&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Producer&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; nobody &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; ch6_ipc_producer &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--ipc&lt;/span&gt; shareable &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    dockerinaction/ch6_ipc &lt;span class=&quot;nt&quot;&gt;-producer&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Consumer&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container run &lt;span class=&quot;nt&quot;&gt;-d&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--name&lt;/span&gt; ch6_ipc_consumer &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--ipc&lt;/span&gt; container:ch6_ipc_producer &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    dockerinaction/ch6_ipc &lt;span class=&quot;nt&quot;&gt;-consumer&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# we can see the process of one container in another : by using docker logs { they share the memory space }&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# IMP NOTE : In docker to clean Volumes&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-vf&lt;/span&gt; name_of_container
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;65-understanding-users&quot;&gt;6.5 Understanding Users&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# if we want to setup a container with diff user then&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--user&lt;/span&gt; nobody &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    busybox:1.29 &lt;span class=&quot;nb&quot;&gt;id&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; 1000:1000 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    busybox:1.29 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    /bin/bash &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;echo This is important info &amp;gt; /logFiles/important.log&quot;&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# with this userID:GroupID we can access the file system of this users&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;66-os-features-access-with-capabilities&quot;&gt;6.6 OS features Access with Capabilities&lt;/h3&gt;

&lt;p&gt;Linux capabilities can be set by using cap-add and cap-drop. See &lt;a href=&quot;https://docs.docker.com/engine/reference/run/#/runtime-privilege-and-linux-capabilities&quot;&gt;https://docs.docker.com/engine/reference/run/#/runtime-privilege-and-linux-capabilities&lt;/a&gt; for details. This should be used for greater security.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
SYS_MODULE —Insert/remove kernel modules
SYS_RAWIO — Modify kernel memory
SYS_NICE — Modify priority of processes
SYS_RESOURCE — Override resource limits
SYS_TIME — Modify the system clock
AUDIT_CONTROL — Configure audit subsystem
MAC_ADMIN — Configure MAC configuration
SYSLOG — Modify kernel print behavior
NET_ADMIN — Configure the network
SYS_ADMIN — Catchall &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;administrative functions
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-u&lt;/span&gt; nobody &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--cap-add&lt;/span&gt; sys_admin &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    ubuntu:16.04 &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    /bin/bash &lt;span class=&quot;nt&quot;&gt;-c&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;capsh --print | grep sys_admin&quot;&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# this --cap-add sys_admin will add admin facilities to container&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# we can inspect docker with .HostConfig.CapAdd &amp;amp;&amp;amp; .HostConfig.CapDrop show capabilities&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Give access to a single device:&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--device&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;/dev/ttyUSB0 debian bash

&lt;span class=&quot;c&quot;&gt;# Give access to all devices:&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;--privileged&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; /dev/bus/usb:/dev/bus/usb debian bash

&lt;span class=&quot;c&quot;&gt;# Docker Container with full privileges&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--privileged&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    ubuntu:16.04 capsh &lt;span class=&quot;nb&quot;&gt;ls&lt;/span&gt; /dev
&lt;span class=&quot;c&quot;&gt;# check out list of mounted devices&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;67-additional-security-with-docker&quot;&gt;6.7 Additional Security with Docker&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker container run &lt;span class=&quot;nt&quot;&gt;--rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-it&lt;/span&gt; &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    &lt;span class=&quot;nt&quot;&gt;--security-opt&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;seccomp&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;path_to_the_secomp conf file &lt;span class=&quot;se&quot;&gt;\&lt;/span&gt;
    ubuntu:16.04 sh

&lt;span class=&quot;c&quot;&gt;# For Linux Security Modules ( LSM )&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
The LSM security option values are specified &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;one of seven formats:

- To prevent a container from gaining new privileges after it starts, use
&lt;span class=&quot;s1&quot;&gt;'no-new-privileges'&lt;/span&gt;

- To  &lt;span class=&quot;nb&quot;&gt;set  &lt;/span&gt;a  SELinux  user  label,  use  the  form
&lt;span class=&quot;nv&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;user:username, where is the name of the user you want to use &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;the label.

- To &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;a SELinux role label, use the form  &lt;span class=&quot;nv&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;role:role where is the name of the role you want to apply to processes &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;the container.

- To &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;a SELinux &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;label, use the form &lt;span class=&quot;nv&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;type&lt;/span&gt;:type , where is the &lt;span class=&quot;nb&quot;&gt;type &lt;/span&gt;name of the processes &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;the container.
- To &lt;span class=&quot;nb&quot;&gt;set &lt;/span&gt;a SELinux-level label, use the form &lt;span class=&quot;s1&quot;&gt;'label:level:label'&lt;/span&gt; , where is the level at which processes &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;the container should run. Levels are specified as  low-high  pairs.  Where  abbreviated  to  the  low  level  only,  SELinux  will  inter-pret the range as single level.

- To disable SELinux label confinement &lt;span class=&quot;k&quot;&gt;for &lt;/span&gt;a container, use the form
&lt;span class=&quot;nv&quot;&gt;label&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;disable

&lt;span class=&quot;c&quot;&gt;# NOTE : Avoid Running Containers in privileged mode whenever possible&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;68-using-socat-to-monitor-docker-api-traffic&quot;&gt;6.8 Using Socat to monitor docker api traffic&lt;/h3&gt;

&lt;p&gt;In this technique you’ll insert a proxy Unix domain socket between your request and the server’s socket to see what passes through it. Note that you’ll need root or sudo privileges to make this work.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# We need a socat ( Install socat as per the OS package Maneger )&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;socat &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; UNIX-LISTEN:/tmp/dockerapi.sock,fork &lt;span class=&quot;se&quot;&gt;\U&lt;/span&gt;NIX-CONNECT:/var/run/docker.sock &amp;amp;

In this &lt;span class=&quot;nb&quot;&gt;command&lt;/span&gt;, &lt;span class=&quot;nt&quot;&gt;-v&lt;/span&gt; makes the output readable, with indications of the flow of data.The UNIX-LISTEN part tells socat to listen on a Unix socket, fork ensures that socatdoesn’t  &lt;span class=&quot;nb&quot;&gt;exit  &lt;/span&gt;after  the  first  request,  and  UNIX-CONNECT  tells  socat  to  connect  toDocker’s  Unix  socket.  The  &amp;amp;  specifies  that  the  &lt;span class=&quot;nb&quot;&gt;command  &lt;/span&gt;runs  &lt;span class=&quot;k&quot;&gt;in  &lt;/span&gt;the  background.If you usually run the Docker client with &lt;span class=&quot;nb&quot;&gt;sudo&lt;/span&gt;, you’ll need to &lt;span class=&quot;k&quot;&gt;do &lt;/span&gt;the same thing here as well.

&lt;span class=&quot;c&quot;&gt;# List all containers&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker &lt;span class=&quot;nt&quot;&gt;-H&lt;/span&gt; unix:///tmp/dockerapi.sock ps &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt;
&lt;span class=&quot;c&quot;&gt;# This will show how client request to daemon&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;69-setting-timezone-in-containers&quot;&gt;6.9 Setting TimeZone in Containers&lt;/h3&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# Runs a command to display the time zone on the host&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt; +%Z // UTC

&lt;span class=&quot;c&quot;&gt;# change TimeZone&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
FROM centos:7
RUN &lt;span class=&quot;nb&quot;&gt;rm&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-rf&lt;/span&gt; /etc/localtime
RUN &lt;span class=&quot;nb&quot;&gt;ln&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-s&lt;/span&gt; /usr/share/zoneinfo/Asia/Kolkata /etc/localtime
CMD &lt;span class=&quot;nb&quot;&gt;date&lt;/span&gt; +%Z
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Build Image&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; timezone_change &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;docker run timezone_change
Asia/Kolkata
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;h3 id=&quot;610-locale-management&quot;&gt;6.10 Locale Management&lt;/h3&gt;

&lt;p&gt;locale will be set in the environment through the LANG,LANGUAGE, and locale-gen variables&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c&quot;&gt;# you are getting encoding error if correct locale is not set.&lt;/span&gt;

&lt;span class=&quot;c&quot;&gt;# Check for locale&lt;/span&gt;
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;env&lt;/span&gt; | &lt;span class=&quot;nb&quot;&gt;grep &lt;/span&gt;LANG
&lt;span class=&quot;nv&quot;&gt;LANG&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;en_GB.UTF-8
This is British English, with text encoded &lt;span class=&quot;k&quot;&gt;in &lt;/span&gt;UTF-8.

&lt;span class=&quot;c&quot;&gt;# Set Locale&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;
FROM ubuntu:16.04
RUN apt-get update &lt;span class=&quot;o&quot;&gt;&amp;amp;&amp;amp;&lt;/span&gt; apt-get &lt;span class=&quot;nb&quot;&gt;install&lt;/span&gt; &lt;span class=&quot;nt&quot;&gt;-y&lt;/span&gt; locales
RUN locale-gen en_US.UTF-8
ENV LANG en_US.UTF-8
ENV LANGUAGE en_US:en
CMD &lt;span class=&quot;nb&quot;&gt;env&lt;/span&gt;
&lt;span class=&quot;nt&quot;&gt;---&lt;/span&gt;

&lt;span class=&quot;nv&quot;&gt;$ $ &lt;/span&gt;docker build &lt;span class=&quot;nt&quot;&gt;-t&lt;/span&gt; encoding &lt;span class=&quot;nb&quot;&gt;.&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;hr /&gt;</content><author><name></name></author><category term="Docker" /><summary type="html">1.0 What is a Docker</summary></entry></feed>