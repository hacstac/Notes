{
  
    
        "post0": {
            "title": "Kubernetes Service Discovery",
            "content": ". 1.1 What is Service Discovery . A Kubernetes Service is a resource you create to make a single, constant point of entry to a group of pods providing the same service. Each service has an IP address and port that never change while the service exists. Clients can open connections to that IP and port, and those connections are then routed to one of the pods backing that service. This way, clients of a service don’t need to know the location of individual pods providing the service, allowing those pods to be moved around the cluster at any time. . Service-discovery is tool that help to solve the problem of finding which processes are listening at which addresses for which services. A good service-discovery system will enable users to resolve this information quickly and reliably. A good system is also low-latency; clients are updated soon after the information associated with a service changes. . If we understand this with an example: Where you have a frontend web server and a backend data base server. There may be multiple pods that all act as the frontend, but there may only be a single backend database pod. You need to solve two problems to make the system function: . External clients need to connect to the frontend pods without caring if there’s only a single web server or hundreds. | The frontend pods need to connect to the backend database. Because the database runs inside a pod, it may be moved around the cluster over time, causing its IP address to change. You don’t want to reconfigure the frontend pods every time the backend database is moved. | . By creating a service for the frontend pods and configuring it to be accessible from outside the cluster, you expose a single, constant IP address through which external clients can connect to the pods. Similarly, by also creating a service for the backend pod, you create a stable address for the backend pod. The service address doesn’t change even if the pod’s IP address changes. Additionally, by creating the service, you also enable the frontend pods to easily find the backend service by its name through either environment variables or DNS. . . . 1.2 Why we need service discovery . To solve the problem of multi node networking, where each pods share same private network and all those pods should be accessible through a single IP address. To access these pods outside a private network, we need a Kubernetes Services. . If we consider a scenario, where in non kubernetes env where a sysadmin would configure each client app by specifying the exact IP address or hostname of the server providing the service in the client’s configuration files, doing the same in Kubernetes wouldn’t work, because: . Pods are ephemeral—They may come and go at any time, whether it’s because a pod is removed from a node to make room for other pods, because someone scaled down the number of pods, or because a cluster node has failed. | Kubernetes assigns an IP address to a pod after the pod has been scheduled to a node and before it’s started —Clients thus can’t know the IP address of the server pod up front. | Horizontal scaling means multiple pods may provide the same service—Each of those pods has its own IP address. Clients shouldn’t care how many pods are backing the service and what their IPs are. They shouldn’t have to keep a list of all the individual IPs of pods. | . That’s why we need Kubernetes Services. . . 1.3 The Service Object . Real service discovery in Kubernetes starts with a Service object. A Service object is a way to create a named label selector. As we will see, the Service object does some other nice things for us, too. . Just as the kubectl run command is an easy way to create a Kubernetes deployment, we can use kubectl expose to create a service. Let’s create some deployments and services so we can see how they work: . ## 1.0 Create a Deployment # Boot the deployment directly by the kubectl command like ( given belowe ) or use a YAML file to setup a deployment. # Boot up the Deployment kubectl run cyberchef --image=mpepping/cyberchef --replicas=3 --port=8000 --labels=&quot;ver=1, env=dev&quot; -- ## 2.0 Expose a Deployment # Creating the service using the YAML cyberchef_service.yml apiVersion: v1 kind: Service metadata: name: cyberchef spec: ports: - port: 80 # Host Port targetPort: 8000 # Container Port selector: app: cyberchef $ kubectl create -f ./cyberchef_service.yml -- ## 3.0 Get the Services # Get the clusterIP using getting the service $ kubectl get svc -o wide NAME CLUSTER-IP PORT(S) SELECTOR cyberchef 10.115.242.13 80/TCP ver=1, env=dev . The list shows that the IP address assigned to the service is 10.115.242.13. Because this is the cluster IP, it’s only accessible from inside the cluster. The primary purpose of services is exposing groups of pods to other pods in the cluster, but you’ll usually also want to expose services externally. . 1.3.1 Testing your service from within the cluster . You can send requests to your service from within the cluster in a few ways: . The obvious way is to create a pod that will send the request to the service’s cluster IP and log the response. You can then examine the pod’s log to see what the service’s response was. | You can ssh into one of the Kubernetes nodes and use the curl command. | You can execute the curl command inside one of your existing pods through the kubectl exec command. | . Remotely executing commands in running containers . The kubectl exec command allows you to remotely run arbitrary commands inside an existing container of a pod. This comes in handy when you want to examine the contents, state, and/or environment of a container. List the pods with the kubectl get pods command and choose one as your target for the exec command. . $ kubectl exec cyberchef-d987s -- curl -s http://10.115.242.13 &lt;!-- CyberChef - The Cyber Swiss Army Knife ....... ....... --&gt; &lt; Full Front Page HTML &gt; . 1.3.2 Configuring session affinity on the service . If you execute the same command a few more times, you should hit a different pod with every invocation, because the service proxy normally forwards each connection to a randomly selected backing pod, even if the connections are coming from the same client. If, on the other hand, you want all requests made by a certain client to be redirected to the same pod every time, you can set the service’s sessionAffinity property to ClientIP . apiVersion: v1 kind: Service spec: sessionAffinity: ClientIP ... . This makes the service proxy redirect all requests originating from the same client IP to the same pod. As an exercise, you can create an additional service with session affinity set to ClientIP and try sending requests to it. . 1.3.3 Exposing multiple ports in the same service . Your service exposes only a single port, but services can also support multiple ports. For example, if your pods listened on two ports—let’s say 8080 for HTTP and 8443 for HTTPS—you could use a single service to forward both port 80 and 443 to the pod’s ports 8080 and 8443. You don’t need to create two different services in such cases. Using a single, multi-port service exposes all the service’s ports through a single cluster IP. . apiVersion: v1 kind: Service metadata: name: cyberchef spec: ports: - name: http port: 80 targetPort: 8080 - name: https port: 443 targetPort: 8443 selector: app: cyberchef . 1.3.4 Discovering services . Method 1: Discovering services through env variables . When a pod is started, Kubernetes initializes a set of environment variables pointing to each service that exists at that moment. If you create the service before creating the client pods, processes in those pods can get the IP address and port of the service by inspecting their environment variables. . # To check out the env variables $ kubectl exec cyberchef-d978l2 env PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin HOSTNAME=cyberchef-d978l2 KUBERNETES_SERVICE_HOST=10.115.115.10 KUBERNETES_SERVICE_PORT=80 . Environment variables are one way of looking up the IP and port of a service, but isn’t this usually the domain of DNS. Let look at the DNS way . Method 2: Discovering services through DNS . In the kubernetes env, One of the pod called as kube-dns. The kube-system namespace also includes a corresponding service with the same name. As the name suggests, the pod runs a DNS server, which all other pods running in the cluster are automatically configured to use (Kubernetes does that by modifying each container’s /etc/resolv.conf file). Any DNS query performed by a process running in a pod will be handled by Kubernetes’ own DNS server, which knows all the services running in your system. . Each service gets a DNS entry in the internal DNS server, and client pods that know the name of the service can access it through its fully qualified domain name (FQDN) instead of resorting to environment variables. Now we connect to the service through its FQDN . # Ex frontend.default.svc.cluster.local # frontend = Service Name # default = NameSpace # svc.cluster.local = Configurable cluster domain suffix $ kubectl exec it cyberchef-973l2 bash -&gt;$ curl http://cyberchef.default.svc.cluster.local . 1.3.5 Connect to services from outside the cluster . In most cases we need apps will exposed to external services through the Kubernetes services feature. Instead of having the service redirect connections to pods in the cluster, you want it to redirect to external IP(s) and port(s). This allows you to take advantage of both service load balancing and service discovery. Client pods running in the cluster can connect to the external service like they connect to internal services. . Service Endpoints: An Endpoints resource (yes, plural) is a list of IP addresses and ports exposing a service. The Endpoints resource is like any other Kubernetes resource, so you can display its basic info with kubectl get endpoints: | . Creating the service endpoints . If you create a service without a pod selector, Kubernetes won’t even create the Endpoints resource (after all, without a selector, it can’t know which pods to include in the service). It’s up to you to create the Endpoints resource to specify the list of endpoints for the service. To create a service with manually managed endpoints, you need to create both a Service and an Endpoints resource. . # Creating an endpoints resource for a service without a selector apiVersion: v1 kind: Endpoints metadata: name: cyberchef-service subsets: - addresses: - ip: 11.11.11.11 # Ip of endpoint that the service will forward connections to - ip: 22.22.22.22 ports: - port: 80 . The Endpoints object needs to have the same name as the service and contain the list of target IP addresses and ports for the service. After both the Service and the Endpoints resource are posted to the server, the service is ready to be used like any regular service with a pod selector. Containers created after the service is created will include the environment variables for the service, and all connections to its IP:port pair will be load balanced between the service’s endpoints. . Creating an alias for an external service . Instead of exposing an external service by manually configuring the service’s End- points, a simpler method allows you to refer to an external service by its fully qualified domain name (FQDN). . apiVersion: v1 kind: Service metadata: name: cyberchef-service spec: type: ExternalName externalName: someapi.somecompany.com # API Endpoint ports: - port: 80 . After the service is created, pods can connect to the external service through the cyberchef-service.default.svc.cluster.local domain name (or even external- service) instead of using the service’s actual FQDN. This hides the actual service name and its location from pods consuming the service, allowing you to modify the service definition and point it to a different service any time later, by only changing the externalName attribute or by changing the type back to ClusterIP and creating an Endpoints object for the service—either manually or by specifying a label selector on the service and having it created automatically. . 1.3.6 Exposing services to external clients . We need apps like web server to the outside, so external can access them. . You have a few ways to make a service accessible externally: . Setting the service type to NodePort—For a NodePort service, each cluster node opens a port on the node itself (hence the name) and redirects traffic received on that port to the underlying service. The service isn’t accessible only at the internal cluster IP and port, but also through a dedicated port on all nodes. . | Setting the service type to LoadBalancer, an extension of the NodePort type— This makes the service accessible through a dedicated load balancer, provisioned from the cloud infrastructure Kubernetes is running on. The load balancer redirects traffic to the node port across all the nodes. Clients connect to the service through the load balancer’s IP. . | Creating an Ingress resource, a radically different mechanism for exposing multiple services through a single IP address— It operates at the HTTP level (network layer 7) and can thus offer more features than layer 4 services. . | . 1.3.6.1 NodePort . Creating a NodePort service, you make Kubernetes reserve a port on all its nodes (the same port number is used across all of them) and forward incoming connections to the pods that are part of the service. This is similar to a regular service (their actual type is ClusterIP), but a NodePort service can be accessed not only through the service’s internal cluster IP, but also through any node’s IP and the reserved node port. . cyberchef_nodeport.yml apiVersion: v1 kind: Service metadata: name: cyberchef-nodeport spec: type: NodePort ports: - port: 80 targetPort: 8000 nodePort: 30123 selector: app: cyberchef --or-- # Directly using the kubectl $ kubectl expose deployment cyberchef --type=NodePort --port=80 --targetPort=8000 # Examining a NodePort service $ kubectl get svc cyberchef NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE cyberchef 10.111.254.223 &lt;nodes&gt; 80:30123/TCP 2m . Look at the EXTERNAL-IP column. It shows nodes, indicating the service is accessible through the IP address of any cluster node. The PORT(S) column shows both the internal port of the cluster IP (80) and the node port (30123). . When an external client connects to a service through the node port (this also includes cases when it goes through the load balancer first), the randomly chosen pod may or may not be running on the same node that received the connection. An additional network hop is required to reach the pod, but this may not always be desirable. You can prevent this additional hop by configuring the service to redirect external traffic only to pods running on the node that received the connection. This is done by setting the externalTrafficPolicy field in the service’s spec section: . spec: externalTrafficPolicy: Local . If a service definition includes this setting and an external connection is opened through the service’s node port, the service proxy will choose a locally running pod. If no local pods exist, the connection will hang (it won’t be forwarded to a random global pod, the way connections are when not using the annotation). You therefore need to ensure the load balancer forwards connections only to nodes that have at least one such pod. . 1.3.6.2 External LoadBalancer . Finally, if you have support from the cloud that you are running on (and your cluster is configured to take advantage of it), you can use the LoadBalancer type. This builds on the NodePort type by additionally configuring the cloud to create a new load balancer and direct it at nodes in your cluster. Edit the cyberchef service again (kubectl edit service cyberchef) and change spec.type to LoadBalancer. . If you do a kubectl get services right away you’ll see that the EXTERNAL-IP column for cyberchef now says pending. Wait a bit and you should see a public address assigned by your cloud. You can look in the console for your cloud account and see the configuration work that Kubernetes did for you. . If Kubernetes is running in an environment that doesn’t support LoadBalancer services, the load balancer will not be provisioned, but the service will still behave like a NodePort service. That’s because a LoadBalancer service is an extension of a NodePort service. . cyberchef_loadbalancer.yml apiVersion: v1 kind: Service metadata: name: cyberchef spec: type: LoadBalancer ports: - port: 80 targetPort: 8000 selector: app: cyberchef # Connecting through a load balancer $ kubectl get svc cyberchef NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE cyberchef 10.111.254.223 210.110.15.4 80:30123/TCP 2m # Access at http://210.110.15.4 . 1.3.6.3 Ingress . One important reason is that each LoadBalancer service requires its own load balancer with its own public IP address, whereas an Ingress only requires one, even when providing access to dozens of services. When a client sends an HTTP request to the Ingress, the host and path in the request determine which service the request is forwarded to Ingresses operate at the application layer of the network stack (HTTP) and can provide features such as cookie-based session affinity and the like, which services can’t. . To make ingress work, you need to install ingress as per your kubernetes enviornment. . apiVersion: extensions/v1beta1 kind: Ingress metadata: name: cyberchef spec: rules: - host: cyberchef.example.com http: paths: - path: / backend: serviceName: cyberchef-nodeport servicePort: 80 . This defines an Ingress with a single rule, which makes sure all HTTP requests received by the Ingress controller, in which the host cyberchef.example.com is requested, will be sent to the cyberchef-nodeport service on port 80. . To access your service through http://cyberchef.example.com, you’ll need to make sure the domain name resolves to the IP of the Ingress controller. . # List Ingress $ kubectl get ingresses NAME HOSTS ADDRESS PORTS AGE cyberchef cyberchef.example.com 192.168.99.100 80 29m # Note: Make sure you need to add this to your hosts file ( /etc/hosts ) 192.168.99.100 cyberchef.example.com . How ingress works?? The client first performed a DNS lookup of cyberchef.example.com, and the DNS server (or the local operating system) returned the IP of the Ingress controller. The client then sent an HTTP request to the Ingress controller and specified cyberchef.example.com in the Host header. From that header, the controller determined which service the client is trying to access, looked up the pod IPs through the Endpoints object associated with the service, and forwarded the client’s request to one of the pods. . | Exposing multiple services through the same ingress . | . - host: apps.example.com http: paths: - path: /cyberchef backend: serviceName: cyberchef servicePort: 80 - path: /linkding backend: serviceName: linkding servicePort: 80 . Exposing diff services to diff hosts | . spec: rules: - host: cyberchef.example.com http: paths: - path: / backend: serviceName: cyberchef servicePort: 80 - host: linkding.example.com http: paths: - path: / backend: serviceName: linkding servicePort: 80 . Configuring Ingress to handle TLS traffic ( for HTTPS ) | . When a client opens a TLS connection to an Ingress controller, the controller terminates the TLS connection. The communication between the client and the controller is encrypted, whereas the communication between the controller and the backend pod isn’t. The application running in the pod doesn’t need to support TLS. For example, if the pod runs a web server, it can accept only HTTP traffic and let the Ingress controller take care of everything related to TLS. To enable the controller to do that, you need to attach a certificate and a private key to the Ingress. The two need to be stored in a Kubernetes resource called a Secret, which is then referenced in the Ingress manifest. . openssl genrsa --out tls.key 2048 openssl req -new -x509 tls.key -out tls.cert -days 360 -subj /CN=cyberchef.example.com kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key . The private key and the certificate are now stored in the Secret called tls-secret. Now, you can update your Ingress object so it will also accept HTTPS requests for cyberchef.example.com. . apiVersion: extensions/v1beta1 kind: Ingress metadata: name: cyberchef spec: tls: - hosts: - cyberchef.example.com secretName: tls-secret rules: - host: cyberchef.example.com http: paths: - path: / backend: serviceName: cyberchef-nodeport servicePort: 80 . # Create Ingress $ kubectl apply -f cyberchef-ingress-tls.yaml $ curl -k -v https://cyberchef.example.com/cyberchef * About to connect() to cyberchef.example.com port 443 (#0) ... * Server certificate: * subject: CN=cyberchef.example.com ... &gt; GET /cyberchef HTTP/1.1 &gt; ... . 1.3.7 Signaling when a pod is ready to accept connections . Consider a scenario where a new pod started with proper labels is created, it becomes part of the service and requests start to be redirected to the pod. But what if the pod isn’t ready to start serving requests immediately? . The pod may need time to load either configuration or data, or it may need to perform a warm-up procedure to prevent the first user request from taking too long and affecting the user experience. In such cases you don’t want the pod to start receiving requests immediately, especially when the already-running instances can process requests properly and quickly. It makes sense to not forward requests to a pod that’s in the process of starting up until it’s fully ready. . Readiness probes . The readiness probe is invoked periodically and determines whether the specific pod should receive client requests or not. When a container’s readiness probe returns success, it’s signaling that the container is ready to accept requests. . How readiness probes works When a container is started, Kubernetes can be configured to wait for a configurable amount of time to pass before performing the first readiness check. After that, it invokes the probe periodically and acts based on the result of the readiness probe. If a pod reports that it’s not ready, . | TYPES OF READINESS PROBES An Exec probe, where a process is executed. The container’s status is determined by the process’ exit status code. | An HTTP GET probe, which sends an HTTP GET request to the container and the HTTP status code of the response determines whether the container is ready or not. | A TCP Socket probe, which opens a TCP connection to a specified port of the container. If the connection is established, the container is considered ready. | . | Difference between liveness probe and rediness probe Liveness probes keep pods healthy by killing off unhealthy containers and replacing them with new, healthy ones, whereas readiness probes make sure that only pods that are ready to serve requests receive them. . | Why we need readiness probe Imagine that a group of pods (for example, pods running application servers) depends on a service provided by another pod (a backend database, for example). If at any point one of the frontend pods experiences connectivity problems and can’t reach the database anymore, it may be wise for its readiness probe to signal to Kubernetes that the pod isn’t ready to serve any requests at that time. If other pod instances aren’t experiencing the same type of connectivity issues, they can serve requests normally. A readiness probe makes sure clients only talk to those healthy pods and never notice there’s anything wrong with the system. | . apiVersion: v1 kind: ReplicationController ... spec: ... template: ... spec: containers: - name: cyberchef image: mpepping/cyberchef readinessProbe: exec: command: - ls - /var/ready . The readiness probe will periodically perform the command ls /var/ready inside the container. The ls command returns exit code zero if the file exists, or a non-zero exit code otherwise. If the file exists, the readiness probe will succeed; otherwise, it will fail. Note: To check status check logs. . This mock readiness probe is useful only for demonstrating what readiness probes do. In the real world, the readiness probe should return success or failure depending on whether the app can (and wants to) receive client requests or not. Manually removing pods from services should be performed by either deleting the pod or changing the pod’s labels instead of manually flipping a switch in the probe. . 1.3.8 Using a headless service for discovering individual pods . If you want to connect to all of the pods. you need a IP of an individual pod, which is can’t possible in default configuration in kubernetes. One option is to have the client call the Kubernetes API server and get the list of pods and their IP addresses through an API call, but because you should always strive to keep your apps Kubernetes-agnostic, using the API server isn’t ideal. . Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually, when you perform a DNS lookup for a service, the DNS server returns a single IP—the service’s cluster IP. But if you tell Kubernetes you don’t need a cluster IP for your service (you do this by setting the clusterIP field to None in the service specification ), the DNS server will return the pod IPs instead of the single service IP. . Instead of returning a single DNS A record, the DNS server will return multiple A records for the service, each pointing to the IP of an individual pod backing the ser- vice at that moment. Clients can therefore do a simple DNS A record lookup and get the IPs of all the pods that are part of the service. The client can then use that information to connect to one, many, or all of them. . # All you need to do is to set ClusterIP to None apiVersion: v1 kind: Service metadata: name: cyberchef-headless spec: clusterIP: None ports: - port: 80 targetPort: 8000 selector: app: cyberchef . # Setup and dns lookup container ( We need nslookup and dif : tutum/dnsutils ) $ kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1 --command -- sleep infinity # Perform DNSLookup for headless $ kubectl exec dnsutils nslookup cyberchef-headless ... Name: cyberchef-headless.default.svc.cluster.local Address: 10.108.1.4 Name: cyberchef-headless.default.svc.cluster.local Address: 10.108.2.5 # Pod IP # Perform DNSLookup for non-headless $ kubectl exec dnsutils nslookup cyberchef ... Name: cyberchef.default.svc.cluster.local Address: 10.111.249.153 ## ClusterIP . 1.3.9 Troubleshooting Best Practices . First, make sure you’re connecting to the service’s cluster IP from within the cluster, not from the outside. | Don’t bother pinging the service IP to figure out if the service is accessible (remember, the service’s cluster IP is a virtual IP and pinging it will never work). | If you’ve defined a readiness probe, make sure it’s succeeding; otherwise the pod won’t be part of the service. | To confirm that a pod is part of the service, examine the corresponding Endpoints object with kubectl get endpoints. | If you’re trying to access the service through its FQDN or a part of it (for example, myservice.mynamespace.svc.cluster.local or myservice.mynamespace) and it doesn’t work, see if you can access it using its cluster IP instead of the FQDN. | Check whether you’re connecting to the port exposed by the service and not the target port. | Try connecting to the pod IP directly to confirm your pod is accepting connec- tions on the correct port. | If you can’t even access your app through the pod’s IP, make sure your app isn’t only binding to localhost. | . .",
            "url": "https://hacstac.github.io/Notes/kubernetes/2020/09/29/Kubernetes-Service-Discovery.html",
            "relUrl": "/kubernetes/2020/09/29/Kubernetes-Service-Discovery.html",
            "date": " • Sep 29, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Kubernetes Controllers",
            "content": ". 1.0 Replication Controller [ Deprecated ] . A ReplicationController is a Kubernetes resource that ensures its pods are always kept running. If the pod disappears for any reason, such as in the event of a node disappearing from the cluster or because the pod was evicted from the node, the ReplicationController notices the missing pod and creates a replacement pod. . . In the figure we saw that if a node goes down and takes two pods with it. Pod A was created directly and is therefore an unmanaged pod, while pod B is managed by a ReplicationController. After the node fails, the ReplicationController creates a new pod (pod B2) to replace the missing pod B, whereas pod A is lost completely— nothing will ever recreate it. . 1.1 The opration of a Replication Controller . A ReplicationController constantly monitors the list of running pods and makes sure the actual number of pods of a “type” always matches the desired number. If too few such pods are running, it creates new replicas from a pod template. If too many such pods are running, it removes the excess replicas. . 1.2 Three main parts of Replication Controller . A label selector, which determines what pods are in the ReplicationController’s scope | A replica count, which specifies the desired number of pods that should be running | A pod template, which is used when creating new pod replicas | . 1.3 Controller Reconciliation Loop . A ReplicationController’s job is to make sure that an exact number of pods always matches its label selector. If it doesn’t, the ReplicationController takes the appropriate action to reconcile the actual with the desired number. . . 1.4 Effect of changing the controller’s label selector or pod template . Changes to the label selector and the pod template have no effect on existing pods. Changing the label selector makes the existing pods fall out of the scope of the ReplicationController, so the controller stops caring about them. ReplicationCon- trollers also don’t care about the actual “contents” of its pods (the container images, environment variables, and other things) after they create the pod. The template therefore only affects new pods created by this ReplicationController. . 1.5 Benefits of Replication Controller . It makes sure a pod (or multiple pod replicas) is always running by starting a new pod when an existing one goes missing. | When a cluster node fails, it creates replacement replicas for all the pods that were running on the failed node (those that were under the Replication- Controller’s control). | It enables easy horizontal scaling of pods—both manual and automatic | . 1.6 Lets get into the Replication Controller . # Create a YAML file to create a Replication Controller cyberchef_rc.yml apiVersion: v1 kind: ReplicationController metadata: name: cyberchef spec: replicas: 3 selector: app: cyberchef template: metadata: labels: app: cyberchef spec: containers: - name: cyberchef image: mpepping/cyberchef ports: - containerPort: 8000 $ kubectl create -f ./cyberchef_rc.yml # Get Pods $ kubectl get pods NAME READY STATUS RESTARTS AGE cyberchef-gzg6n 1/1 Running 0 5m11s cyberchef-h4crn 1/1 Running 0 5m11s cyberchef-vlp7f 1/1 Running 0 5m11s # Trying to delete the pod $ kubectl delete pod cyberchef-vlp7f # Replication Controller Immediately spins up a new pod $ kubectl get pods NAME READY STATUS RESTARTS AGE cyberchef-gzg6n 1/1 Running 0 6m30s cyberchef-h4crn 1/1 Running 0 6m30s cyberchef-hs9cp 1/1 Running 0 9s # Getting details of replication controller $ kubectl get rc NAME DESIRED CURRENT READY AGE cyberchef 3 3 3 8m16s # Getting additional details about rc $ kubectl describe rc Name: cyberchef Namespace: default Selector: app=cyberchef Labels: app=cyberchef Annotations: &lt;none&gt; Replicas: 3 current / 3 desired Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: app=cyberchef Containers: cyberchef: Image: mpepping/cyberchef Port: 8000/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt; Events: Type Reason Age From Message - - - - Normal SuccessfulCreate 9m33s replication-controller Created pod: cyberchef-vlp7f Normal SuccessfulCreate 9m33s replication-controller Created pod: cyberchef-h4crn Normal SuccessfulCreate 9m33s replication-controller Created pod: cyberchef-gzg6n Normal SuccessfulCreate 3m12s replication-controller Created pod: cyberchef-hs9cp . 1.7 Moving pods in and out of the scope of a ReplicationController . Pods created by a ReplicationController aren’t tied to the ReplicationController in any way. At any moment, a ReplicationController manages pods that match its label selector. By changing a pod’s labels, it can be removed from or added to the scope of a ReplicationController. It can even be moved from one ReplicationController to another. . If you change a pod’s labels so they no longer match a ReplicationController’s label selector, the pod becomes like any other manually created pod. It’s no longer managed by anything. If the node running the pod fails, the pod is obviously not rescheduled. But keep in mind that when you changed the pod’s labels, the replication controller noticed one pod was missing and spun up a new pod to replace it. . There in below example, we now have four pods altogether: one that isn’t managed by our ReplicationController and three that are. Among them is the newly created pod. After we change the pod’s label from app=cyberchef to app=testing, the ReplicationController no longer cares about the pod. Because the controller’s replica count is set to 3 and only two pods match the label selector so replication controller will spin a new pod and this changed lebal pod is keep running but no one can manage this pod, so we can test this and perform operation on this. . $ kubectl get pods NAME READY STATUS RESTARTS AGE cyberchef-gbrgh 1/1 Running 0 12h cyberchef-h728j 1/1 Running 0 12h cyberchef-npql7 1/1 Running 0 12h $ kubectl label pod cyberchef-h728j type=vulnerable $ kubectl get pods --show-labels NAME READY STATUS RESTARTS AGE LABELS cyberchef-gbrgh 1/1 Running 0 12h app=cyberchef cyberchef-h728j 1/1 Running 0 12h app=cyberchef,type=vulnerable cyberchef-npql7 1/1 Running 0 12h app=cyberchef $ kubectl label pod cyberchef-h728j app=testing --overwrite $ kubectl get pods -L app NAME READY STATUS RESTARTS AGE APP cyberchef-gbrgh 1/1 Running 0 13h cyberchef cyberchef-h728j 1/1 Running 0 13h testing cyberchef-kwn7r 1/1 Running 0 2m44s cyberchef cyberchef-npql7 1/1 Running 0 13h cyberchef $ kubectl get rc NAME DESIRED CURRENT READY AGE cyberchef 3 3 3 13h . 1.8 Changing pod template . A ReplicationController’s pod template can be modified at any time. Changing the pod template is like replacing a cookie cutter with another one. It will only affect the cookies you cut out afterward and will have no effect on the ones you’ve already cut. To modify the old pods, you’d need to delete them and let the ReplicationController replace them with new ones based on the new template. . Editing a ReplicationController like this to change the container image in the pod template, deleting the existing pods, and letting them be replaced with new ones from the new template could be used for upgrading pods . # This will open the ReplicationController’s YAML definition in your default text editor. # Find the pod template section and add an additional label to the metadata # After you save your changes and exit the editor, kubectl will update the ReplicationController and print the following message: replicationcontroller/cyberchef edited $ kubectl edit rc cyberchef . 1.9 Horizontally scaling pods . Scaling the number of pods up or down is as easy as changing the value of the rep- licas field in the ReplicationController resource. After the change, the Replication- Controller will either see too many pods exist (when scaling down) and delete part of them, or see too few of them (when scaling up) and create additional pods. . # By kubectl command $ kubectl scale rc cyberchef --replicas=4 # By changing Yaml $ kubeclt edit rc cyberchef # Edit replicas section $ kubectl get rc NAME DESIRED CURRENT READY AGE cyberchef 4 4 4 2m . 1.10 Deleting a Replication Controller . # This will delete rc as well all the pods $ kubectl delete rc cyberchef # If you want to keep running your pods and delete the replication controller $ kubectl delete rc cyberchef --cascade=false # Now pods are running their own . . 2.0 ReplicaSets . A ReplicaSet acts as a cluster-wide Pod manager, ensuring that the right types and number of Pods are running at all times. . Initially, ReplicationControllers were the only Kubernetes component for replicating pods and rescheduling them when nodes failed. Later, a similar resource called a ReplicaSet was introduced. It’s a new generation of ReplicationController and replaces it completely (ReplicationControllers will eventually be deprecated). ReplicaSet also use same policy as replicationController, ReplicaSets create and manage Pods, they do not own the Pods they create. ReplicaSets use label queries to identify the set of Pods they should be managing. . If we going in deep detail, Despite the value placed on declarative configuration of software, there are times when it is easier to build something up imperatively. In particular, early on you may be simply deploying a single Pod with a container image without a ReplicaSet manag‐ ing it. But at some point you may want to expand your singleton container into a replicated service and create and manage an array of similar containers. You may have even defined a load balancer that is serving traffic to that single Pod. If ReplicaSets owned the Pods they created, then the only way to start replicating your Pod would be to delete it and then relaunch it via a ReplicaSet. This might be disruptive, as there would be a moment in time when there would be no copies of your container running. However, because ReplicaSets are decoupled from the Pods they manage, you can simply create a ReplicaSet that will “adopt” the existing Pod, and scale out additional copies of those containers. In this way, you can seamlessly move from a single imperative Pod to a replicated set of Pods managed by a ReplicaSet. . 2.1 Comparing a ReplicaSet to a ReplicationController . A ReplicaSet behaves exactly like a ReplicationController, but it has more expressive pod selectors. Whereas a ReplicationController’s label selector only allows matching pods that include a certain label, a ReplicaSet’s selector also allows matching pods that lack a certain label or pods that include a certain label key, regardless of its value. Also, for example, a single ReplicationController can’t match pods with the label env=production and those with the label env=devel at the same time. It can only match either pods with the env=production label or pods with the env=devel label. But a sin- gle ReplicaSet can match both sets of pods and treat them as a single group. Similarly, a ReplicationController can’t match pods based merely on the presence of a label key, regardless of its value, whereas a ReplicaSet can. For example, a Replica- Set can match all pods that include a label with the key env, whatever its actual value is (you can think of it as env=*). . 2.2 Reconciliation Loops . The central concept behind a reconciliation loop is the notion of desired state versus observed or current state. Desired state is the state you want. With a ReplicaSet, it is the desired number of replicas and the definition of the Pod to replicate. For example, “the desired state is that there are three replicas of a Pod running the cyberchef”. . In contrast, the current state is the currently observed state of the system. For example, “there are only two cyberchef Pods currently running.” The reconciliation loop is constantly running, observing the current state of the world and taking action to try to make the observed state match the desired state. For instance, with the previous examples, the reconciliation loop would create a new cyberchef Pod in an effort to make the observed state match the desired state of three replicas. . There are many benefits to the reconciliation loop approach to managing state. It is an inherently goal-driven, self-healing system, yet it can often be easily expressed in a few lines of code. As a concrete example of this, note that the reconciliation loop for ReplicaSets is a single loop, yet it handles user actions to scale up or scale down the ReplicaSet as well as node failures or nodes rejoining the cluster after being absent. . 2.3 Quarantining Containers . Oftentimes, when a server misbehaves, Pod-level health checks will automatically restart that Pod. But if your health checks are incomplete, a Pod can be misbehaving but still be part of the replicated set. In these situations, while it would work to simply kill the Pod, that would leave your developers with only logs to debug the problem. Instead, you can modify the set of labels on the sick Pod. Doing so will disassociate it from the ReplicaSet (and service) so that you can debug the Pod. The ReplicaSet con‐ troller will notice that a Pod is missing and create a new copy, but because the Pod is still running it is available to developers for interactive debugging, which is significantly more valuable than debugging from logs. . 2.4 DeepDive with ReplicaSet . You’re creating a resource of type ReplicaSet which has much the same contents as the ReplicationController you created earlier. The only difference is in the selector. Instead of listing labels the pods need to have directly under the selector property, you’re specifying them under selector .matchLabels. This is the simpler (and less expressive) way of defining label selectors in a ReplicaSet. . # Creating a YAML for ReplicaSet: cyberchef_rs.yml apiVersion: apps/v1 kind: ReplicaSet metadata: name: cyberchef labels: app: cyberchef spec: replicas: 3 selector: matchLabels: app: cyberchef template: metadata: labels: app: cyberchef spec: containers: - name: cyberchef image: mpepping/cyberchef:testing ports: - containerPort: 8000 # To submit the cyberChef replicaSet to the kubernetes API: $ kubectl apply -f ./cyberchef_rs.yml $ kubectl get rs NAME DESIRED CURRENT READY AGE cyberchef 3 3 3 40m # To get further details: $ kubectl describe rs Name: cyberchef Namespace: default Selector: app=cyberchef Labels: app=cyberchef Annotations: &lt;none&gt; Replicas: 3 current / 3 desired Pods Status: 3 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: app=cyberchef Containers: cyberchef: Image: mpepping/cyberchef:testing Port: 8000/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt; Events: Type Reason Age From Message - - - - Normal SuccessfulCreate 41m replicaset-controller Created pod: cyberchef-l68b2 Normal SuccessfulCreate 41m replicaset-controller Created pod: cyberchef-8hg5p Normal SuccessfulCreate 41m replicaset-controller Created pod: cyberchef-qfqrk # Finding a replicaSet from a pod # Method1: Check for annotation : kubernetes.io/created-by $ kubectl get pods &lt;pod-name&gt; -o yaml # Method2: Check for the kind $ kubectl get pods cyberchef-qm28m -o yaml | grep kind kind: ReplicaSet # Finding a Set of pods for a replicaSet # use the selector to filter out the pods that a replicaSet is using $ kubectl get pods -l app=cyberchef NAME READY STATUS RESTARTS AGE cyberchef-qm28m 1/1 Running 0 15m cyberchef-r6fff 1/1 Running 0 15m cyberchef-scjlr 1/1 Running 0 15m ## Scaling ReplicaSets # ReplicaSets are scaled up or down by updating the spec.replicas key on the ReplicaSet object stored in Kubernetes. # Easiest way to achieve this using scale command $ kubectl scale replicasets cyberchef --replicas=4 # Note : change ReplicaSet configuration file count as best practices, so use this imperative scale option only on emergency sitautions otherwise update the replicas from the files. # Autoscaling a ReplicaSet # If we understand this with a scenario, if the application uses the high cpu and memory kubernetes will automatically scale the application. # This scaling is based on responses of some custom application metrics. # Kubernetes can handle all of these scenarios via Horizontal Pod Autoscaling (HPA). ## Autoscaling based on cpu $ kubectl autoscale rs cyberchef --min=2 --max=5 --cpu-percent=80 # This command creates an autoscaler that scales between two and five replicas with a CPU threshold of 80%. # To view, modify, or delete this resource you can use the standard kubectl commands and the horizontalpodautoscalers resource. # horizontalpodautoscalers is quite a bit to type, but it can be shortened to hpa: $ kubectl get hpa # To Delete ReplicaSets $ kubectl delete rs cyberchef # If you don’t want to delete the Pods that are being managed by the ReplicaSet, you can set the --cascade flag to false to ensure only the ReplicaSet object is deleted and not the Pods: $ kubectl delete rs kuard --cascade=false . 2.5 Using the ReplicaSet more expressive label selectors . The main improvements of ReplicaSets over ReplicationControllers are their more expressive label selectors. You intentionally used the simpler matchLabels selector in the first ReplicaSet example to see that ReplicaSets are no different from ReplicationControllers. Now, you’ll rewrite the selector to use the more powerful matchExpressions property. . selector: matchExpressions: - key: app operator: In values: - cyberchef . You can add additional expressions to the selector. As in the example, each expression must contain a key, an operator, and possibly (depending on the operator) a list of values. You’ll see four valid operators: . In—Label’s value must match one of the specified values. | NotIn—Label’s value must not match any of the specified values. | Exists—Pod must include a label with the specified key (the value isn’t important). When using this operator, you shouldn’t specify the values field. | DoesNotExist—Pod must not include a label with the specified key. The values property must not be specified. | . If you specify multiple expressions, all those expressions must evaluate to true for the selector to match a pod. If you specify both matchLabels and matchExpressions, all the labels must match and all the expressions must evaluate to true for the pod to match the selector. . . 3.0 DaemonSets . DaemonSet: Running exactly one pod on each node . To run a pod on all cluster nodes, you create a DaemonSet object, which is much like a ReplicationController or a ReplicaSet, except that pods created by a DaemonSet already have a target node specified and skip the Kubernetes Scheduler. Whereas a ReplicaSet (or ReplicationController) makes sure that a desired number of pod replicas exist in the cluster, a DaemonSet doesn’t have any notion of a desired replica count. It doesn’t need it because its job is to ensure that a pod matching its pod selector is running on each node. . A DaemonSet ensures a copy of a Pod is running across a set of nodes in a Kuber‐ netes cluster. DaemonSets are used to deploy system daemons such as log collectors and monitoring agents, which typically must run on every node. DaemonSets share similar functionality with ReplicaSets; both create Pods that are expected to be long-running services and ensure that the desired state and the observed state of the cluster match. . If a node goes down, the DaemonSet doesn’t cause the pod to be created else- where. But when a new node is added to the cluster, the DaemonSet immediately deploys a new pod instance to it. It also does the same if someone inadvertently deletes one of the pods, leaving the node without the DaemonSet’s pod. Like a Replica- Set, a DaemonSet creates the pod from the pod template configured in it. . You can use labels to run DaemonSet Pods on specific nodes; for example, you may want to run specialized intrusion-detection software on nodes that are exposed to the edge network. . 3.1 DaemonSet Scheduler . By default a DaemonSet will create a copy of a Pod on every node unless a node selec‐ tor is used, which will limit eligible nodes to those with a matching set of labels. DaemonSets determine which node a Pod will run on at Pod creation time by specifying the nodeName field in the Pod spec. As a result, Pods created by DaemonSets are ignored by the Kubernetes scheduler. . Like ReplicaSets, DaemonSets are managed by a reconciliation control loop that measures the desired state (a Pod is present on all nodes) with the observed state (is the Pod present on a particular node?). Given this information, the DaemonSet controller creates a Pod on each node that doesn’t currently have a matching Pod. . If a new node is added to the cluster, then the DaemonSet controller notices that it is missing a Pod and adds the Pod to the new node. . 3.2 DeepDive with DaemonSet . DaemonSets are created by submitting a DaemonSet configuration to the Kubernetes API server. . # we can only run these pods on selected labels, like if some nodes have ssd&#39;s and we want to run those pods only on ssd based nodes. # Like in below example we use node selector to choose only those pods that have ssd&#39;s. --cyberchef_daemonSet.yml apiVersion: apps/v1 kind: DaemonSet metadata: name: cyberchef labels: app: cyberchef spec: selector: matchLabels: app: cyberchef template: metadata: labels: app: cyberchef spec: nodeSelector: disk: ssd containers: - name: cyberchef image: mpepping/cyberchef ports: - containerPort: 8000 # To Supply to Kubernetes API $ kubectl apply -f ./cyberchef_daemonSet.yml # Get the DaemonSet $ kubectl get ds kubectl get daemonset NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE cyberchef 2 2 0 2 0 &lt;none&gt; 4s # Get the pods $ kubectl get pods NAME READY STATUS RESTARTS AGE cyberchef-dnrx9 1/1 Running 0 53s cyberchef-gfhcl 1/1 Running 0 53s # Get further details $ kubectl describe daemonset cyberchef Name: cyberchef Selector: app=cyberchef Node-Selector: &lt;none&gt; Labels: app=cyberchef Annotations: deprecated.daemonset.template.generation: 1 Desired Number of Nodes Scheduled: 2 Current Number of Nodes Scheduled: 2 Number of Nodes Scheduled with Up-to-date Pods: 2 Number of Nodes Scheduled with Available Pods: 2 Number of Nodes Misscheduled: 0 Pods Status: 2 Running / 0 Waiting / 0 Succeeded / 0 Failed Pod Template: Labels: app=cyberchef Containers: cyberchef: Image: mpepping/cyberchef Port: 8000/TCP Host Port: 0/TCP Environment: &lt;none&gt; Mounts: &lt;none&gt; Volumes: &lt;none&gt; Events: Type Reason Age From Message - - - - Normal SuccessfulCreate 111s daemonset-controller Created pod: cyberchef-gfhcl Normal SuccessfulCreate 111s daemonset-controller Created pod: cyberchef-dnrx9 # Get the nodes $ kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready master 2d17h v1.19.2 minikube-m02 Ready &lt;none&gt; 2d16h v1.19.2 minikube-m03 Ready &lt;none&gt; 5m v1.19.2 # To check pod running on which node $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES cyberchef-dnrx9 1/1 Running 0 4m5s 172.17.0.2 minikube-m02 &lt;none&gt; &lt;none&gt; cyberchef-gfhcl 1/1 Running 0 4m5s 172.17.0.5 minikube &lt;none&gt; &lt;none&gt; # To limiting daemonSets to specific nodes # Change the labels of the node $ kubectl label node minikube-m03 disk=hdd --overwrite # Show Labels $ kubectl get nodes --show-labels NAME STATUS ROLES AGE VERSION LABELS minikube Ready master 2d17h v1.19.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disk=ssd ... minikube-m03 Ready master 2d17h v1.19.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disk=hdd ... minikube-m02 Ready master 2d17h v1.19.2 beta.kubernetes.io/arch=amd64,beta.kubernetes.io/os=linux,disk=ssd ... . 3.3 Limiting DaemonSets to Specific Nodes . The most common use case for DaemonSets is to run a Pod across every node in a Kubernetes cluster. However, there are some cases where you want to deploy a Pod to only a subset of nodes. For example, maybe you have a workload that requires a GPU or access to fast storage only available on a subset of nodes in your cluster. In cases like these, node labels can be used to tag specific nodes that meet workload requirements. . Adding Lables to Nodes | . DaemonSets to specific nodes is to add the desired set of labels to a subset of nodes. This can be achieved using the kubectl label command. . kubectl label nodes minikube ssd=true kubectl get nodes --selector ssd=true NAME STATUS AGE VERSION minikube Ready 2d17h v1.19.2 minikube-m02 Ready 2d17h v1.19.2 . NodeSelector | . Node selectors can be used to limit what nodes a Pod can run on in a given Kuber‐ netes cluster. Node selectors are defined as part of the Pod spec when creating a DaemonSet. . # I specified this on previous cyberchef daemon example x.yml apiVersion: kind: metadata: spec: template: metadata: spec: nodeSelector: ssd: &quot;true&quot; . 3.4 Updating a DaemonSet . DaemonSets are great for deploying services across an entire cluster, but what about upgrades? Prior to Kubernetes 1.6, the only way to update Pods managed by a DaemonSet was to update the DaemonSet and then manually delete each Pod that was managed by the DaemonSet so that it would be re-created with the new configuration. . DaemonSets can be rolled out using the same RollingUpdate strategy that deployments use. You can configure the update strategy using the spec.updateStrategy.type field, which should have the value RollingUpdate. When a DaemonSet has an update strategy of RollingUpdate, any change to the spec.template field (or subfields) in the DaemonSet will initiate a rolling update. . The RollingUpdate strategy gradually updates members of a DaemonSet until all of the Pods are running the new configuration. There are two parameters that control the rolling update of a DaemonSet: . spec.minReadySeconds, which determines how long a Pod must be “ready” before the rolling update proceeds to upgrade subsequent Pods | spec.updateStrategy.rollingUpdate.maxUnavailable, which indicates how many Pods may be simultaneously updated by the rolling update | . Once a rolling update has started, you can use the kubectl rollout commands to see the current status of a DaemonSet rollout. For example, kubectl rollout status daemonSets cyberchef will show the current rollout status of a DaemonSet named cyberchef. . 3.5 Deleting a DaemonSet . Deleting a DaemonSet is pretty straightforward using the kubectl delete command. Just be sure to supply the correct name of the DaemonSet you would like to delete: . # Delete using specification $ kubectl delete -f ./cyberchef_daemon.yaml # Delete using daemonSet $ kubectl delete ds cyberchef . . 4.0 Running pods that perform a single completable task . ReplicationControllers, ReplicaSets, and DaemonSets run continuous tasks that are never considered completed. Processes in such pods are restarted when they exit. But in a completable task, after its process terminates, it should not be restarted again. . In the event of a node failure, the pods on that node that are managed by a Job will be rescheduled to other nodes the way ReplicaSet pods are. In the event of a failure of the process itself (when the process returns an error exit code), the Job can be configured to either restart the container or not. . For example, Jobs are useful for ad hoc tasks, where it’s crucial that the task finishes properly. You could run the task in an unmanaged pod and wait for it to finish, but in the event of a node failing or the pod being evicted from the node while it is performing its task, you’d need to manually recreate it. Doing this manually doesn’t make sense—especially if the job takes hours to complete. . An example . batch_job.yml apiVersion: batch/v1 kind: Job metadata: name: batch-job spec: template: metadata: labels: app: batch-job spec: restartPolicy: Never containers: - name: pi image: perl command: [&quot;perl&quot;, &quot;-Mbignum=bpi&quot;, &quot;-wle&quot;, &quot;print bpi(50)&quot;] $ kubectl apply -f ./batch_job.yml # Get the jobs $ kubectl get jobs NAME COMPLETIONS DURATION AGE batch-job 1/1 44s 46s # Get the pods $ kubectl get pods NAME READY STATUS RESTARTS AGE batch-job-8tb2j 1/1 Running 0 1m # Showing the running job $ kubectl logs batch-job-8tb2j 3.1415926535897932384626433832795028841971693993751 # After completing the job NAME READY STATUS RESTARTS AGE batch-job-8tb2j 0/1 Completed 0 2m -- ## If you want to run multiple pods instances in a job # Jobs may be configured to create more than one pod instance and run them in parallel or sequentially. # This is done by setting the completions and the parallelism properties in the Job spec apiVersion: batch/v1 kind: Job metadata: name: multi-completion-batch-job spec: completions: 5 template: &lt;template is the same as in listing 4.11&gt; # This Job will run five pods one after the other. It initially creates one pod, and when the pod’s container finishes, it creates the second pod, and so on, until five pods complete successfully. If one of the pods fails, the Job creates a new pod, so the Job may create more than five pods overall. -- # Instead of running single Job pods one after the other, you can also make the Job run multiple pods in parallel. You specify how many pods are allowed to run in parallel with the parallelism Job spec property, as shown in the following listing. apiVersion: batch/v1 kind: Job metadata: name: multi-completion-batch-job spec: completions: 5 parallelism: 2 template: &lt;same as in listing 4.11&gt; -- # Scaling a Job : You can even change a Job’s parallelism property while the Job is running. # This is similar to scaling a ReplicaSet or ReplicationController, and can be done with the kubectl scale command: $ kubectl scale job batch-job --replicas 3 . . 5.0 Scheduling Jobs to run periodically or once in the future . Job resources run their pods immediately when you create the Job resource. But many batch jobs need to be run at a specific time in the future or repeatedly in the specified interval. In Linux and UNIX-like operating systems, these jobs are better known as cron jobs. Kubernetes supports them, too. . A cron job in Kubernetes is configured by creating a CronJob resource. The schedule for running the job is specified in the well-known cron format, so if you’re familiar with regular cron jobs, you’ll understand Kubernetes’ CronJobs in a matter of seconds. . At the configured time, Kubernetes will create a Job resource according to the Job template configured in the CronJob object. When the Job resource is created, one or more pod replicas will be created and started according to the Job’s pod template, as you learned in the previous section. There’s nothing more to it. . 5.1 Creating a Cronjob . cronjob.yml apiVersion: batch/v1beta1 kind: CronJob metadata: name: hello spec: schedule: &quot;*/1 * * * *&quot; jobTemplate: spec: template: spec: containers: - name: hello image: busybox args: - /bin/sh - -c - date; echo Hello from the Kubernetes cluster restartPolicy: OnFailure . 5.2 Understanding how scheduled jobs are run . Job resources will be created from the CronJob resource at approximately the scheduled time. The Job then creates the pods. . It may happen that the Job or pod is created and run relatively late. You may have a hard requirement for the job to not be started too far over the scheduled time. In that case, you can specify a deadline by specifying the startingDeadlineSeconds field in the CronJob. . # In thi example one of the times the job is supposed to run is 10:30:00. # If it doesn’t start by 10:30:15 for whatever reason, the job will not run and will be shown as Failed. apiVersion: batch/v1beta1 kind: CronJob spec: schedule: &quot;0,15,30,45 ****&quot; startingDeadlineSeconds: 15 . .",
            "url": "https://hacstac.github.io/Notes/kubernetes/2020/09/27/Kubernetes-Controllers.html",
            "relUrl": "/kubernetes/2020/09/27/Kubernetes-Controllers.html",
            "date": " • Sep 27, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Kubernetes Pods Deep Dive",
            "content": ". 1.0 Pods . POD: Kubernetes groups multiple containers into a single atomic unit called a Pod . A pod is a co-located group of containers and represents the basic building block in Kubernetes. Instead of deploying containers individually, we always deploy and operate on a pod of containers. We’re not implying that a pod always includes more than one container—it’s common for pods to contain only a single container. The key thing about pods is that when a pod does have multiple containers, all of them are always run on a single worker node—it never spans multiple worker nodes. A Pod represents a collection of application containers and volumes running in the same execution environment. Pods, not containers, are the smallest deployable arti‐ fact in a Kubernetes cluster. This means all of the containers in a Pod always land on the same machine. . Applications running in the same Pod share the same IP address and port space (net‐ work namespace), have the same hostname (UTS namespace), and can communicate using native interprocess communication channels over System V IPC or POSIX message queues (IPC namespace). However, applications in different Pods are isolated from each other; they have different IP addresses, different hostnames, and more. Containers in different Pods running on the same node might as well be on different servers. . . . 1.1 Some general FAQs regarding pods . 1.1.1 Why we need pods . Understanding why multiple containers are better than one container running multiple processes | . Imagine an app consisting of multiple processes that either communicate through IPC (Inter-Process Communication) or through locally stored files requires them to run on the same machine. Because in Kubernetes, we always run processes in containers, and each container is much like an isolated machine. Containers are designed to run only a single process per container (unless the process itself spawns child processes). If you run multiple unrelated operations in a single container, it is your responsibility to keep all those processes running, manage their logs, and so on. For example, you’d have to include a mechanism for automatically restarting individual processes if they crash. Also, all those processes would log to the same standard output, so you’d have difficulty figuring out what method logged. That’s why we need to run each process in its container, and because of this problem, we need a higher-level construct that will allow us to bind containers together and manage them as a single unit. This is the reasoning behind pods. . A pod of containers allows us to run near related processes together and provide them with (almost) the same environment as if they were all running in a single container, while keeping them somewhat isolated. This way, we can get the best of both worlds. We can take advantage of all the features containers provide, while at the same time giving the processes the illusion of running together. . Understanding the partial isolation between containers of the same pod | . We want containers inside each group to share certain resources, although not all, so that they’re not fully isolated. Kubernetes achieves this by config- uring Docker to have all containers of a pod share the same set of Linux namespaces instead of each container having its own set. Because all containers of a pod run under the same Network and UTS namespaces (we’re talking about Linux namespaces here), they all share the same hostname and network interfaces. Similarly, all containers of a pod run under the same IPC namespace and can communicate through IPC. In the latest Kubernetes and Docker versions, they can also share the same PID namespace, but that feature isn’t enabled by default. But when it comes to the filesystem, things are a little different. Because most of the container’s filesystem comes from the container image, by default, the filesystem of each container is fully isolated from other containers. . 1.1.2 How containers share the same IP and Port space in pods . One thing to stress here is that because containers in a pod run in the same Network namespace, they share the same IP address and port space. This means processes running in containers of the same pod need to take care not to bind to the same port numbers or they’ll run into port conflicts. But this only concerns containers in the same pod. Containers of different pods can never run into port conflicts, because each pod has a separate port space. All the containers in a pod also have the same loopback network interface, so a container can communicate with other containers in the same pod through localhost. All pods in a Kubernetes cluster reside in a single flat, shared, network-address space, which means every pod can access every other pod at the other pod’s IP address. No NAT (Network Address Translation) gateways exist between them. When two pods send network packets between each other, they’ll each see the actual IP address of the other as the source IP in the packet. . Consequently, communication between pods is always simple. It doesn’t matter if two pods are scheduled onto a single or onto different worker nodes; in both cases the containers inside those pods can communicate with each other across the flat NAT- less network, much like computers on a local area network (LAN), regardless of the actual inter-node network topology. Like a computer on a LAN, each pod gets its own IP address and is accessible from all other pods through this network established specifically for pods. This is usually achieved through an additional software-defined net- work layered on top of the actual network. . . 1.1.3 How to organizing containers across pods properly . Pods are relatively lightweight, we can have as many as we need without incurring almost any overhead. Instead of stuffing everything into a single pod, we should organize apps into multiple pods, where each one contains only tightly related components or processes. For Ex: a multi-tier application consisting of a frontend application server and a backend database should be configured as a single pod or as two pods? Although nothing is stopping us from running both the frontend server and the database in a single pod with two containers, it isn’t the most appropriate way. We’ve said that all containers of the same pod always run co-located, but do the web server and the database really need to run on the same machine? The answer is obviously no. . If both the frontend and backend are in the same pod, then both will always be run on the same machine. If you have a two-node Kubernetes cluster and only this single pod, you’ll only be using a single worker node and not taking advantage of the computational resources (CPU and memory) you have at your disposal on the second node. Splitting the pod into two would allow Kubernetes to schedule the frontend to one node and the backend to the other node, thereby improving the utilization of your infrastructure. . Another reason why you shouldn’t put them both into a single pod is scaling. A pod is also the basic unit of scaling. Kubernetes can’t horizontally scale individual containers; instead, it scales whole pods. If your pod consists of a frontend and a backend container, when you scale up the number of instances of the pod to, let’s say, two, you end up with two frontend containers and two backend containers. Usually, frontend components have completely different scaling requirements than the backends, so we tend to scale them individually. Not to mention the fact that backends such as databases are usually much harder to scale compared to (stateless) frontend web servers. . 1.1.4 When to use multiple containers in a pod . The main reason to put multiple containers into a single pod is when the application consists of one main process and one or more complementary processes. For example, the main container in a pod could be a web server that serves files from a certain file directory, while an additional container (a sidecar container) periodically downloads content from an external source and stores it in the web server’s directory. But always remember you need a good reason to place a two containers in a single pod. ( like another container for gathering logs or to be used for more specific need ) . 1.1.5 What is pod manifest . Pods are described in a Pod manifest. The Pod manifest is just a text-file representation of the Kubernetes API object. Kubernetes strongly believes in declarative configu‐ ration. Declarative configuration means that you write down the desired state of the world in a configuration and then submit that configuration to a service that takes actions to ensure the desired state becomes the actual state. . The Kubernetes API server accepts and processes Pod manifests before storing them in persistent storage (etcd). The scheduler also uses the Kubernetes API to find Pods that haven’t been scheduled to a node. The scheduler then places the Pods onto nodes depending on the resources and other constraints expressed in the Pod manifests. Multiple Pods can be placed on the same machine as long as there are sufficient resources. However, scheduling multiple replicas of the same application onto the same machine is worse for reliability, since the machine is a single failure domain. Consequently, the Kubernetes scheduler tries to ensure that Pods from the same application are distributed onto different machines for reliability in the presence of such failures. Once scheduled to a node, Pods don’t move and must be explicitly destroyed and rescheduled. . . 1.2 Let get into the pods . Pods and other Kubernetes resources are usually created by posting a JSON or YAML manifest to the Kubernetes REST API endpoint. Also, we can use other, simpler ways of creating resources, such as the kubectl run command, but this usually allow us to configure a limited set of properties. Additionally, defining all your Kubernetes objects from YAML files makes it possible to store them in a version control system, with all the benefits it brings. . 1.2.1 Main parts of pod definition . The three important sections of any pod defination is: Metadata includes the name, namespace, labels, and other information about the pod. | Spec contains the actual description of the pod’s contents, such as the pod’s containers, volumes, and other data. | Status contains the current information about the running pod, such as what condition the pod is in, the description and status of each container, and the pod’s internal IP and other basic info. The status part contains read-only runtime data that shows the state of the resource at a given moment. When creating a new pod, you never need to provide the status part. | . | . EX: let’s look at what a YAML definition for one of those pods looks like: I know this looks complicated, but it becomes simple once you understand the basics and know how to distinguish between the important parts and the minor details. Also, you can take comfort in the fact that when creating a new pod, the YAML you need to write is much shorter . linkding.yml apiVersion: v1 kind: Pod metadata: annotations: cattle.io/timestamp: &quot;2020-09-19T07:58:22Z&quot; cni.projectcalico.org/podIP: 10.42.0.66/32 cni.projectcalico.org/podIPs: 10.42.0.66/32 field.cattle.io/ports: &#39;[[{&quot;containerPort&quot;:9090,&quot;dnsName&quot;:&quot;linkding-hostport&quot;,&quot;hostPort&quot;:9090,&quot;kind&quot;:&quot;HostPort&quot;,&quot;name&quot;:&quot;port&quot;,&quot;protocol&quot;:&quot;TCP&quot;,&quot;sourcePort&quot;:9090}]]&#39; creationTimestamp: null generateName: linkding-5c77d5f4cf- labels: pod-template-hash: 5c77d5f4cf workload.user.cattle.io/workloadselector: deployment-default-linkding managedFields: - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:metadata: f:annotations: .: {} f:cattle.io/timestamp: {} f:field.cattle.io/ports: {} f:generateName: {} f:labels: .: {} f:pod-template-hash: {} f:workload.user.cattle.io/workloadselector: {} f:ownerReferences: .: {} k:{&quot;uid&quot;:&quot;762057a3-c1d7-4fa1-91ca-01e961fbc72c&quot;}: .: {} f:apiVersion: {} f:blockOwnerDeletion: {} f:controller: {} f:kind: {} f:name: {} f:uid: {} f:spec: f:containers: k:{&quot;name&quot;:&quot;linkding&quot;}: .: {} f:image: {} f:imagePullPolicy: {} f:livenessProbe: .: {} f:failureThreshold: {} f:initialDelaySeconds: {} f:periodSeconds: {} f:successThreshold: {} f:tcpSocket: .: {} f:port: {} f:timeoutSeconds: {} f:name: {} f:ports: .: {} k:{&quot;containerPort&quot;:9090,&quot;protocol&quot;:&quot;TCP&quot;}: .: {} f:containerPort: {} f:hostPort: {} f:name: {} f:protocol: {} f:readinessProbe: .: {} f:failureThreshold: {} f:initialDelaySeconds: {} f:periodSeconds: {} f:successThreshold: {} f:tcpSocket: .: {} f:port: {} f:timeoutSeconds: {} f:resources: {} f:securityContext: .: {} f:allowPrivilegeEscalation: {} f:capabilities: {} f:privileged: {} f:readOnlyRootFilesystem: {} f:runAsNonRoot: {} f:stdin: {} f:terminationMessagePath: {} f:terminationMessagePolicy: {} f:tty: {} f:volumeMounts: .: {} k:{&quot;mountPath&quot;:&quot;/etc/linkding/data&quot;}: .: {} f:mountPath: {} f:name: {} f:dnsPolicy: {} f:enableServiceLinks: {} f:restartPolicy: {} f:schedulerName: {} f:securityContext: {} f:terminationGracePeriodSeconds: {} f:volumes: .: {} k:{&quot;name&quot;:&quot;data&quot;}: .: {} f:hostPath: .: {} f:path: {} f:type: {} f:name: {} manager: kube-controller-manager operation: Update time: &quot;2020-09-19T07:58:22Z&quot; - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:metadata: f:annotations: f:cni.projectcalico.org/podIP: {} f:cni.projectcalico.org/podIPs: {} manager: calico operation: Update time: &quot;2020-09-19T07:58:26Z&quot; - apiVersion: v1 fieldsType: FieldsV1 fieldsV1: f:status: f:conditions: k:{&quot;type&quot;:&quot;ContainersReady&quot;}: .: {} f:lastProbeTime: {} f:lastTransitionTime: {} f:status: {} f:type: {} k:{&quot;type&quot;:&quot;Initialized&quot;}: .: {} f:lastProbeTime: {} f:lastTransitionTime: {} f:status: {} f:type: {} k:{&quot;type&quot;:&quot;Ready&quot;}: .: {} f:lastProbeTime: {} f:lastTransitionTime: {} f:status: {} f:type: {} f:containerStatuses: {} f:hostIP: {} f:phase: {} f:podIP: {} f:podIPs: .: {} k:{&quot;ip&quot;:&quot;10.42.0.66&quot;}: .: {} f:ip: {} f:startTime: {} manager: kubelet operation: Update time: &quot;2020-09-19T07:59:15Z&quot; ownerReferences: - apiVersion: apps/v1 blockOwnerDeletion: true controller: true kind: ReplicaSet name: linkding-5c77d5f4cf uid: 762057a3-c1d7-4fa1-91ca-01e961fbc72c selfLink: /api/v1/namespaces/default/pods/linkding-5c77d5f4cf-zlz6h spec: containers: - image: sissbruecker/linkding:latest imagePullPolicy: Always livenessProbe: failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 2 successThreshold: 1 tcpSocket: port: 9090 timeoutSeconds: 2 name: linkding ports: - containerPort: 9090 hostPort: 9090 name: port protocol: TCP readinessProbe: failureThreshold: 3 initialDelaySeconds: 10 periodSeconds: 2 successThreshold: 2 tcpSocket: port: 9090 timeoutSeconds: 2 resources: {} securityContext: allowPrivilegeEscalation: false capabilities: {} privileged: false readOnlyRootFilesystem: false runAsNonRoot: false stdin: true terminationMessagePath: /dev/termination-log terminationMessagePolicy: File tty: true volumeMounts: - mountPath: /etc/linkding/data name: data - mountPath: /var/run/secrets/kubernetes.io/serviceaccount name: default-token-zh8ch readOnly: true dnsPolicy: ClusterFirst enableServiceLinks: true nodeName: rancherserver priority: 0 restartPolicy: Always schedulerName: default-scheduler securityContext: {} serviceAccount: default serviceAccountName: default terminationGracePeriodSeconds: 30 tolerations: - effect: NoExecute key: node.kubernetes.io/not-ready operator: Exists tolerationSeconds: 300 - effect: NoExecute key: node.kubernetes.io/unreachable operator: Exists tolerationSeconds: 300 volumes: - hostPath: path: /data type: &quot;&quot; name: data - name: default-token-zh8ch secret: defaultMode: 420 secretName: default-token-zh8ch status: phase: Pending qosClass: BestEffort # This is what we actually write: # linkding.yaml apiVersion: v1 kind: Pod metadata: name: linkding spec: containers: - image: sissbruecker/linkding:latest name: linkding ports: - containerPort: 9090 protocol: TCP . 1.2.2 Lets create a pod . # Creation- # The simplest way to create a pod is via the imperative kubectl run command. $ kubectl run linkding --image=sissbruecker/linkding:latest --port=9090 --or-- # To create a pod from file $ kubectl apply -f ./linkding.yaml . 1.2.3 Get details of a pod . # Details # Listing pods $ kubectl get pods If you ran this command immediately after the Pod was created, you might see: NAME READY STATUS RESTARTS AGE linkding 0/1 Pending 0 1s # The Pending state indicates that the Pod has been submitted but hasn’t been scheduled yet. # Get small overview $ kubectl explain pods $ kubectl explain pod.spec # Getting the whole defination $ kubectl get pod linkding -o yaml # To get pod details $ kubectl describe pods linkding Name: linkding-5c77d5f4cf-zlz6h Namespace: default Priority: 0 Node: rancherserver/10.0.1.69 Start Time: Sat, 19 Sep 2020 07:58:23 +0000 Labels: pod-template-hash=5c77d5f4cf workload.user.cattle.io/workloadselector=deployment-default-linkding Annotations: cattle.io/timestamp: 2020-09-19T07:58:22Z cni.projectcalico.org/podIP: 10.42.0.120/32 cni.projectcalico.org/podIPs: 10.42.0.120/32 field.cattle.io/ports: [[{&quot;containerPort&quot;:9090,&quot;dnsName&quot;:&quot;linkding-hostport&quot;,&quot;hostPort&quot;:9090,&quot;kind&quot;:&quot;HostPort&quot;,&quot;name&quot;:&quot;port&quot;,&quot;protocol&quot;:&quot;TCP&quot;,&quot;sourcePort&quot;:9090}]... Status: Running IP: 10.42.0.120 IPs: IP: 10.42.0.120 Controlled By: ReplicaSet/linkding-5c77d5f4cf Containers: linkding: Container ID: docker://5afb45048bae434f94571685f822ba8922d86e829d10c0dd4d067cb7f3889ac2 Image: sissbruecker/linkding:latest Image ID: docker-pullable://sissbruecker/linkding@sha256:96089547c4829f6578d35ec2ba4dbfb7afced22c971b692c3a7ed0105ca59af8 Port: 9090/TCP Host Port: 9090/TCP State: Running Started: Wed, 23 Sep 2020 08:22:02 +0000 Last State: Terminated Reason: Error Exit Code: 137 Started: Wed, 23 Sep 2020 08:21:04 +0000 Finished: Wed, 23 Sep 2020 08:21:50 +0000 Ready: True Restart Count: 3 Liveness: tcp-socket :9090 delay=10s timeout=2s period=2s #success=1 #failure=3 Readiness: tcp-socket :9090 delay=10s timeout=2s period=2s #success=2 #failure=3 Environment: &lt;none&gt; Mounts: /etc/linkding/data from data (rw) /var/run/secrets/kubernetes.io/serviceaccount from default-token-zh8ch (ro) Conditions: Type Status Initialized True Ready True ContainersReady True PodScheduled True Volumes: data: Type: HostPath (bare host directory volume) Path: /data HostPathType: default-token-zh8ch: Type: Secret (a volume populated by a Secret) SecretName: default-token-zh8ch Optional: false QoS Class: BestEffort Node-Selectors: &lt;none&gt; Tolerations: node.kubernetes.io/not-ready:NoExecute for 300s node.kubernetes.io/unreachable:NoExecute for 300s Events: Type Reason Age From Message - - - - Normal SandboxChanged 9m1s kubelet, rancherserver Pod sandbox changed, it will be killed and re-created. Normal Pulling 7m12s kubelet, rancherserver Pulling image &quot;sissbruecker/linkding:latest&quot; Normal Pulled 6m55s kubelet, rancherserver Successfully pulled image &quot;sissbruecker/linkding:latest&quot; Normal Created 6m44s kubelet, rancherserver Created container linkding Normal Started 6m33s kubelet, rancherserver Started container linkding . 1.2.4 Deletion of pods . ## Deletion # deleting pod by a name $ kubectl delete pod linkding # Using the yaml, which is used when creating a pod $ kubectl delete -f ./linkding.yaml # delete multiple pods by a name $ kubectl delete pods pod1 pod2 # delete pods using the label selectors $ kubectl delete pod -l creation_method=manual # deleting pods by deleting the whole namespace $ kubectl delete ns custom-namespace # delete all pods in a namespace $ kubectl delete pods --all # delete all resources in a namespace $ kubectl delete all --all . 1.2.5 Accessing the pods . # Accessing the pod # Using the port forwarding $ kubectl port-forward linkding 9090:9090 # A secure tunnel is created from your local machine, through the Kubernetes master, to the instance of the Pod running on one of the worker nodes. As long as the port-forward command is still running, you can access the Pod (in this case the kuard web interface) at http://localhost:9090. # Get the logs # When your application needs debugging, it’s helpful to be able to dig deeper than describe to understand what the application is doing. Kubernetes provides two commands for debugging running containers. The kubectl logs command downloads the current logs from the running instance: $ kubectl logs linkding # If you are using multicontainer pods ( use container name ) $ kubectl logs linkding -c &lt;container&gt; # Running commands in container with exec # Sometimes logs are insufficient, and to truly determine what’s going on you need to execute commands in the context of the container itself. $ kubectl exec -it linkding date # Copying files to and from containers # At times you may need to copy files from a remote container to a local machine for more in-depth exploration. $ kubectl cp &lt;pod-name&gt;:/website/main.js ./main.js $ kubectl cp $HOME/config.txt &lt;pod-name&gt;:/config.txt # Copying files into a container is an anti-pattern. . 1.2.6 Health Checks . # Health Checks # When you run your application as a container in Kubernetes, it is automatically kept alive for you using a process health check. This health check simply ensures that the main process of your application is always running. If it isn’t, Kubernetes restarts it. # Add healthcheck apiVersion: v1 kind: Pod metadata: name: linkding spec: containers: - image: sissbruecker/linkding:latest name: linkding livenessProbe: httpGet: path: /healthy port: 9090 initialDeleySeconds: 5 timeoutSeconds: 1 periodSeconds: 10 failureThreshold: 3 ports: - containerPort: 9090 protocol: TCP # The preceding Pod manifest uses an httpGet probe to perform an HTTP GET request against the /healthy endpoint on port 9090 of the kuard container. # The probe sets an initialDelaySeconds of 5, and thus will not be called until 5 seconds after all the containers in the Pod are created. The probe must respond within the 1-second time‐out, and the HTTP status code must be equal to or greater than 200 and less than 400 to be considered successful. # Kubernetes will call the probe every 10 seconds. If more than three consecutive probes fail, the container will fail and restart. # Types of Health Checks # In addition to HTTP checks, Kubernetes also supports tcpSocket health checks that open a TCP socket; if the connection is successful, the probe succeeds. This style of probe is useful for non-HTTP applications; for example, databases or other non– HTTP-based APIs. # Finally, Kubernetes allows exec probes. These execute a script or program in the context of the container. Following typical convention, if this script returns a zero exit code, the probe succeeds; otherwise, it fails. exec scripts are often useful for custom application validation logic that doesn’t fit neatly into an HTTP call. . 1.2.7 Resource Management . # Resource Management ## Resource Requests: Minimum Required Resources # A Pod requests the resources required to run its containers. Kubernetes guarantees that these resources are available to the Pod. The most commonly requested resources are CPU and memory, but Kubernetes has support for other resource types as well, such as GPUs and more ## Resource limits # Requests are used when scheduling Pods to nodes. The Kubernetes scheduler will ensure that the sum of all requests of all Pods on a node does not exceed the capacity of the node # Therefore, a Pod is guaranteed to have at least the requested resources when running on the node. Importantly, “request” specifies a minimum. It does not specify a maximum cap on the resources a Pod may use apiVersion: v1 kind: Pod metadata: name: linkding spec: containers: - image: sissbruecker/linkding:latest name: linkding resources: requests: cpu: &quot;500m&quot; memory: &quot;128Mi&quot; limits: cpu: &quot;1000m&quot; memory: &quot;256Mi&quot; livenessProbe: httpGet: path: /healthy port: 9090 initialDeleySeconds: 5 timeoutSeconds: 1 periodSeconds: 10 failureThreshold: 3 ports: - containerPort: 9090 protocol: TCP . 1.2.8 Persistent Volumes with Pods . # Persisting Data with Volumes # When a Pod is deleted or a container restarts, any and all data in the container’s file‐system is also deleted # This is often a good thing, since you don’t want to leave around cruft that happened to be written by your stateless web application # In other cases, having access to persistent disk storage is an important part of a healthy application # Kubernetes models such persistent storage apiVersion: v1 kind: Pod metadata: name: linkding spec: containers: - image: sissbruecker/linkding:latest name: linkding resources: requests: cpu: &quot;500m&quot; memory: &quot;128Mi&quot; limits: cpu: &quot;1000m&quot; memory: &quot;256Mi&quot; livenessProbe: httpGet: path: /healthy port: 9090 initialDeleySeconds: 5 timeoutSeconds: 1 periodSeconds: 10 failureThreshold: 3 volumeMounts: - mountPath: &quot;/data&quot; name: &quot;linkding-data&quot; ports: - containerPort: 9090 protocol: TCP # Different ways of using volumes with pods ## Communication/Synchronization # To achieve this, the Pod uses an emptyDir volume. Such a volume is scoped to the Pod’s lifespan, but it can be shared between two containers. -- ## Cache # An application may use a volume that is valuable for performance, but not required for correct operation of the application # For example, perhaps the application keeps prerendered thumbnails of larger images. Of course, they can be reconstructed from the original images, but that makes serving the thumbnails more expensive # You want such a cache to survive a container restart due to a health-check failure, and thus emptyDir works well for the cache use case as well -- ## Persitent Data # Sometimes you will use a volume for truly persistent data—data that is independent of the lifespan of a particular Pod, and should move between nodes in the cluster if a node fails or a Pod moves to a different machine for some reason # To achieve this, Kubernetes supports a wide variety of remote network storage volumes, including widely supported protocols like NFS and iSCSI as well as cloud provider network storage like Amazon’s Elastic Block Store, Azure’s Files and Disk Storage, as well as Google’s Persistent Disk -- ## Mounting the host filesystem # Other applications don’t actually need a persistent volume, but they do need some access to the underlying host filesystem. For example, they may need access to the /dev filesystem in order to perform raw block-level access to a device on the system # For these cases, Kubernetes supports the hostPath volume, which can mount arbitrary locations on the worker node into the container. The previous example uses the hostPath volume type. -- ## Persisting Data Using Remote Disks # Oftentimes, you want the data a Pod is using to stay with the Pod, even if it is restarted on a different host machine # To achieve this, you can mount a remote network storage volume into your Pod. When using network-based storage, Kubernetes automatically mounts and unmounts the appropriate storage whenever a Pod using that volume is scheduled onto a particular machine # There are numerous methods for mounting volumes over the network # Kubernetes includes support for standard protocols such as NFS and iSCSI as well as cloud provider–based storage APIs for the major cloud providers (both public and private) # In many cases, the cloud providers will also create the disk for you if it doesn’t already exist volumes: - name: &quot;linkding&quot; nfs: server: my.nfs.server.local path: &quot;/exports&quot; . .",
            "url": "https://hacstac.github.io/Notes/kubernetes/2020/09/23/Kubernetes-Pods.html",
            "relUrl": "/kubernetes/2020/09/23/Kubernetes-Pods.html",
            "date": " • Sep 23, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Step-By-Step Process To Deploy Kubernetes On Your Raspberry PIs",
            "content": ". Overview . Kubernetes is one of orchestration system which is getting popular for the last five years. Google initially makes it, but now it’s maintained by its vast community worldwide. That’s why it might become a new standard for container orchestration. But having a Kubernetes cluster might be a bit expensive. You have to run at least two or three nodes to run the cluster and try several demo projects on that cluster. That could be overkill for someone. But how if we have a Kubernetes that might run in our local computer? That should be an exciting thing. . Hardware Need ( Must Have ) . Raspberry PI (3/4 Min 2GB RAM): Minimum 2 Nodes ( 1 Master Node, 1 Worker Node ) | Power Supply Unit ( 15W Type-C for Rasp4 or You can use POE Hat ) | Ethernet Cable ( Minimum CAT 5e/6 ( Rasp supports up to 1Gbps )) | Micro SD Card ( Min 8 GB, Recommended 32 GB ) | Good Case With FAN ( Saves your rasp from dirt and excessive heat ) | . Software Requirement . Etcher or Raspberry PI Imager | Ubuntu 20.4 64Bit ARM ( OS ) | . . Setup Process . Part - 1: Setup RaspberryPIs . Use Raspberry PI Imager (Recommended) or Etcher to boot ubuntu 20.4 64Bit on SD Card | When Imager flashes the OS in the SD Card, open the boot drive and create a file name ssh (Without any extension). | Boot your Rasps | Check the rasp IP from your router or use Nmap to network scan like this: nmap -sP 192.168.0.0/24 | SSH to rasps ( Ex: ssh ubuntu@10.0.1.49 ) | Default username : ubuntu &amp; default password : ubuntu | Do all the basics things you always do with your rasps like setup static IP, setup dotfiles, and other stuff. | . Part - 2: Setup MicroK8s . Microk8s provides a single command installation of the latest Kubernetes release on a local machine for development and testing. Setup is quick, fast (~30 sec), and supports many plugins, including Istio, with a single command. Since K8s is not the easiest thing to get started with, having a tool that would make it easy for you to get going is very desirable. . microk8s is strictly for Linux. There is no VM involved. It is distributed and runs as a snap — a pre-packaged application (similar to a Docker container). Snaps can be used on all major Linux distributions, including Ubuntu, Linux Mint, Debian, and Fedora. . A. Setup Docker and Do Some Other Tweaks Before Installing Kubernetes . It is not a surprise we are going to use Docker Engine for the container runtime. Despite there are alternatives in rkt, cri-o, and others. However, at a closer look, we can see Kubernetes uses containers. We use docker because it is the most famous container system and relatively easy to deploy. . # Do this on all of your nodes ( Rasps ) # Install Docker $ curl -sSL https://get.docker.com | sh -- ## Enable cgroups (Control Groups). Cgroups allow the Linux kernel to limit and isolate resources. # Practically speaking, this allows Kubernetes to manage better resources used by the containers it runs and increases security by isolating containers from one another. # Inspect Docker $ docker info (...) Cgroup Driver: cgroups (...) # If docker info shows this you need to change Cgroup to systemd, To allow systemd to act as the cgroups manager. # Create a /etc/docker/daemon.json ( Add the content to that file ) $ vim /etc/docker/daemon.json { &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: { &quot;max-size&quot;: &quot;100m&quot; }, &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;insecure-registries&quot; : [&quot;localhost:32000&quot;] } # Restart Docker and Inspect Again $ sudo systemctl restart docker # Inspect $ docker info (...) Cgroup Driver: systemd (...) # Enable cgroups limit support # Enable limit support, as shown by the warnings in the docker info output above. You need to modify the kernel command line to enable these options at boot. # For the Raspberry Pi 4, add the following to the /boot/firmware/cmdline.txt file: Manually cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1 --or-- Automatically with sed # Note the space before &quot;cgroup_enable=cpuset&quot;, to add a space after the last existing item on the line $ sudo sed -i &#39;$ s/$/ cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1/&#39; /boot/firmware/cmdline.txt -- ## Allow Iptables to see bridged traffic # According to the documentation, Kubernetes needs iptables to be configured to see bridged network traffic. # Enable net.bridge.bridge-nf-call-iptables and -iptables6 $ cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF # To check $ sudo sysctl --system . B. Setup MicroK8s . You may follow the installation instruction of MicroK8S in the official documentation. . Start with installing MicroK8s using Snap which will take only a few seconds. . # Do this on all the nodes ( Only the Installation step, Rest of the steps are for just master server ) ## Install MicroK8s $ sudo snap install microk8s --channel=1.19 --classic # Add user to group microk8s &amp; give user permission to ~/.kube $ sudo usermod -a -G microk8s user $ sudo chown -f -R user ~/.kube -- ## Check MicroK8s is Running $ sudo microk8s.status microk8s is running high-availability: no datastore master nodes: 10.0.1.2:19001 datastore standby nodes: none addons: disabled: dashboard # The Kubernetes dashboard dns # CoreDNS ha-cluster # Configure high availability on the current node helm # Helm 2 - the package manager for Kubernetes metrics-server # K8s Metrics Server for API access to service metrics storage # Storage class; allocates storage from host directory helm3 # Helm 3 - Kubernetes package manager host-access # Allow Pods connecting to Host services smoothly ingress # Ingress controller for external access metallb # Loadbalancer for your Kubernetes cluster rbac # Role-Based Access Control for authorisation registry # Private image registry exposed on localhost:32000 # Microk8s comes with a set of tools: microk8s.config microk8s.docker microk8s.inspect microk8s.kubectl microk8s.start microk8s.stop microk8s.disable microk8s.enable microk8s.istioctl microk8s.reset microk8s.status -- ## Check the nodes ( shows only master node ) $ microk8s.kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready &lt;none&gt; 33m v1.19.0-34+09a4aa08bb9e93 ## Add an alias for microk8s.kubectl to saving a ton of time $ sudo snap alias microk8s.kubectl kubectl ## Add AddOns $ sudo microk8s.enable dns dashboard ingress helm helm3 storage metrics-server prometheus # To Check $ microk8s status microk8s is running high-availability: no datastore master nodes: 10.0.1.2:19001 datastore standby nodes: none addons: enabled: dashboard # The Kubernetes dashboard dns # CoreDNS ha-cluster # Configure high availability on the current node helm # Helm 2 - the package manager for Kubernetes metrics-server # K8s Metrics Server for API access to service metrics ingress # Ingress controller for external access helm3 # Helm 3 - Kubernetes package manager storage # Storage class; allocates storage from host directory disabled: host-access # Allow Pods connecting to Host services smoothly metallb # Loadbalancer for your Kubernetes cluster rbac # Role-Based Access Control for authorisation registry # Private image registry exposed on localhost:32000 # To get details of all of your namespaces ( this returns your all the running services, pods, deployements and namespaces etc) $ sudo microk8s.kubectl get all --all-namespaces . C. Access the Kubernetes Dashboard . # We use the headless OS, So we have no option to access services other than exposing to the internal network. # By default Kubernetes dashboard is not accessible on the local network, but if you were using raspbionOS ( GUI ), you could access the clusterIP in your Raspberry pi. # --Exposing-- $ kubectl -n kube-system edit service kubernetes-dashboard # Change type: ClusterIP to NodePort # Get service port $ microk8s.kubectl --namespace=kube-system get service kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.152.183.188 &lt;none&gt; 443:30355/TCP 22m # Dashboard Access : https://MasterServer:Port ( Ex: https://10.0.1.86:30355 ) -- ## Kubernetes dashboard needs authentication to access to the dashboard # Method 1 : Generate Token $ token=$(microk8s kubectl -n kube-system get secret | grep default-token | cut -d &quot; &quot; -f1) $ microk8s kubectl -n kube-system describe secret $token # Copy Token and Paste on Dashboard Login # If you again need this token $ sudo microk8s.kubectl -n kube-system get secret # Look for something like this kubernetes-dashboard-token-r62xm $ sudo microk8s.kubectl -n kube-system describe secret kubernetes-dashboard-token-r62xm # Shows Secret on Terminal Window --or-- # Method 2 : Setup a Proxy # To Setup Proxy $ sudo microk8s.kubectl proxy --accept-hosts=.* --address=0.0.0.0 &amp; # Edit Dashboard Yaml $ sudo microk8s.kubectl -n kube-system edit deploy kubernetes-dashboard -o yaml # Add ( - --enable-skip-login ) spec: containers: - args: - --enable-skip-login # when login to dashboard just use ( skip ) option # Access the server ( Master Server IP: 10.0.1.86 ) # http://10.0.1.86:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ -- . D. Add Nodes To The Cluster . # On Master Server $ microk8s add-node # It give join command like this: microk8s join 10.0.1.86:25000/6cae23f7273dc6700f439f8c19abc7de # On Worker Nodes $ microk8s join 10.0.1.86:25000/6cae23f7273dc6700f439f8c19abc7de # To Check $ microk8s kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready &lt;none&gt; 64m v1.19.0-34+09a4aa08bb9e93 node1 Ready &lt;none&gt; 2m49s v1.19.0-34+09a4aa08bb9e93 . E. Deploy Some Fun . E1. Install Portainer . Portainer is a lightweight management UI that allows you to manage your different Docker environments easily. Portainer provides an easy and straightforward solution for managing Docker containers and Swarm services through a web interface. Portainer supports a wide range of features for managing the Docker containers, such as controlling the creation and deletion of Swarm services, user authentication, authorizations, connecting, executing commands in the console of running containers viewing containers’ logs. . ## Install Portainer # Using Arkade ( Works on arm &amp; amd64 ) # If Arkade shows cluster unreachable, use (kubectl config view --raw &gt;~/.kube/config) $ curl -sLS https://dl.get-arkade.dev | sudo sh $ arkade install portainer # Access Portainer UI on http://http://10.0.1.86:30777 ( Master Server IP : 10.0.1.86 ) # If you are using ( arm64 ( 64 bit OS on Rasps )) $ curl -LO https://raw.githubusercontent.com/portainer/portainer-k8s/master/portainer-nodeport.yaml $ kubectl apply -f portainer-nodeport.yaml # If it says it not exposed # microk8s kubectl expose deployment portainer --type=NodePort . . E2. Install Linkding ( Bookmark Manager ) . Linkding is a self-hosted bookmark service : sissbruecker/linkding . . # I m installing linkding with persistent storage ( To know about What persistent storage is, check out my Kubernetes 101 post ) # YAML for Persistent Volume ( pv.yml ) apiVersion: v1 kind: PersistentVolume metadata: name: data spec: accessModes: - ReadWriteOnce capacity: storage: 8Gi hostPath: path: /home/user/data storageClassName: development # YAML for Persistent Volume Claim ( pvc.yml ) apiVersion: v1 kind: PersistentVolumeClaim metadata: name: data spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: development # YAML for Linkding Container Deployment ( Install Linkding With Persistent Storage ) apiVersion: apps/v1 kind: Deployment metadata: name: linkding labels: application: frontend spec: replicas: 1 selector: matchLabels: application: frontend template: metadata: labels: application: frontend spec: containers: - name: linkding image: sissbruecker/linkding ports: - containerPort: 9090 imagePullPolicy: IfNotPresent volumeMounts: - name: data mountPath: /etc/linkding/data volumes: - name: data persistentVolumeClaim: claimName: data -- ## Create Deployments $ kubectl apply -f ./pv.yml $ kubectl apply -f ./pvc.yml $ kubectl apply -f ./linkding.yml # Apply Deployments $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE linkding 1/1 1 1 102s # Expose to the local network $ microk8s kubectl expose deployment linkding --type=NodePort # Get the Port $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE linkding NodePort 10.152.183.77 &lt;none&gt; 9090:31071/TCP 30s # Access the Linkding Bookmark Manager ( http://10.0.1.86:31071 ) -- # To create username password for Linkding ( Go to Portainer -&gt; Application -&gt; Linkding -&gt; Console Access # python manage.py createsuperuser --username=user --email=admin@example.com # user = user # password = you set in above command . . E3. Install CodeServer . CodeServer is nothing but VS Code on the browser ( To use on any machine anywhere and access it in the browser. ) : codercom/code-server . ## Installing code-server with persistent storage, because this gives us the freedom to access code files from desired directories -- # We already create PersitentVolume, but we need another claim for VSCode # YAML for Persistent Volume Claim ( code-server-pvc.yml ) apiVersion: v1 kind: PersistentVolumeClaim metadata: name: code-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 3Gi storageClassName: development # YAML for code-server container deployment code-server.yml apiVersion: apps/v1 kind: Deployment metadata: labels: app: code-server name: code-server spec: selector: matchLabels: app: code-server replicas: 3 template: metadata: labels: app: code-server spec: containers: - image: codercom/code-server:latest imagePullPolicy: IfNotPresent name: code-server env: - name: PASSWORD value: &quot;password&quot; volumeMounts: - name: data mountPath: /home/coder/project volumes: - name: data persistentVolumeClaim: claimName: code-data -- # Apply Deployments $ kubectl apply -f ./code-server-pvc.yml $ kubectl apply -f ./code-server.yml # Expose to internal network $ kubectl expose deploy code-server --type=NodePort --port=80 --target-port=8080 # Access the server on http://10.0.1.86 # To Scale the deployment $ kubectl scale deployment code-server --replicas=5 . . Conclusion . You should now have an operational Kubernetes master and several worker nodes ready to accept workloads. . I hope MicroK8 will be a great help for newcomers into Kubernetes to try it out and learn Kubernetes by playing with it. If you gave it a shot &amp; liked it, leave me a comment here! . .",
            "url": "https://hacstac.github.io/Notes/kubernetes/2020/09/15/MicroK8s-Installation-Guide.html",
            "relUrl": "/kubernetes/2020/09/15/MicroK8s-Installation-Guide.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Kubernetes 101",
            "content": ". 1.0 What is Kubernetes . Kubernetes is an open-source platform/tool created by Google. It is written in GO-Lang. So currently Kubernetes is an open-source project under Apache 2.0 license. Sometimes in the industry, Kubernetes is also known as “K8s”. With Kubernetes, you can run any Linux container across private, public, and hybrid cloud environments. Kubernetes provides some edge functions, such as Loadbalancer, Service discovery, and Roled Based Access Control(RBAC). . Basically, kubernetes is a software that allows us to deploy, manage and scale applications. The applications will be packed in containers and kubernetes groups them into units. It allows us to span our application over thousands of servers while looking like one single unit. . Key Features of Kubernetes Horizontal Scaling | Auto Scaling | Health check &amp; Self-healing | Load Balancer | Service Discovery | Automated rollbacks &amp; rollouts | Canary Deployment | . | . . 2.0 Why we need Kubernetes . First, we need to familier with few terms: . Monolithic Applications: Years ago, most software applications were big monoliths, running either as a single process or as a small number of processes spread across a handful of servers. These legacy systems are still widespread today. They have slow release cycles and are updated relatively infrequently. At the end of every release cycle, developers pack- age up the whole system and hand it over to the ops team, who then deploys and monitors it. In case of hardware failures, the ops team manually migrates it to the remaining healthy servers. So these components that are all tightly coupled together and have to be developed, deployed, and managed as one entity, because they all run as a single OS process. . Problems with Monolithic Applications: Change of one part of the application require a redeployment of the whole application. Requires powerful servers, Uses Vertical Scaling ( which is Very Expensive ) and If one components creates problem, the whole application becomes unscalable. | . Microservices Applications: Today, these big monolithic legacy applications are slowly being broken down into smaller, independently running components called microservices. Each microservice runs as an independent process and communicates with other microservices through simple, well defined interfaces ( APIs ). Microservices communicate through synchronous protocols such as HTTP, over which they usually expose RESTful (REpresentational State Transfer) APIs, or through asyn- chronous protocols such as AMQP (Advanced Message Queueing Protocol). . Microservices are decoupled from each other, they can be developed, deployed, updated, and scaled individually. This enables you to change components quickly and as often as necessary to keep up with today’s rapidly changing business requirements. . Problems with Microservices Based Applications: The bigger numbers of deployable components and increasingly larger datacenters, it becomes increasingly difficult to configure, manage, and keep the whole system running smoothly. It’s much harder to figure out where to put each of those components to achieve high resource utilization and thereby keep the hardware costs down and the one of biggest problem with scaling microservices is multiple apps that are running on same host mey have conflicting dependencies. One solution of this is applications could run in the exact same environment during development and in production so they have the exact same operating system, libraries, system configuration, networking environment, and everything else but this will not solve the conflicting, versions of libraries or different environment requirements to solve this issue we have great technology called containers. ( You are also provide Dedicated VM to particular service but it is not a ideal solution because it increases hardware cost and management) | . Doing all this manually is hard work. We need automation, which includes automatic scheduling of those components to our servers, automatic configuration, supervision, and failure-handling. This is where Kubernetes comes in. . . Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example, Kubernetes can easily manage a canary deployment for your system. . Before used Kubernetes, you need to prepare your infrastructure to deploy a new microservice. I believe it cost you a few days or weeks. Without Kubernetes, large teams would have to manually script the deployment workflows. With Kubernetes, you don’t need to create your deployment script manually and it will reduce the amount of time and resources spent on DevOps. . 2.1 What kubernetes provides . Service discovery and load balancing: Kubernetes can expose a container using the DNS name or using their own IP address. If traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is stable. | Storage orchestration: Kubernetes allows you to automatically mount a storage system of your choice, such as local storages, public cloud providers, and more. Automated rollouts and rollbacks You can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new containers for your deployment, remove existing containers and adopt all their resources to the new container. | Automatic bin packing: You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources. | Self-healing: Kubernetes restarts containers that fail, replaces containers, kills containers that don’t respond to your user-defined health check, and doesn’t advertise them to clients until they are ready to serve. | Secret and configuration management: Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your container images, and without exposing secrets in your stack configuration. | . 2.2 What kubernetes not provides . Kubernetes is a lot of good things, I hope to have been clear exposing all Kubernetes benefits. The main problem from Kubernetes newbie is that they discover it is not a PaaS (Platform as a Service) system like they suppose. Kubernetes is a lot of things but not an “all included” service. It is great and reduces the amount of work, especially on the sysadmin side, but doesn’t offer you any apart from the infrastructure. . Said that most of the things you are looking for in a fully-managed system are there: simplified deployments, scaling, load balancing, logging, and monitoring. Usually, you get a standard configuration from your hosting but you can theoretically customize it if you really need it. . It does not limit the types of applications supported. Everything is written into the container so every container application, no matter on technology, can be run. The counterpart is that you still have to define the container by hand. | It doesn’t offer an automated deployment. You just have to push to a docker repository your built images, no more. This is quite easy if you already work in a process with Continuous Integration, Delivery, and Deployment (CI/CD), but consider that without it will be quite tricky. | It does not provide any application-level services, just infrastructure. If you need a database, you have to buy a service or run it into a dedicated container. Of course, taking charges of backups and so on. | Most of the interactions with the system are by a command line that wraps API. That’s very good because it allows automating each setting. Commands syntax is very simple but, if you are looking to a system that is managed fully by a UI you are looking to the worst place. | . . 3.0 How does Kubernetes work . The system is composed of a master node and any number of worker nodes. When the developer submits a list of apps to the master, Kubernetes deploys them to the cluster of worker nodes. What node a component lands on doesn’t (and shouldn’t) matter—neither to the developer nor to the system administrator. . . Kubernetes will run your containerized app somewhere in the cluster, provide information to its components on how to find each other, and keep all of them running. Because your application doesn’t care which node it’s running on, Kubernetes can relocate the app at any time, and by mixing and matching apps, achieve far better resource utilization than is possible with manual scheduling. . 3.1 Architecture of Kubernetes Cluster . We’ve seen a bird’s-eye view of Kubernetes’ architecture. Now let’s take a closer look at what a Kubernetes cluster is composed of. At the hardware level, a Kubernetes cluster is composed of many nodes, which can be split into two types: . The master node, which hosts the Kubernetes Control Plane that controls and manages the whole Kubernetes system | Worker nodes that run the actual applications you deploy | . . 3.1.1 Master ( Control Plane ) . The Control Plane is what controls the cluster and makes it function. It consists of multiple components that can run on a single master node or be split across multiple nodes and replicated to ensure high availability. These components are . Kubernetes API Server: The application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster ( which admin and other control plane communicated with ). . | Scheduler: Scheduler watches API server for new Pod requests. It communicates with Nodes to create new pods and to assign work to nodes while allocating resources or imposing constraints. . | Controller Manager: Component on the master that runs controllers. Includes Node controller, Endpoint Controller, Namespace Controller, etc. ( Performs cluster-level functions, such as replicating components, keeping track of worker nodes, handling node failures, and so on ) . Node Controller: Responsible for noticing and responding when nodes go down. | Replication Controller: Responsible for maintaining the correct number of pods for every replication controller object in the system. | Endpoints Controller: Populates the Endpoints object (that is, it joins Services and Pods). | Service Account and Token Controllers: Create default accounts and API access tokens for new namespaces. | Cloud-Controller-Manager: Cloud-controller-manager runs controllers that interact with the underlying cloud providers. The cloud-controller-manager binary is an alpha feature introduced in Kubernetes release 1.6. Cloud-controller-manager runs cloud-provider-specific controller loops only. You must disable these controller loops in the Kube-controller-manager. You can disable the controller loops by setting the –cloud-provider flag to external when starting the Kube-controller-manager. | Node Controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding. | Route Controller: For setting up routes in the underlying cloud infrastructure. | Service Controller: For creating, updating, and deleting cloud provider load balancers. | Volume Controller: For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes. | . | etcd: A reliable distributed data store that persistently stores the cluster configuration. . | . The components of the Control Plane hold and control the state of the cluster, but they don’t run your applications. This is done by the (worker) nodes. . 3.1.2 Slave ( Worker Nodes ) . The worker nodes are the machines that run your containerized applications. The task of running, monitoring, and providing services to your applications is done by the following components: . Docker, rkt, or another container runtime, which runs your containers . | Kubelet: Kubectl registering the nodes with the cluster, watches for work assignments from the scheduler, instantiate new Pods, report back to the master. Container Engine: Responsible for managing containers, image pulling, stopping the container, starting the container, destroying the container, etc. . | Kube Proxy: Responsible for forwarding app user requests to the right pod (load-balances network traffic between application components). . | The sequence of deployment: DevOps -&gt; API Server -&gt; Scheduler -&gt; Cluster -&gt;Nodes -&gt; Kubelet -&gt; Container Engine -&gt; Spawn Container in Pod . | The sequence of App user request: App user -&gt; Kube proxy -&gt; Pod -&gt; Container(Your app is run here) . | . 3.2 The Six Layers of K8s . . Deployments create and manage ReplicaSets, which create and manage Pods, which run on Nodes, which have a container runtime, which run the app code you put in your Docker image. . The levels shaded blue are higher-level K8s abstractions. The green levels represent Nodes and Node subprocess that you should be aware of, but may not touch. . 3.2.1 Deployments . Although pods are the basic unit of computation in Kubernetes, they are not typically directly launched on a cluster. Instead, pods are usually managed by one more layer of abstraction: the deployment. A deployment’s primary purpose is to declare how many replicas of a pod should be running at a time. When a deployment is added to the cluster, it will automatically spin up the requested number of pods, and then monitor them. If a pod dies, the deployment will automatically re-create it. Using a deployment, you don’t have to deal with pods manually. You can just declare the desired state of the system, and it will be managed for you automatically. . 3.2.2 ReplicaSet . The Deployment creates a ReplicaSet that will ensure your app has the desired number of Pods. ReplicaSets will create and scale Pods based on the triggers you specify in your Deployment. Replication Controllers perform the same function as ReplicaSets, but Replication Controllers are old school. ReplicaSets are the smart way to manage replicated Pods in 2019. . 3.2.3 Pods . All containers will run in a pod. Pods abstract the network and storage away from the underlying containers. Your app will run here. Each pod has one unique IP address assigned which means one pod can communicate with each other like a traditional container in a docker environment. Each container inside the pod can reach all other pods into the virtual network, but cannot deep to the other container on other pods. That’s important to guarantee the pod abstraction: nobody has to know how the Pods are composed internally. Moreover, the IP assigned is volatile so you must use always the service name (resolved to the right IP directly). Pods are used as the unit of replication in Kubernetes. If your application becomes too popular and a single pod instance can’t carry the load, Kubernetes can be configured to deploy new replicas of your pod to the cluster as necessary. Even when not under heavy load, it is standard to have multiple copies of a pod running at any time in a production system to allow load balancing and failure resistance. Pods handle Volumes, Secrets, and configuration for containers. Pods are ephemeral. They are intended to be restarted automatically when they die. . Note: Worker Node is already explained in above section . 3.3 Key Terms used with Kubernetes . 3.3.1 Services . The name “service” in informatics science is overused. In Kubernetes scope think to a service like something you want to serve. A Kubernetes service involves a set of pods and may offer a complex feature or just expose a single Pod with a single container. So you can have a service that provides a CMS feature, with database and web server inside, or two different services, one for the database and one for the webserver. That’s up to you. Basically it is abstraction layer on top of a set of ephemeral pods (think of this as the ‘face’ of a set of pods) . 3.3.2 Ingress . By default, Kubernetes provides isolation between pods and the outside world. If you want to communicate with a service running in a pod, you have to open up a channel for communication. This is referred to as ingress. To do this there is an ingress controller that does something similar to a load balancer. It virtually forwards traffic from outside to the services. During this step, based on the ingress implementation you have chosen, you can add HTTPS encryption, route traffic based on the hostname or URL segments. The only service can be linked to the ingress controller, not Pods. There are multiple ways to add ingress to your cluster. The most common ways are by adding either an Ingress controller, or a LoadBalancer. . 3.3.3 Volume . By default Pod storage is volatile. This is important to know because at the first restart you will lose everything. Kubernetes volumes allow mapping some part of the hard drive of containers to a safe place. That space can be shared between containers. The mount point can be any part of the container, but a volume cannot be mount into another one. . PersistentVolumes and PersistentVolumeClaims: To help abstract away infrastructure specifics, K8s developed PersistentVolumes and PersistentVolumeClaims. Unfortunately the names are a bit misleading, because vanilla Volumes can have persistent storage, as well. PersisententVolumes (PV) and PersisentVolumeClaims (PVC) add complexity compared to using Volumes alone. However, PVs are useful for managing storage resources for large projects. With PVs, a K8s user still ends up using a Volume, but two steps are required first. . A PersistentVolume is provisioned by a Cluster Administrator (or it’s provisioned dynamically). | An individual Cluster user who needs storage for a Pod creates PersistentVolumeClaim manifest. It specifies how much and what type of storage they need. K8s then finds and reserves the storage needed. | The user then creates a Pod with a Volume that uses the PVC. PersistentVolumes have lifecycles independent of any Pod. In fact, the Pod doesn’t even know about the PV, just the PVC. PVCs consume PV resources, analogously to how Pods consume Node resources. . 3.3.4 Namespaces . Think to namespace like the feature that makes Kubernetes multitenant. The namespace is the tenant level. Each namespace can partitioning resources to isolate services, ingress, and many other things. This feature is good to have a strong separation between application, delegate safely to different teams, and have separated environments in a single infrastructure. It is a virtual cluster on top of an underlying physical cluster . 3.3.5 Labels and Selectors . Labels are key-value pairs used to tag objects. Objects are the items that you create in a Kubernetes cluster (pods, deployments, replica sets, services, volumes etc). Selectors are used to collect objects based on tags. . 3.3.6 StatefulSets . As we know, a ReplicaSet creates and manages Pods. If a Pod shuts down because a Node fails, a ReplicaSet can automatically replace the Pod on another Node. You should generally create a ReplicaSet through a Deployment rather than creating it directly, because it’s easier to update your app with a Deployment. . . Sometimes your app will need to keep information about its state. You can think of state as the current status of your user’s interaction with your app. So in a video game it’s all the unique aspects of the user’s character at a point in time. What do you do when your app has state you need to keep track of? Use a StatefulSet. . Like a ReplicaSet, a StatefulSet manages deployment and scaling of a group of Pods based on a container spec. Unlike a Deployment, a StatefulSet’s Pods are not interchangeable. Each Pod has a unique, persistent identifier that the controller maintains over any rescheduling. StatefulSets for good for persistent, stateful backends like databases. The state information for the Pod is held in a Volume associated with the StatefulSet. . 3.3.7 DaemonSets . DaemonSets are for continuous process. They run one Pod per Node. Each new Node added to the cluster automatically gets a Pod started by the DaemonSet. DaemonSets are useful for ongoing background tasks such as monitoring and log collection. StatefulSets and DaemonSets are not controlled by a Deployment. Although they are at the same level of abstraction as a ReplicaSet, there is not a higher level of abstraction for them in the current API. . 3.3.8 ETCD . It stores the configuration information which can be used by each of the nodes in the cluster. It is a high availability key-value store that can be distributed among multiple nodes. It is accessible only by Kubernetes API server as it may have some sensitive information. It is a distributed key-value store which is accessible to all. ETCD is a distributed reliable key-value store used by Kubernetes to store all data used to manage the cluster. Think of it this way, when you have multiple nodes and multiple masters in your cluster, etcd stores all that information on all the nodes in the cluster in a distributed manner. ETCD is responsible for implementing locks within the cluster to ensure there are no conflicts between the Masters. . 3.3.9 Cluster IP . Only has a Virtual IP (also called Cluster IP). This service can be used within a cluster only Acts like a traffic router to your Pods inside the cluster. Each port exposed by a pod will need a service if you want a client to talk to it via that port. By default, the service port is the same as the port exposed by the Pod. Different methods to access this service: From any node inside the cluster: &lt;cluster IP&gt;:&lt;service port&gt; From the node where a replica of the Pod is running: &lt;pod IP&gt;:&lt;pod port&gt;. . 3.3.10 Node Port . For this service, a physical port on the node is mapped to the service port and the service connects to the Pod via the pod port. Remember, this will also have a Virtual IP (i.e. cluster IP). Different methods to access this service: From outside the cluster, if any node in the cluster has a public IP: &lt;node public IP&gt;:&lt;node port&gt; From any node inside the cluster: &lt;cluster IP&gt;:&lt;service port&gt; From the node where a replica of the Pod is running: &lt;pod IP&gt;:&lt;pod port&gt;. . 3.3.11 Load Balancer . Giving public access to a node in your cluster is not a recommended method, When you want to give public access to a service, create a load balancer service (not getting into ingress at this stage). For each service that you want to give public access, you need to create a load balancer service, This type of service will generally work only in a cloud environment. Remember, this will also have a Virtual IP (i.e. cluster IP) This service will have a node port too Different methods to access this service: From outside the cluster: &lt;load balancer dns&gt;:&lt;service port&gt; From outside the cluster, if any node in the cluster has a public IP: &lt;node public IP&gt;:&lt;node port&gt; From any node inside the cluster: &lt;cluster IP&gt;:&lt;service port&gt; From the node where a replica of the Pod is running: &lt;pod IP&gt;:&lt;pod port&gt;. . 3.3.12 External Name . This maps a service to endpoints completely outside of the cluster. . . 4.0 Befits of using kubernetes . 4.1 Kubernetes will keeping your containers running . Once the application is running, Kubernetes continuously makes sure that the deployed state of the application always matches the description you provided. For example, if you specify that you always want five instances of a web server running, Kubernetes will always keep exactly five instances running. If one of those instances stops working properly, like when its process crashes or when it stops responding, Kubernetes will restart it automatically. Similarly, if a whole worker node dies or becomes inaccessible, Kubernetes will select new nodes for all the containers that were running on the node and run them on the newly selected nodes. . 4.2 Scaling the number of copies . While the application is running, you can decide you want to increase or decrease the number of copies, and Kubernetes will spin up additional ones or stop the excess ones, respectively. You can even leave the job of deciding the optimal number of cop- ies to Kubernetes. It can automatically keep adjusting the number, based on real-time metrics, such as CPU load, memory consumption, queries per second, or any other metric your app exposes. . 4.3 Hitting a moving target . We’ve said that Kubernetes may need to move your containers around the cluster This can occur when the node they were running on has failed or because they were evicted from a node to make room for other containers. If the container is providing a service to external clients or other containers running in the cluster, how can they use the container properly if it’s constantly moving around the cluster? And how can clients connect to containers providing a service when those containers are replicated and spread across the whole cluster? To allow clients to easily find containers that provide a specific service, you can tell Kubernetes which containers provide the same service and Kubernetes will expose all of them at a single static IP address and expose that address to all applications run- ning in the cluster. This is done through environment variables, but clients can also look up the service IP through good old DNS. The kube-proxy will make sure connec- tions to the service are load balanced across all the containers that provide the service. The IP address of the service stays constant, so clients can always connect to its con- tainers, even when they’re moved around the cluster. . 4.4 Simplifying application deployment . Kubernetes exposes all its worker nodes as a single deployment platform, application developers can start deploying applications on their own and don’t need to know anything about the servers that make up the cluster. In essence, all the nodes are now a single bunch of computational resources that are waiting for applications to consume them. A developer doesn’t usually care what kind of server the application is running on, as long as the server can provide the application with adequate system resources. Certain cases do exist where the developer does care what kind of hardware the application should run on. ( Like some app needs SSD deployment instead of HDD ) . 4.5 Achieving better utilization of hardware . By setting up Kubernetes on your servers and using it to run your apps instead of running them manually, you’ve decoupled your app from the infrastructure. When you tell Kubernetes to run your application, you’re letting it choose the most appropriate node to run your application on based on the description of the application’s resource requirements and the available resources on each node. By using containers and not tying the app down to a specific node in your cluster, you’re allowing the app to freely move around the cluster at any time, so the different app components running on the cluster can be mixed and matched to be packed tightly onto the cluster nodes. This ensures the node’s hardware resources are utilized as best as possible. . The ability to move applications around the cluster at any time allows Kubernetes to utilize the infrastructure much better than what you can achieve manually. . 4.6 Health checking and self-healing . Having a system that allows moving an application across the cluster at any time is also valuable in the event of server failures. As your cluster size increases, you’ll deal with failing computer components ever more frequently. Kubernetes monitors your app components and the nodes they run on and auto- matically reschedules them to other nodes in the event of a node failure. This frees the ops team from having to migrate app components manually and allows the team to immediately focus on fixing the node itself and returning it to the pool of available hardware resources instead of focusing on relocating the app. . 4.7 Automatic scaling . Using Kubernetes to manage your deployed applications also means the ops team doesn’t need to constantly monitor the load of individual applications to react to sudden load spikes. As previously mentioned, Kubernetes can be told to monitor the resources used by each application and to keep adjusting the number of running instances of each application. If Kubernetes is running on cloud infrastructure, where adding additional nodes is as easy as requesting them through the cloud provider’s API, Kubernetes can even automatically scale the whole cluster size up or down based on the needs of the deployed applications. . 4.8 Simplifying application development . Then there’s the fact that developers don’t need to implement features that they would usually implement. This includes discovery of services and/or peers in a clustered application. Kubernetes does this instead of the app. Usually, the app only needs to look up certain environment variables or perform a DNS lookup. If that’s not enough, the application can query the Kubernetes API server directly to get that and/or other information. Querying the Kubernetes API server like that can even save developers from having to implement complicated mechanisms such as leader election. . 4.9 Storage orchestration . Consequently, mount local or public cloud or network storage. . 4.10 Secret and configuration management . Create and update secrets and configs without rebuilding your image . 4.11 Horizontal Scaling . first, we need to know what are the horizontal and vertical scalings are: . Vertically Scaling : Adding more CPUs, memory, and other server components | Horizontal Scaling : Setting up additional Servers and running multiple copies | . Kubernetes uses horizontal scaling, which provides greater flexibilities to the companies . . 5.0 Ways to spin up kubernetes cluster . 5.1 Kubeadm . The official CNCF tool for provisioning Kubernetes clusters in a variety of shapes and forms (e.g. single-node, multi-node, HA, self-hosted) . 5.2 Minikube . Minikube can run on Windows and MacOS, because it relies on virtualization (e.g. Virtualbox) to deploy a kubernetes cluster in a Linux VM. You can also run minikube directly on linux with or without virtualization. It also has some developer-friendly features, like add-ons. . From a user perspective minikube is a very beginner friendly tool. You start the cluster using minikube start, wait a few minutes and your kubectl is ready to go. To specify a Kubernetes version you can use the --kubernetes-version flag. . If you are new to Kubernetes the first class support for its dashboard that minikube offers may help you. With a simple minikube dashboard the application will open up giving you a nice overview of everything that is going on in your cluster. This is being achieved by minikube’s addon system that helps you integrating things like, Helm, Nvidia GPUs and an image registry with your cluster. . 5.3 MicroK8s . MicroK8s is a Kubernetes distribution from Canonical that is designed for fast and simple deployment, which makes it a good choice to run Kubernetes locally. . Microk8s is similar to minikube in that it spins up a single-node Kubernetes cluster with its own set of add-ons. If MicroK8s runs on Linux, it also offers the advantage of not requiring VMs. On Windows and macOS, MicroK8s uses a VM framework called Multipass to create VMs for the Kubernetes cluster. . Like minikube, microk8s is limited to a single-node Kubernetes cluster, with the added limitation of only running on Linux and only on Linux where snap is installed. . 5.4 K3s . K3s is a minified version of Kubernetes developed by Rancher Labs. By removing dispensable features (legacy, alpha, non-default, in-tree plugins) and using lightweight components (e.g. sqlite3 instead of etcd3) they achieved a significant downsizing. This results in a single binary with a size of around 60 MB. . K3s runs on any Linux distribution without any additional external dependencies or tools. It is marketed by Rancher as a lightweight Kubernetes offering suitable for edge environments, IoT devices, CI pipelines, and even ARM devices, like Raspberry Pi’s. K3s achieves its lightweight goal by stripping a bunch of features out of the Kubernetes binaries (e.g. legacy, alpha, and cloud-provider-specific features), replacing docker with containerd, and using sqlite3 as the default DB (instead of etcd). As a result, this lightweight Kubernetes only consumes 512 MB of RAM and 200 MB of disk space. K3s has some nice features, like Helm Chart support out-of-the-box. . Unlike the previous two offerings, K3s can do multiple node Kubernetes cluster. However, due to technical limitations of SQLite, K3s currently does not support High Availability (HA), as in running multiple master nodes. . One feature that stands out is called auto deployment. It allows you to deploy your Kubernetes manifests and Helm charts by putting them in a specific directory. K3s watches for changes and takes care of applying them without any further interaction. This is especially useful for CI pipelines and IoT devices (both target use cases of K3s). Just create/update your configuration and K3s makes sure to keep your deployments up to date. . 5.5 Kind . Kind (Kubernetes-in-Docker), as the name implies, runs Kubernetes clusters in Docker containers. This is the official tool used by Kubernetes maintainers for Kubernetes v1.11+ conformance testing. It supports multi-node clusters as well as HA clusters. Because it runs K8s in Docker, kind can run on Windows, Mac, and Linux. . Kind is optimized first and foremost for CI pipelines, so it may not have some of the developer-friendly features of other offerings. Kind leads to a significantly faster startup speed compared to spawning VM. . Creating a cluster is very similar to minikube’s approach. Executing kind create cluster, playing the waiting game and afterwards you are good to go. By using different names (–name) kind allows you to create multiple instances in parallel. . If you are looking for a way to programmatically create a Kubernetes cluster, kind kindly (you have been for waiting for this, don’t you ) publishes its Go packages that are used under the hood. . 5.6 K3d . A new project that aims to bring K3s-in-Docker (similar to kind). . .",
            "url": "https://hacstac.github.io/Notes/kubernetes/2020/09/15/Kubernetes-Introduction.html",
            "relUrl": "/kubernetes/2020/09/15/Kubernetes-Introduction.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Docker Hard Parts [Part - 2]",
            "content": "7.0 Building Images automatically with DockerFiles . . 7.1 Instructions . .dockerignore | FROM Sets the Base Image for subsequent instructions. | MAINTAINER (deprecated - use LABEL instead) Set the Author field of the generated images. | RUN execute any commands in a new layer on top of the current image and commit the results. | CMD provide defaults for an executing container. | EXPOSE informs Docker that the container listens on the specified network ports at runtime. NOTE: does not actually make ports accessible. | ENV sets environment variable. | ADD copies new files, directories or remote file to container. Invalidates caches. Avoid ADD and use COPY instead. | COPY copies new files or directories to container. By default this copies as root regardless of the USER/WORKDIR settings. Use --chown=&lt;user&gt;:&lt;group&gt; to give ownership to another user/group. (Same for ADD.) | ENTRYPOINT configures a container that will run as an executable. | VOLUME creates a mount point for externally mounted volumes or other containers.* USER sets the user name for following RUN / CMD / ENTRYPOINT commands. | WORKDIR sets the working directory. | ARG defines a build-time variable. | ONBUILD adds a trigger instruction when the image is used as the base for another build. | STOPSIGNAL sets the system call signal that will be sent to the container to exit. | LABEL apply key/value metadata to your images, containers, or daemons. | . NOTE : If your running dangerous commands ( add a logic that it fails if your shell is outside docker ) . # Shell script fails if it’s run outside a container #!/bin/bash if ! [ -f /.dockerenv ] then echo &#39;Not in a Docker container, exiting.&#39; exit 1 fi . . 7.2 Getting Practical . 7.2.1 Packaging Git with Dockerfile . # Create a file name Dockerfile FROM ubuntu:latest LABEL maintainer=&quot;dia@allingeek.com&quot; RUN apt-get update &amp;&amp; apt-get install -y git ENTRYPOINT [&quot;git&quot;] # Instantly Create Dockerfile Images $ docker build -t htop - &lt;&lt; EOF FROM alpine RUN apk --no-cache add htop EOF $ docker image build --tage ubuntu-git:auto . FROM ubuntu:latest — Tells Docker to start from the latest Ubuntu image just as you did when creating the image manually. LABEL maintainer — Sets the maintainer name and email for the image. Provid- ing this information helps people know whom to contact if there’s a problem with the image. This was accomplished earlier when you invoked commit. RUN apt-get update &amp;&amp; apt-get install -y git — Tells the builder to run the provided commands to install Git. ENTRYPOINT [&quot;git&quot;] — Sets the entrypoint for the image to git. # --file or -f will read from diff file like &#39;BuildScript or anything&#39; # --quiet or -q will run in quiet mode . . 7.2.2 Dockerignore . .dockerignore file will help us to exclude some files to add to the image during the build | CLI modifies the context to exclude files and directories that match patterns in it. This helps to avoid unnecessarily sending large or sensitive files and directories to the daemon and potentially adding them to images using ADD or COPY. | . . 7.2.3 File System Instructions . - COPY : Will copy files from the filesystem where the image is being built. - VOLUME : Same as --volume flag ( bound mount volume ) - CMD : This is a closely related to ENTRYPOINT - ADD : This operates similarly to the COPY instruction with two imp differences: - fetch remote sources files if a Url is specified - Extract the files of any source determined to be an archive file - ONBUILD : This instruction let the other instructions to execute ( if resulting image is used as base img for another build ) # Example # ADD : We can ADD large no of files to a container without any problem # Docker will unpack tarfiles of most standard types (.gz, .bz2, .xz, .tar). # some.tar FROM Debian RUN mkdir -p /opt/libeatmydata ADD some.tar.gz /opt/libeatmydata/ RUN ls -lRt /opt/libeatmydata # ONBUILD : Use the ONBUILD command to automate and encapsulate the building of an image. # GO Example ( Outyet : Simple Go App ) $ git clone https://github.com/golang/example $ cd example/outyet $ docker build -t outyet . $ docker run --publish 8080:8080 --name outyet1 -d outyet With ONBUILD FROM golang:onbuild EXPOSE 8080 golang:onbuild Dockerfile FROM golang:1.7 RUN mkdir -p /go/src/app WORKDIR /go/src/app CMD [&quot;go-wrapper&quot;, &quot;run&quot;] ONBUILD COPY . /go/src/app ONBUILD RUN go-wrapper download ONBUILD RUN go-wrapper install The result of this technique is that you have an easy way to build an image that only contains the code required to run it, and no more. There are also other examples of ONBUILD exists : node:onbuild , python:onbuild # ENTRYPOINT : Sets the entrypoint for the image # Basic Shell Script for clean logs #!/bin/bash echo &quot;Cleaning logs over $1 days old&quot; find /log_dir -ctime &quot;$1&quot; -name &#39;*log&#39; -exec rm {} ; # Create a DockerFile ( Create a container with clean_log script ) FROM ubuntu:17.04 ADD clean_log /usr/bin/clean_log RUN chmod +x /usr/bin/clean_log ENTRYPOINT [&quot;/usr/bin/clean_log&quot;] CMD [&quot;7&quot;] $ docker build -t log-cleaner . $ docker run -v /var/log/myapplogs:/log_dir log-cleaner 365 # Clean The logs of over a year ( default 7 days if no arg given ) . . 7.2.4 Create a Maintainable Dockerfiles . - ARG : arg defines a variable thta users can provide to docker when building and a image. Ex: Dockerfile ARG VERSION=unknown ENV VERSION=&quot;${VERSION}&quot; LABEL base.version=&quot;${VERSION}&quot; $ version=0.6; docker image build -t dockerinaction/mailer-base:${version} -f mailer-base.df --build-arg VERSION=${version} . $ docker image inspect --format &#39;&#39; dockerinaction/mailer-base:0.6 { &quot;base.name&quot;: &quot;Mailer Archetype&quot;, &quot;base.version&quot;: &quot;0.6&quot;, &quot;maintainer&quot;: &quot;dia@allingeek.com&quot; } . . 7.2.5 Init System for Docker . Most Popular init systems are : runit, tini, BusyBox init, Supervisord, and DAEMON | By Default docker comes with tini init system | . $ docker container run -it --init alpine:3.6 nc -l -p 3000 # Docker ran /dev/init -- nc -l -p 3000 inside the container instead of just nc . . 7.2.6 Health Check In Docker . - There are two ways to specify the health check command: 1. Use a HEALTHCHECK instruction when defining the image 2. On the command-line when running a container 1. This is a 1st Mothed : It is used when we define an Image $ FROM nginx:1.13-alpine HEALTHCHECK --interval=5s --retries=2 CMD nc -vz -w 2 localhost 80 || exit 1 $ docker ps --format &#39;table t t&#39; NAMES IMAGE STATUS healthcheck_ex dockerinaction/healthcheck Up 3 minutes (healthy) # Exit Status Codes - 0: success—The container is healthy and ready for use. - 1: unhealthy—The container is not working correctly. - 2: reserved—Do not use this exit code. 2. Command-line Method $ docker container run --name=healthcheck_ex -d --health-cmd=&#39;nc -vz -w 2 localhost 80 || exit 1&#39; nginx:1.13-alpine . . 7.2.7 Hardening Application Images . - There are Three Methods to hardended the images 1. We can enforce that our images are built from a specific image. 2. we can make sure that regardless of how containers are built from our image, they will have a sensible default user. 3. we should eliminate a common path for root user escalation from programs with setuid or setgid attributes set. 1. Content Addressable Images identifiers ( CAIID ) # Build Images using authentic digest : that digest is known as CAIID # So now how many images we build from these they are authentic. docker pull debian:stable stable: Pulling from library/debian 31c6765cabf1: Pull complete Digest: sha256:6aedee3ef827... # Dockerfile: FROM debian@sha256:6aedee3ef827... ... 2. Create a User &amp; groups $ RUN groupadd -r postgres &amp;&amp; useradd -r -g postgres postgres 3. SUID &amp; GUID $ FROM ubuntu:latest # Set the SUID bit on whoami RUN chmod u+s /usr/bin/whoami # Create an example user and set it as the default RUN adduser --system --no-create-home --disabled-password --disabled-login --shell /bin/sh example USER example # Set the default to compare the container user and # the effective user for whoami CMD printf &quot;Container running as: %s n&quot; $(id -u -n) &amp;&amp; printf &quot;Effectively running whoami as: %s n&quot; $(whoami) Output Container running as: example Effectively running whoami as: root The output of the default command shows that even though you’ve executed the whoami command as the example user, it’s running from the context of the root user. . . 7.2.8 Complete Story of Cache . # --no-cache will downlaod fresh containers from source. it will not use the cache files # Example $ docker build --no-cache . # Busting the Cache # we need this because some time our image build takes so much time. so we need cache up to a certain point. # Method 1 (cheat) : Add a benign comment after the command to invalidate the cache. This works because Docker treats the non-whitespace change to theline as though it were a new command, so the cached layer is not re-used. CMD [&quot;npm&quot;,&quot;start&quot;] #bust the cache # Method 2 ( using ARG ) Use the ARG directive in your Dockerfile to enable surgical cache-busting. If this ARG variable isset to a value never used before on your host, the cache will be busted from that point. WORKDIR todo ARG CACHEBUST=no RUN npm install $ docker build --build-arg CACHEBUST=${RANDOM} . $ echo ${RANDOM} 19856 $ echo ${RANDOM} 26429 # If not using Bash $ docker build --build-arg CACHEBUST=$(date +%s) . # Method 3 ( Using ADD ) There are two useful features of ADD that you can use to your advantage in this context: it caches the contents of the file it refers to, and it can take a network resource as an argument. # Git Repo Example It means if repo is not changed it uses cache or if git repo is changed it rebuild the image from scratch. But it will vary from resources type to resource type. we can take help of Github API here : It has URLs foreach repository that return JSON for the most recent commits. When a new commit ismade, the content of the response changes. FROM ubuntu:16.04 ADD https://api.github.com/repos/nodejs/node/commits /dev/null RUN git clone https://github.com/nodejs/node . . 7.2.9 Flattening Images . # This is imp because images can reveal the imp information Example : # create a docker file : It has sensitive information FROM debian RUN echo &quot;My Big Secret&quot; &gt;&gt; /tmp/secret_key RUN cat /tmp/secret_key RUN rm /tmp/secret_key $ docker build -t secret . # But now problem arise $ docker history secret ... 5e39caf7560f 3 days ago /bin/sh -c echo &quot;My Big Secret&quot; &gt;&gt; /tmp/se 14 B ... $ docker run 5b376ff3d7cd cat /tmp/secret_key My Big Secret But if someone could download this image from public repo and insect the history &amp; run thi command : It reveal the secret. # To get rid of this type of problem : we need to remove intermediate layering # we need to export the image as a trivially run container and then re-import and tag the resulting image: $ docker run -d secret /bin/true - new_id # Runs a docker export, taking a conatiner iD as arg and outgoing a tar file of fs contents. # This is piped to docker import which takes tar file and create a new image. $ docker export new_id | docker import - new_secret $ docker history new_secret . . 7.3 Refer . Examples | Best practices for writing Dockerfiles | Michael Crosby has some more Dockerfiles best practices / take 2. | Building Good Docker Images / Building Better Docker Images | Managing Container Configuration with Metadata | How to write excellent Dockerfiles | . . . 8.0 Registry &amp; Repository . . A repository is a hosted collection of tagged images that together create the file system for a container. . A registry is a host – a server that stores repositories and provides an HTTP API for managing the uploading and downloading of repositories. . 8.1 Enviornment . docker login to login to a registry. | docker logout to logout from a registry. | docker search searches registry for image. | docker pull pulls an image from registry to local machine. | docker push pushes an image to the registry from local machine. | . NOTE : Refer Chapter 9 of Docker in Action : Public and private software distribution ( This Cover’s Every Basic detail about Registry &amp; Repository ) . 8.2 Setup Local Docker Registry . # To start a registry on local Network $ docker run -d -p 5000:5000 -v $HOME/registry:/var/lib/registry registry:2 # This command makes the registry available on port 5000 of the Docker host(-p 5000:5000). With the -v flag, it makes the registry folder on your host(/var/lib/registry) available in the container as $HOME/registry. The registry’s fileswill therefore be stored on the host in the /var/lib/registry folder. # HOSTNAME is the hostname or IP address of your new reg-istry server # we also use --insecure-registry ( We know our local network is secure :) [Docker will only allow you to pull from registries with a signedHTTPS certificate.] ) # Push the image to the registry $ docker push HOSTNAME:5000/image:tag. . . . 9.0 Docker Compose . Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration. To learn more about all the features of Compose, see the list of features. . The standard filename for Compose files is docker-compose.yml. . 9.1 YAML Basics . A YAML document can include a comment at the end of any line. Com￾ments are marked by a space followed by a hash sign ( #). Any characters that follow until the end of the line are ignored by the parser. | YAML uses three types of data and two styles of describing that data, block and flow. Flow collections are specified similarly to collection literals in JavaScript and other languages. For example, the following is a list of strings in the flow style: [&quot;PersonA&quot;,&quot;PersonB&quot;] | The block style is more common and will be used in this primer except where noted. The three types of data are maps, lists, and scalar values. Maps are defined by a set of unique properties in the form of key/value pairs that are delimited by a colon and space (: ). | Scaler Values : Scaler String image: &quot;alpine&quot; , Scaler Command : command: echo hello world | Scaler Rules : Must not be empty, | Must not contain leading or trailing whitespace characters | Must not begin with an indicator character (for example, - or :) in places where doing so would cause an ambiguity. | Must never contain character combinations using a colon (:) and hash sign (#) | | Lists (or block sequences) are series of nodes in which each element is denoted by a leading hyphen (-) indicator. For example: - item 1 - item 2 - item 3 - # an empty item - item 4 | . | . | Indentation Rules : YAML uses indentation to indicate content scope. Scope determines which block each element belongs to. There are a few rules: Only spaces can be used for indentation. | The amount of indentation does not matter as long as – All peer elements (in the same scope) have the same amount of indentation. – Any child elements are further indented. | . | . These documents are equivalent: . top-level: second-level: # three spaces third-level: # two more spaces - &quot;list item&quot; # single additional indent on items in this list another-third-level: # a third-level peer with the same two spaces fourth-level: &quot;string scalar&quot; # 6 more spaces another-second-level: # a 2nd level peer with three spaces - a list item # list items in this scope have # 15 total leading spaces - a peer item # A peer list item with a gap in the list # every scope level adds exactly 1 space top-level: second-level: third-level: - &quot;list item&quot; another-third-level: fourth-level: &quot;string scalar&quot; another-second-level: - a list item - a peer item . 9.2 Docker Compose Basics . By using the following command you can start up your application: . # first install docker-compose on your system (eg: Ubuntu ) $ sudo apt install docker-compose $ docker-compose -f &lt;docker-compose-file&gt; up . You can also run docker-compose in detached mode using -d flag, then you can stop it whenever needed by the following command: . docker-compose stop . You can bring everything down, removing the containers entirely, with the down command. Pass --volumes to also remove the data volume. . Let understand Docker compose with an Example : . 9.2.1 Example yml . # wikijs.yml version: &#39;2&#39; services: db: image: postgres:11-alpine environment: POSTGRES_DB: wiki POSTGRES_PASSWORD: wikijsrocks POSTGRES_USER: wikijs logging: driver: &quot;none&quot; restart: unless-stopped volumes: - db-data:/var/lib/postgresql/data wiki: image: requarks/wiki:2 depends_on: - db environment: DB_TYPE: postgres DB_HOST: db DB_PORT: 5432 DB_USER: wikijs DB_PASS: wikijsrocks DB_NAME: wiki restart: unless-stopped ports: - &quot;80:3000&quot; volumes: db-data: # we can create a docker stack with this yml file ( it established a wikijs and postgresql db ) $ docker stack deploy -c wikijs.yml wikijs # if we use docker swarn or Kubernetes we can create a replicas of this images into 3 diff servers -- deploy: replicas: 3 -- # add this to end of yml and again deploy it. It will update the container # To Check The State of stack $ docker stack ps --format &#39; t&#39; wikijs # To remove a service from stack ( like limit the replicas from 3 to 2 ) $ docker stack deploy -c wikijs.yml --prune wikijs We Need To use Prue Here : Because without Prune it will not completely remove services which causes problmes ( like for Instead of using postgressql we want to use a mysql so if we updated our stack yml file and deploy it. it will add mysql db but doesnt remove postgres containers so to remove postgres we use prune ) The --prune flag will clean up any resource in the stack that isn’t explicitly referenced in the Compose file used for the deploy operation. # Prune The new [Data Management Commands](https://github.com/docker/docker/pull/26108) have landed as of Docker 1.13: * `docker system prune` * `docker volume prune` * `docker network prune` * `docker container prune` * `docker image prune` Note : There is a problem here everytime a container replaced docker will create a new volume space for container and its replicas. This would cause problems in a real-world system. So, to get rid of this EX: volumes: pgdata: # empty definition uses volume defaults services: postgres: image: dockerinaction/postgres:11-alpine volumes: - type: volume source: pgdata # The named volume above target: /var/lib/postgresql/data environment: POSTGRES_PASSWORD: example The file defines a volume named pgdata, and the postgres service mounts that volume at /var/lib/postgresql/data. That location is where the Postgre￾SQL software will store any database schema or data. Inspect $ docker stack deploy -c databases.yml --prune my-databases $ docker volume ls DRIVER VOLUME NAME local my-databases_pgdata $ docker service remove my-databases_postgres Then restore the service by using the Compose file: $ docker stack deploy -c databases.yml --prune my-databases . . . 10.0 DevOps Operations With Docker . . 10.1 Convert Your Virtual Box VM To Docker Container . # Process : VM FILE (.vdi or anything) =&gt; TAR =&gt; Import TAR as an Image in Docker. $ sudo apt install qemu-utils # Identify the path to your VM disk image. ( Stop the VM ) # Sets up a variable pointingto your VM disk image. $ VMDISK=&quot;$HOME/VirtualBox VMs/myvm/myvm.vdi&quot; # Initializes a kernelmodule requiredby qemu-nbd $ sudo modprobe nbd # Connects the VM disk to a virtual device node $ sudo qemu-nbd -c /dev/nbd0 -r $VMDISK3((CO1-3)) # Lists the partition numbers available to mount on this disk $ ls /dev/nbd0p* /dev/nbd0p1 /dev/nbd0p2 # Mounts the selected partition at /mnt with qemu-nbd $ sudo mount /dev/nbd0p2 /mnt # Creates a TAR filecalled img.tar from /mnt $ sudo tar cf img.tar -C /mnt . # Unmounts and cleans up after qemu-nbd $ sudo umount /mnt &amp;&amp; sudo qemu-nbd -d /dev/nbd0 # Dockerfile FROM scratch ADD img.tar / $ docker build . . . 10.2 Host Like Container . Containers are not virtual machines—there are significant differences—and pretending there aren’t can cause confusion and issues down the line. . Differences between VMs and Docker containers: Docker is application-oriented, whereas VMs are operating-system oriented. | Docker containers share an operating system with other Docker containers. Incontrast, VMs each have their own operating system managed by a hypervisor. | Docker containers are designed to run one principal process, not manage mul-tiple sets of processes. | . | . docker run -d phusion/baseimage [ This Image designed to run multiple processes.] docker exec -i -t container_ID /bin/bash ps -ef [ It starts (cron, sshd, and syslog). It Much like a host. ] . . 10.3 Running GUI in Containers . Process : Create an image with your user credentials and the program, and bind mount your Xserver to it. Note : another method is to setup VNC server on container . # Dockerfile for setting up firefox on container FROM ubuntu:14.04 RUN apt-get update RUN apt-get install -y firefox RUN groupadd -g GID USERNAME RUN useradd -d /home/USERNAME -s /bin/bash -m USERNAME -u UID -g GID USER USERNAME ENV HOME /home/USERNAME CMD /usr/bin/firefox $ docker build -t gui . $ docker run -v /tmp/.X11-unix:/tmp/.X11-unix -h $HOSTNAME -v $HOME/.Xauthority:/home/$USER/.Xauthority -e DISPLAY=$DISPLAY gui # It will popup firefox ( which runs on container ) . . 10.4 Using Docker Machine to Provision Docker Hosts . Docker-machine is a tool just like a vagrant. EX: we can setup VM with virtualBox with docker command. walkthrough: . # Install Docker Machine on Linux $ curl -L https://github.com/docker/machine/releases/download/v0.16.2/docker-machine-`uname -s`-`uname -m` &gt;/tmp/docker-machine &amp;&amp; chmod +x /tmp/docker-machine &amp;&amp; sudo cp /tmp/docker-machine /usr/local/bin/docker-machine # Create a VM by docker daemon on Oracle Virtual Box $ docker-machine create --driver virtualbox host1 # Run this command to set the DOCKER_HOST environment variable, which sets the default host that Docker commands will be run on $ docker-machine env host1 $ eval $(docker-machine env host1) # we can direct to VM $ docker-machine ssh host1 Commands of Docker-Machine: create : Creates a new Machine ls : List Machines stop : Stop Machines start : Start Machines restart : Restart Machines rm : Destroys the machine inspect : Returns a JSON representation of the machine’s metadata config : Return the config of machine ip : Returns the IP address of the machine url : Returns a URL for the Docker daemon on the machine upgrade : Upgrades the Docker version on the host to the latest . . 10.5 Build Images using a Chef Solo . Chef : It is a configuration Management Tool ( using this can reduce the amount of work required to configure Images ) . Here we setup hello world apache website ( Example ). | . # Need a working code $ git clone https://github.com/docker-in-practice/docker-chef-solo-example.git # All the working code in there $ cd into it ( there is a Dockerfile and some other files and folders ( chef recepies etc ) $ docker build -t chef-example . $ docker run -it -p 8080:80 chef-example # This is a one time written code we can anywhere to deploy a website ( in such a small steps ) . . 10.6 CI Operations With Docker . CI : Continious Integration . . 10.6.1 Steps . Check out a clean copy of the source code defining the image and build scripts so the origin and process used to build the image is known. | Retrieve or generate artifacts that will be included in the image, such as the application package and runtime libraries. | Build the image by using a Dockerfile. | Verify that the image is structured and functions as intended. | (Optional) Verify that the image does not contain known vulnerabilities. | Tag the image so that it can be consumed easily. | Publish the image to a registry or another distribution channel. | . 10.6.2 Method 1 : Builds a Image Using DockerHub Workflow ( Test and Push Images ) . # For this you will Git Repo and docker Hub Repo # Link Docker Hub to to git repo ( it take code from git repo and compile and create a desired Image ( like other ci tools do ) # wait for the docker hub to build to complete # Remember this is a basic solution . 10.6.3 Method 2 : Setting up a package cache for faster Builds . While building the images it will take caches instead of download everytime from internet. . # We are using squid Proxy Here $ sudo apt-get install squid-deb-proxy $ check for port 8000 $ create a Docker File using a apt proxy FROM debian RUN apt-get update -y &amp;&amp; apt-get install net-tools RUN echo &quot;Acquire::http::Proxy &quot;http://$( route -n | awk &#39;/^0.0.0.0/ {print $2}&#39; ):8000 &quot;;&quot; &gt; /etc/apt/apt.conf.d/30proxy RUN echo &quot;Acquire::http::Proxy::ppa.launchpad.net DIRECT;&quot; &gt;&gt; /etc/apt/apt.conf.d/30proxy CMD [&quot;/bin/bash&quot;] This will cache all the webpages and apt package we downlaod after running this container : if again download them they will be download in miliseconds . 10.6.3 Method 3 : Running the Jenkins Master Withing the Docker Container . Portable Jenkins Server . # download git clone https://github.com/docker-in-practice/jenkins.git. # Put your required plugins in jenkins_plugins.txt Dockerfile FROM jenkins COPY jenkins_plugins.txt /tmp/jenkins_plugins.txt RUN /usr/local/bin/plugins.sh /tmp/jenkins_plugins.txt USER root RUN rm /tmp/jenkins_plugins.txt RUN groupadd -g 999 docker RUN addgroup -a jenkins docker USER jenkins $ docker build -t jenkins . $ docker run --name jenkins -p 8080:8080 -p 50000:50000 -v /var/run/docker.sock:/var/run/docker.sock -v /tmp:/var/jenkins_home -d jenkins # go to localhost:8080 : Copy master password from logs # Reliably Upgrade a Jenkins Server Dockerfile FROM docker ADD jenkins_updater.sh /jenkins_updater.sh RUN chmod +x /jenkins_updater.sh ENTRYPOINT /jenkins_updater.sh Shell script to backup and restart Jenkins #!/bin/sh set -e set -x if ! docker pull jenkins | grep up.to.date then docker stop jenkins docker rename jenkins jenkins.bak.$(date +%Y%m%d%H%M) cp -r /var/docker/mounts/jenkins_home /var/docker/mounts/jenkins_home.bak.$(date +%Y%m%d%H%M) docker run -d --restart always -v /var/docker/mounts/jenkins_home:/var/jenkins_home --name jenkins -p 8080:8080 jenkins fi # docker Command to run the Jenkins Updater $ docker run --rm -d -v /var/lib/docker:/var/lib/docker -v /var/run/docker.sock:/var/run/docker.sock -v/var/docker/mounts:/var/docker/mounts dockerinpractice/jenkins-updater # to automate the process add this command to crontab 0 * * * * dokcker_command . . 10.7 CD Operations with Docker . CD : Continious Delivery ( CI + Deployment ) . # Copy an Image Between Two Registries Process =&gt; Pulling the image from the registry -&gt; retag -&gt; pushing the new Image $ docker tag -f $OLDREG/$MYIMAGE $NEWREG/$MYIMAGE $ docker push $NEWREG/$MYIMAGE $ docker rmi $OLDREG/$MYIMAGE $ docker image prune -f # Copy an Image btw Two Machine With a very low-bandwidth Connection Process =&gt; Here we use backup tool called Bup ( creates a bup data tool ) # Example ( Not Exact data is used so please Don&#39;t judge me ) # pull two images like Ubuntu:18.04 &amp; 19.10 ( Both are example = 65 MB Each = 130 MB ) $ mkdir bup_pool $ alias dbup=&quot;docker run --rm -v $(pwd)/bup_pool:/pool -v /var/run/docker.sock:/var/run/docker.sock dockerinpractice/dbup&quot; $ dbup save ubuntu:18.04 $ du -sh bup_pool ( 74 MB ) $ dbup save ubuntu:19.10 $ du -sh bup_pool ( 96 MB ) ( Saves 35 MB ) # On other machine ( rsync from host1 to host2 ) $ dbup load ubuntu:18.04 # Also copies files between host with TAR Docker export : creates Container To TAR Docker Import : TAR to Image Docker save : Image To TAR Docker load : TAR to Docker Image Example : Transfer docker Image directory over ssh $ docker export $(docker run -d debian:7.3 true) | ssh user@host docker import . . 10.8 Cordination Between Containers . We need coordination between containers : Like if we take an example of one server and one python based echo client. P1: If we start the client container first : It will lead to failure P2 : forgetting to remove the containers will result in problems when you try to restart P3 : Naming containers incorrectly will result in failure. So how to get rid of this types of problem : we need a solution,where we can run the container without any problem. . # Solution : create a compose file version: &quot;3&quot; services: echo-server: image: server expose: - &quot;2000&quot; client: image: client links: - echo-server:talkto # with this we can start container in correct order &amp; also we call rebuild the container anywhere $ docker-compose up Attaching to dockercompose_server_1, dockercompose_client_1 client_1 | Received: Hello, world client_1 | client_1 | Received: Hello, world client_1 | . . 11.0 Security With Docker . . This is where security tips about Docker go. The Docker security page goes into more detail. . First things first: Docker runs as root. If you are in the docker group, you effectively have root access. If you expose the docker unix socket to a container, you are giving the container root access to the host. . Docker should not be your only defense. You should secure and harden it. . For an understanding of what containers leave exposed, you should read Understanding and Hardening Linux Containers by Aaron Grattafiori. This is a complete and comprehensive guide to the issues involved with containers, with a plethora of links and footnotes leading on to yet more useful content. The security tips following are useful if you’ve already hardened containers in the past, but are not a substitute for understanding. . 11.1 Security Tips . For greatest security, you want to run Docker inside a virtual machine. This is straight from the Docker Security Team Lead – slides / notes. Then, run with AppArmor / seccomp / SELinux / grsec etc to limit the container permissions. See the Docker 1.10 security features for more details. . Docker image ids are sensitive information and should not be exposed to the outside world. Treat them like passwords. . See the Docker Security Cheat Sheet by Thomas Sjögren: some good stuff about container hardening in there. . Check out the docker bench security script, download the white papers. . Snyk’s 10 Docker Image Security Best Practices cheat sheet . You should start off by using a kernel with unstable patches for grsecurity / pax compiled in, such as Alpine Linux. If you are using grsecurity in production, you should spring for commercial support for the stable patches, same as you would do for RedHat. It’s $200 a month, which is nothing to your devops budget. . Since docker 1.11 you can easily limit the number of active processes running inside a container to prevent fork bombs. This requires a linux kernel &gt;= 4.3 with CGROUP_PIDS=y to be in the kernel configuration. . docker run --pids-limit=64 . Also available since docker 1.11 is the ability to prevent processes from gaining new privileges. This feature have been in the linux kernel since version 3.5. You can read more about it in this blog post. . docker run --security-opt=no-new-privileges . From the Docker Security Cheat Sheet (it’s in PDF which makes it hard to use, so copying below) by Container Solutions: . Turn off interprocess communication with: . docker -d --icc=false --iptables . Set the container to be read-only: . docker run --read-only . Verify images with a hashsum: . docker pull debian@sha256:a25306f3850e1bd44541976aa7b5fd0a29be . Set volumes to be read only: . docker run -v $(pwd)/secrets:/secrets:ro debian . Define and run a user in your Dockerfile so you don’t run as root inside the container: . RUN groupadd -r user &amp;&amp; useradd -r -g user user USER user . 11.2 Security Videos . Using Docker Safely | Securing your applications using Docker | Container security: Do containers actually contain? | Linux Containers: Future or Fantasy? | . 11.3 Security Roadmap . The Docker roadmap talks about seccomp support. There is an AppArmor policy generator called bane, and they’re working on security profiles. . . Till 21 Aprill . Path to Complete . Docker access | Security Measures In Docker | Securing Access to Docker | Security from Outside docker | . . 12.0 Monitoring Docker . 12.1 Monitoring . . 12.2 Resource Control . . 12.3 Some Advance Approches .",
            "url": "https://hacstac.github.io/Notes/docker/2020/09/01/Docker-Hard-Parts.html",
            "relUrl": "/docker/2020/09/01/Docker-Hard-Parts.html",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Docker Easy Parts [Part - 1]",
            "content": "1.0 What is a Docker . . 1.1 Introduction . Defination : Docker is a tool designed to make it easier to create, deploy, and run applications by using containers. Containers allow a developer to package up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package. . Docker container technology was launched in 2013 as an open source Docker Engine. Containers encapsulate an application as a single executable package of software that bundles application code together with all of the related configuration files, libraries, and dependencies required for it to run. . Containerized applications are “isolated” in that they do not bundle in a copy of the operating system. Instead, an open source Docker engine is installed on the host’s operating system and becomes the conduit for containers to share an operating system with other containers on the same computing system. . . . 1.2 Below is a list of common Docker terms . Docker Engine is a client-server application with 3 major components - a server which is a type of long-running program called a daemon process; a REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do; a command line interface (CLI) client. . | Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. . | Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon. . | Image is a read-only template with instructions for creating a Docker container. You might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. . | Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. . | Container is a runnable instance of an image. Containers are made possible by operating system (OS) process isolation and virtualization, which enable multiple application components to share the resources of a single instance of an OS kernel. . | . 1.3 Architecture of docker . Docker on your host machine is (at the time of writing) split into two parts—a daemon with a RESTful API and a client that talks to the daemon. You invoke the Docker client to get information from or give instructions to the daemon; the daemon is a server that receives requests and returns responses from the client using the HTTP protocol. In turn, it will make requests to other services to send and receive images, also using the HTTP protocol. The server will accept requests from the command-line client or anyone else authorized to connect. The daemon is also responsible for taking care of your images and containers behind the scenes, whereasthe client acts as the intermediary between you and the RESTful API. . . . . . 2.0 Installation . . Docker can operate on most of the Operating Systems In Industries : Windows, MacOS, Most Of flavours of Linux ( Ubuntu, RHEL, Arch) | Docker Operates on both AMD64 and ARM Based Systems | . # Simple script to install docker on Linux $ curl -sSL https://get.docker.com/ | sh # Check Docker Verison $ docker version --format &#39;&#39; 19.03.8 # Dump Raw JSON DATA : Like Kernel, Architecture , details , build time etc $ docker version --format &#39;&#39; # Running Docker without sudo $ sudo usermod -aG docker username or $ sudo addgroup -a username docker # restart docker . . . 3.0 Docker Basics . . 3.1 Day To Day Docker Commands . . docker create creates a container but does not start it. | docker rename allows the container to be renamed. | docker run creates and starts a container in one operation. | docker rm deletes a container. | docker update updates a container’s resource limits. | docker images shows all images. | docker cp copies files or folders between a container and the local filesystem. | docker build creates image from Dockerfile. | docker commit creates image from a container, pausing it temporarily if it is running. | docker rmi removes an image. . | docker start starts a container so it is running. | docker stop stops a running container. | docker restart stops and starts a container. | docker pause pauses a running container, “freezing” it in place. | docker unpause will unpause a running container. | docker wait blocks until running container stops. | docker kill sends a SIGKILL to a running container. | docker attach will connect to a running container. . | docker ps shows running containers. | docker logs gets logs from container. (You can use a custom log driver, but logs is only available for json-file and journald in 1.10). | docker inspect looks at all the info on a container (including IP address). | docker events gets events from container. | docker port shows public facing port of container. | docker top shows running processes in container. | docker stats shows containers’ resource usage statistics. | docker diff shows changed files in the container’s FS. | docker history shows history of image. | docker tag tags an image to a name (local or registry). | . . 3.2 Getting Practical . . 3.2.1 List Images . docker images // show images docker ps -a docker ps // shows started containers -a = all containers . 3.2.2 Start/Stop/Restart . docker stop/start/restart . Note : If you want to detach from a running container, use Ctrl + P, Ctrl + Q. If you want to integrate a container with a host process manager, start the daemon with -r=false then use docker start -a. . 3.2.3 Logs . docker logs {Name_of_container} . 3.2.4 Rename . docker rename new_name current_name // rename container . 3.2.5 Create container . docker create nginx // will only create a container . 3.2.6 Example Run . $ docker run --interactive --tty --link web:web --name web_test busybox:1.29 /bin/sh -d = detach automatically the container (run container in background and print container ID) --interactive --tty or -t that will allocate a pseudo-TTY session --link = link to other container --name = name of docker container $ docker exec web_test ps // show extra process run with this containers . 3.2.7 Docker Run with SHELL Variables . CID=$(docker create nginx:latest) echo $CID // assigns to a Shell variable MAILER_CID=$(docker run -d dockerinaction/ch2_mailer) WEB_CID=$(docker create nginx) $ docker start $AGENT_CID $ docker start $WEB_CID . 3.2.8 Env Variables . $ docker run -d --name wpdb -e MYSQL_ROOT_PASSWORD=ch2demo mysql:5.7 $ docker run --env MY_ENVIRONMENT_VAR=&quot;this is a test&quot; busybox:1.29 env # --env flag, or -e for short, can be used to inject any environment variable. . 3.2.9 With Read Only Option . docker run -d --name wp --read-only wordpress:5.0.0-php7.2-apache // create a container with only readonly options . 3.2.10 Inspect . # The docker inspect command will display all the metadata(JSON) $ docker inspect --format &quot;&quot; wp // Prints true if container is running $ docker inspect --format &quot;&quot; Id/name // Show ip of container . 3.2.11 Diff ( Filesystem check ) . $ docker run -d --name wp_writable wordpress:5.0.0-php7.2-apache # let’s check where Apache changed the container’s filesystem with the docker $ docker container diff wp_writable A - A file or directory was added D - A file or directory was deleted C - A file or directory was changed C /run C /run/apache2 A /run/apache2/apache2.pid . 3.2.12 Clean Up . docker rm -f {container_ID) docker rm -vf $(docker ps -a -q) docker rmi [name] // Remove an image . 3.2.13 Executing Commands . # docker exec to execute a command in container. # To enter a running container, attach a new shell process to a running container called foo, use: $ docker exec -it foo /bin/bash. # exec Modes : # 1 Basic : Runs the command in the container synchronously on the command line $ docker exec name_of_container echo &quot;Hello User&quot; Hello User # 2 Daemon : Runs the command in the background on the container $ docker exec -d name_C find / -ctime 7 -name &#39;*log&#39; -exec rm {} ; # 3 Interactive : Runs the command and allows the user to interact with it $ docker exec -it ubuntu /bin/bash . 3.2.15 Linking containers for port isolation . Note : This is an older method of declaring container communication—Docker’s link flag. This isn’t the recommended way of working anymore. . # Example # This will allow us communication between containers without using user-defined networks. $ docker run --name wp-mysql -e MYSQL_ROOT_PASSWORD=yoursecretpassword -d mysql $ docker run --name wordpress --link wp-mysql:mysql -p 10003:80 -d wordpress . 3.2.16 Search a Docker Image . docker search node docker pull node // Pull the Image by Name ( on Hub ) docker run -it node /bin/bash ( Start Node Container ) . 3.2.17 Cleanly Kill Containers . # Always use docker stop ( it actually stops the containers ). # Docker kill will send immediate signal which will kill process while running ( so they can create temp files ) kill Term 15 docker kill Kill 9 docker stop Term 15 . 3.2.18 Docker Prune . # Prune commands docker system prune docker volume prune docker network prune docker container prune docker image prune # Example # Nuclear Option ( if you want to remove all containers of your host machine ) [Removes all : Runnig &amp; exited] $ docker ps -a -q | xargs --no-run-if-empty docker rm -f # To keep running containers $ docker ps -a -q --filter status=exited | xargs --no-run-if-empty docker rm # To list out all exited &amp; failed Containers $ comm -3 &lt;(docker ps -a -q --filter=status=exited | sort) &lt;(docker ps -a -q --filter=exited=0 | sort) | xargs --no-run-if-empty docker inspect &gt; error_containers # Prune Volumes # List out all docker voluems $ docker volume ls # Delete Unused Volumes $ docker volume prune . 3.2.19 Space Occupied By docker System . $ docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 7 1 1.963GB 1.885GB (95%) Containers 1 1 0B 0B Local Volumes 2 1 242.4MB 242.3MB (99%) Build Cache 0 0 0B 0B . 3.2.20 Container Stats . $ docker stats ID CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS . 3.2.21 Tag . docker image tag ubuntu-git:latest ubuntu-git:2.7 . 3.2.22 Commit . $ docker container commit -a &quot;@dockerinaction&quot; -m &quot;Added git&quot; image-dev ubuntu-git # Outputs a new unique image identifier like: # bbf1d5d430cdf541a72ad74dfa54f6faec41d2c1e4200778e9d4302035e5d143 # Build a New Image For Commited Image $ docker container run -d ubuntu-git . 3.2.23 Set an EntryPoint . docker container run --name cmd-git --entrypoint git ubuntu-git . . 3.2.24 Versioning Best Practice . # Docker official Repo&#39;s are the best example of tagging an image # Example for go lang 1.x 1.9 1.9.6 1.9-stretch 1.10-alpine latest # this is an example of tags to build that don&#39;t confuse the end user . . 3.3 States of Docker . Docker container can be in one of six states: . Created | Running | Restarting | Paused | Removing | Exited (also used if the container has never been started) | . . Created : A container that has been created (e.g. with docker create) but not started | Running : A currently running container | Paused : A container whose processes have been paused | Exited : A container that ran and completed (“stopped” in other contexts, although a created container is technically also “stopped”) | Dead : A container that the daemon tried and failed to stop (usually due to a busy device or resource used by the container) | Restarting : A container that is in the process of being restarted | . 3.3.1 Restart State . Using the --restart flag at container-creation time, you can tell Docker to do any of the following: . Never restart (default) | Attempt to restart when a failure is detected | Attempt for some predetermined time to restart when a failure is detected | Always restart the container regardless of the condition . | Methods . no = = Don’t restart when the container exits | always == Always restart when the container exits | unless-stopped == Always restart, but remember explicitly stopping | on-failure[:max-retry] == Restart only on failure | . | . # Example Run $ docker run -d --name backoff-detector --restart always busybox:1.29 date $ docker logs -f backoff-detector . . 3.4 Init &amp; PID Systems for Docker . Several such init systems could be used inside a container. The most popular include runit, Yelp/dumb-init, tini, supervisord, and tianon/gosu . . $ docker run -d -p 80:80 --name lamp-test tutum/lamp $ docker top lamp-test $ docker exec lamp-test ps $ docker exec lamp-test kill &lt;PID&gt; // kill a process $ docker run --entrypoint=&quot;cat&quot; wordpress:5.0.0-php7.2-apache /usr/local/bin/docker-entrypoint.sh # If you run through the displayed script, you’ll see how it validates the environment variables against the dependencies of the software and sets default values. Once the script has validated that WordPress can execute . . 3.5 Software Installation Simplified . Three main ways to install Docker images: . Using Docker registries | Using image files with docker save and docker load | Building images with Dockerfiles | . we can install software in three other ways: . You can use alternative repository registries or run your own registry. | You can manually load images from a file. | You can download a project from some other source and build an image by using a provided Dockerfile. | . Note : - Keep in mind for Tags with images [latest, stable, alpha, Beta] . Download image from another regestry instead of docker hub : docker pull quay.io/dockerinaction/ch3_hello_registry:latest | [REGISTRYHOST:PORT/][USERNAME/]NAME[:TAG] | . 3.5.1 Installing Images using dockerfile . git clone https://github.com/dockerinaction/ch3_dockerfile.git docker build -t dia_ch3/dockerfile:latest ch3_dockerfile . . 3.6 Backup &amp; Restore Docker Images . docker export turns container filesystem into tarball archive stream to STDOUT. | docker import creates an image from a tarball. | docker load loads an image from a tar archive as STDIN, including images and tags (as of 0.7). | docker save saves an image to a tar archive stream to STDOUT with all parent layers, tags &amp; versions (as of 0.7). | . 3.6.1 Comparison . Docker export : Container To TAR Docker Import : TAR to Image Docker save : Image To TAR Docker load : TAR to Image . 3.6.2 Getting Practical . $ docker login/logout // get access to private repo on docker hub $ docker save [image_name] $ docker save -o myfile.tar image_name:latest // saves tar file in current directory $ docker load –i myfile.tar # Load/Save image - Load an image from file: $ docker load &lt; my_image.tar.gz # Save an existing image: $ docker save my_image:my_tag | gzip &gt; my_image.tar.gz # Import/Export container # Import a container as an image from file: $ cat my_container.tar.gz | docker import - my_image:my_tag # Export an existing container: $ docker export my_container | gzip &gt; my_container.tar.gz # Difference between loading a saved image and importing an exported container as an image Loading an image using the load command creates a new image including its history. Importing a container as an image using the import command creates a new image excluding the history which results in a smaller image size compared to loading an image. # Save the State of Docker Image: # we can save the state of image by commiting ( like we do in source control ) $ docker commit my_container # Creates a new Image ID # Restore State of Conatiner $ docker run [options] New_Image_ID # There is problem here. Docker Images ID&#39;s are 256Bit long, So there is no way to remember. what stuff we commit ( solution : is Tagging ) # Tag $ docker tag ID_OF_IMAGE imagename $ docker run imagename ( instead of 256Bit long ID ) # We can also Reffer to a specific image in builds # mention ID of Specific Build of Image ( like we did in previous steps ) and use it docker file. # Remember : This is image is locally available ( docker is not looking this on Docker HUB ) FROM 8eaa4ff06b53 ## Walkthrough of Saving States # Install 2048 Game ( for this we need VNC viewer ( TigerVNC ) $ docker run -d -p 5901:5901 -p 6080:6080 --name win2048 imiell/win2048 $ vncviewer localhost:1 (:1 If you have no X display on host ) # connect to port 5901 &amp; default password for vnc viewer is &#39;vncpass&#39; # Save a Game ( Commit Container ) $ docker commit win2048 1((co14-1)) $ docker tag ID 2048tag:$(date +%s) # Return To the Save Game $ docker run -d -p 5901:5901 -p 6080:6080 --name win2048 my2048tag:$mytag . . 3.7 Generating Dependency graph of Docker Image . # genrate a tree of dependecies of image $ git clone https://github.com/docker-in-practice/docker-image-graph $ cd docker-image-graph $ docker build -t dockerinpractice/docker-image-graph $ docker run --rm -v /var/run/docker.sock:/var/run/docker.sock dockerinpractice/docker-image-graph &gt; docker_images.png . . 3.8 Tricks for Making an Image Smaller . 3.8.1 Method 1 : Reduce the size of Third Party Image . # Step 1 : Remove Unnecessary file # Step 2 : Flatten the Image ( describe in this book ) # Step 3 : Check Which Packages we dont need ( $ dpkg -l | awk &#39;{print $2}&#39; # Step 4 : Remove Packages ( apt-get purge -y package name ) # Step 5 : Clean the cache ( apt-get autoremove, apt-get clean ) # Step 6 : Remove all the man pages and other doc files : $ rm -rf /usr/share/doc/* /usr/share/man/* /usr/share/info/* # Step 7 : Clean the Temp data &amp; logs in (/var) $ find /var | grep &#39; .log$&#39; | xargs rm -v # Step 8 : Commit The Image ( These Steps Creates a Much Smaller Image ) . 3.8.2 Method 2 : Tiny Docker Images with BusyBox and Alpine . Small, usable OSs that can be embedded onto a low-power or cheap computer have existed since Linux began. . # BusyBox ( Weight of BusyBox 2.5 MB ) # BusyBox is so small ( so it can&#39;t uses the bash. It uses ash ) $ docker run -it busybox /bin/ash # problem is that busybox don&#39;t uses any package manager : so for installing packages $ docker run -it progrium/busybox /bin/ash (Size: 5 MB ) # its uses opkg package manager $ opkg-install bash &gt; /dev/null $ bash ( Size of contianer is 6 MB with Bash Shell ) ( get ready to play with bash ) # Alpine ( 36 MB ) Package Manager : APK FROM gliderlabs/alpine:3.6 RUN apk-install mysql-client ENTRYPOINT [&quot;mysql&quot;] list of packages : https://pkgs.alpinelinux.org/packages . 3.8.3 Method 3 : The GO model of minimal containers . # we can minimal Web server with go [ 5 MB Web Server ] https://github.com/docker-in-practice/go-web-server Dockerfile FROM golang:1.4.2 RUN CGO_ENABLED=0 go get -a -ldflags &#39;-s&#39; -installsuffix cgo github.com/docker-in-practice/go-web-server CMD [&quot;cat&quot;,&quot;/go/bin/go-web-server&quot;] $ docker build -t go-webserver . $ mkdir -p go-web-server &amp;&amp; cd go-web-server $ docker run go-webserver &gt; go-web-server $ chmod +x go-web-server $ echo Hi &gt; page.html FROM scratch ADD go-web-server /go-web-server ADD page.html /page.html ENTRYPOINT [&quot;/go-web-server&quot;] $ docker build -t go-web-server . $ docker images | grep go-web-server $ docker run -p 8080:8080 go-web-server -port 8080 . Note : Remember One Large image is much efficient than some small images : Because it saves space on your HDD and save network bandwidth also &amp; Easy to maintainable. . Example : One Ubuntu Image with Node, Python, Nginx and other services ( around 1GB) (assigns only 1 IP) Many small container are request internet ( so they consume bandwidth more &amp; also they will consume more space than 1 GB ) . . . 4.0 Storage Volumes . . 4.1 Day To Day Volumes Command . docker volume create | docker volume rm | docker volume ls | docker volume inspect | . . 4.2 Types of Volumes . The three most common types of storage mounted into containers: . Bind mounts | In-memory storage | Docker volumes | . 4.2.1 Bind Mounts . Bind mounts are mount points used to remount parts of a filesystem tree onto other locations. When working with containers, bind mounts attach a user-specified location on the host filesystem to a specific point in a container file tree. . CONF_SRC=~/example.conf; CONF_DST=/etc/nginx/conf.d/default.conf; LOG_SRC=~/example.log; LOG_DST=/var/log/nginx/custom.host.access.log; docker run -d --name diaweb --mount type=bind,src=${CONF_SRC},dst=${CONF_DST} --mount type=bind,src=${LOG_SRC},dst=${LOG_DST} -p 80:80 nginx:latest . 4.2.2 In-Memory Storage . Most service software and web applications use private key files, database passwords, API key files, or other sensitive configuration files, and need upload buffering space. In these cases, it is important that you never include those types of files in an image or write them to disk. Instead, you should use in-memory storage. You can add in-memory storage to containers with a special type of mount. . # 1777 permissions in octal # tmpfs-size=16k memory-based filesystem into a containerdocker run --rm --mount type=tmpfs,dst=/tmp,tmpfs-size=16k,tmpfs-mode=1770 --entrypoint mount alpine:latest -v . 4.2.3 Docker Volumes . Docker volumes are named filesystem trees managed by Docker. They can be implemented with disk storage on the host filesystem, or another more exotic backend such as cloud storage. All operations on Docker volumes can be accomplished using the docker volume subcommand set. . docker volume create --driver local --label example=location location-example docker volume inspect --format &quot;&quot; location-example . . 4.3 Moving Docker to a different Partition . # Stop Docker Daemon ( service docker stop ) $ $ dockerd -g /home/dockeruser/mydocker . Note : This will wipe all the containers and images from your previous Docker daemon. . . 4.4 Access Filesystem from Docker Container . # This will mount /dotfiles folder to container $ docker run -v /home/hacstac/dotfiles:~/dotfiles -t debian bash . . 4.5 Share Volumes Across the internet . # In this we use a technology called Resilio # Example ( 2 Machines ) : Setup Resilio on both Machine - That synchronized a volume ( Connected through a Secret Key ) # Machine 1 $ docker run -d -p 8888:8888 -p 55555:55555 --name resilio ctlc/btsync # docker logs resilio Or ( Lazy ) -&gt; Use Portainer ( Logs ) # copy secret key $ docker run -it --volumes-from resilio ubuntu /bin/bash # create Data in ubuntu : touch /data/shared_from_server # Machine 2 # setup resilio client $ docker run -d --name resilio-client -p 8888:8888 -p 55555:55555 ctlc/btsync key_of_server # Setup ubuntu $ docker run -it --volumes-from resilio-client ubuntu /bin/bash # our data folder is now available in this &amp; if you create a file in here then it will synchronized to Machine 1 also. . . 4.6 Using a Centralized Data Volumes For Containers . # Create a volume with docker which store a data which you need in other docker containers $ docker run -v /codebase --name codebase busybox # access the codebase $ docker run -it --volumes-from codebase ubuntu /bin/bash . . 4.7 Mounting the remote file systems . # This will need FUSE Kernel Module to be loaded on Host OS ( filesystem and userspace ) # Required Root Access ( Danger ) # 4.7.1. SSHFS # Local Host $ docker run -it --privileged debian /bin/bash # Inside a Container $ sudo apt-get update &amp;&amp; apt-get install sshfs $ LOCALPATH=/path/to/directory/ $ mkdir $LOCALPATH $ sshfs -o nonempty user@host:/path/to/directory $LOCALPATH # to unmount fusermount -u /path/to/local/directory # Now the remote folder is mount on LOCALPATH # 4.7.2 NFS # Install a NFS on host ( because docker doesn&#39;t support NFS ) $ apt-get install nfs-kernel-server $ mkdir /export $ chmod 777 /export $ mount --bind /opt/test/db /export # add this to fstab ( if you want to persist over reboot ) /etc/fstab file: /opt/test/db /export none bind 0 0 # add this to /etc/exports /export 127.0.0.1(ro,fsid=0,insecure,no_subtree_check,async) # to Read/Write : change ro to rw &amp; add no_root_squash (After async) # To open to the internet replace localhost to * ( Danger : Think about it ) $ mount -t nfs 127.0.0.1:/export /mnt $ exportfs -a $ service nfs-kernel-server restart # Run a container $ docker run -it --name nfs-client --privileged -v /mnt:/mnt busybox /bin/true # Mount on other container $ docker run -it --volumes-from nfs-client debian /bin/bash . . . 5.0 Networks in Docker . . 5.1 Day To Day Network Commands . docker network create | docker network rm | docker network ls | docker network inspect | docker network connect | docker network disconnect | . . 5.2 Examples . 5.2.1 To list all networks . $ docker network ls NETWORK ID NAME DRIVER SCOPE f32f6d51e8c8 bridge bridge local 366d3d1f4719 hacstac_default bridge local 6c08bddba2c4 host host local b726554d155b none null local . 5.2.2 To Create a New Network . $ docker network create --driver bridge --label project=dockerinaction --label chapter=5 --attachable --scope local --subnet 10.0.42.0/24 --ip-range 10.0.42.128/25 user-network $ docker run -it --network user-network --name network-explorer alpine:3.8 sh # CTRL-P + CTRL-Q Detech # docker attach network-explorer # walkthrough $ docker network create my_network [ Create a Network ] $ docker network connect my_network blog1 [ blog1 container connect to a network my_network ] $ docker run -it --network my_network ubuntu:16.04 bash [ Now this ubuntu container have access to blog1 Container ] $ ip -f inet -4 -o addr // this will list loopback and assign ip subnet address . 5.2.3 Create a another bridge network . $ docker network create --driver bridge --attachable --scope local --subnet 10.0.43.0/24 --ip-range 10.0.43.128/25 user-network2 # Attach priviously created container attach to this network $ docker network connect user-network2 network-explorer # then this container lists two ethernet addresses # scan with nmap $ nmap -sn 10.0.42.* -sn 10.0.43.* -oG /dev/stdout | grep Status . 5.2.4 With Network - none : Means container with no external excess . $ docker run --rm --network none alpine:3.8 ping -w 2 1.1.1.1 # it will try to ping 1.1.1.1 but it failed because this container hace no external network access # NodePort Publishing # 8080:8000 will denote 8080 port of host machine and 8000 port of container $ docker run -d -p 8080 --name listener alpine:3.8 $ docker port listener // To view port of running container . 5.2.5 DNS with docker . # Feature 1 : --hostname will add hostname : so we open with DN $ docker run --rm --hostname barker alpine:3.8 nslookup barker Server: 10.0.2.3 Address 1: 10.0.2.3 Name: barker Address 1: 172.17.0.22 barker - # Feature 2 : --dns : set dns server on container $ docker run --rm --dns 8.8.8.8 alpine:3.8 nslookup docker.com # Feature 3 : --dns-search : allows us to specify a DNS searchdomain, which is like a default hostname suffix $ docker run --rm --dns-search docker.com --dns 1.1.1.1 alpine:3.8 cat /etc/resolv.conf # Will display contents that look like: # search docker.com # nameserver 1.1.1.1 # Feature 4 : --add-host $ docker run --rm --hostname mycontainer --add-host docker.com:127.0.0.1 --add-host test:10.10.10.2 alpine:3.8 cat /etc/hosts 172.17.0.45 mycontainer 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopbackfe00 ::0 ip6-localnetff00::0 ip6-mcastprefixff02 ::1 ip6-allnodesff02::2 ip6-allrouters 10.10.10.2 test 127.0.0.1 docker.com . 5.2.6 You can specify a specific IP address for a container . # create a new bridge network with your subnet and gateway for your ip block $ docker network create --subnet 203.0.113.0/24 --gateway 203.0.113.254 iptastic # run a nginx container with a specific ip in that block $ docker run --rm -it --net iptastic --ip 203.0.113.2 nginx # curl the ip from any other place (assuming this is a public ip block duh) $ curl 203.0.113.2 . 5.2.7 Open a Docker Daemon to the World . # first of stop the docker service $ sudo service docker stop / or $ sudo systemctl stop docker # Checks for docker daemon ?? $ ps -ef | grep -E &#39;docker(d| -d| daemon) b&#39; | grep -v grep # Expose to local host : 2375 $ sudo docker daemon -H tcp://0.0.0.0:2375 # To connect $ docker -H tcp://&lt;your host&#39;s ip&gt;:2375 &lt;subcommand&gt; . 5.2.8 Get an IP &amp; Ports of a Docker Container . $ alias dl=&#39;docker ps -l -q&#39; ( latest container ID ) $ docker inspect $(dl) | grep -wm1 IPAddress | cut -d &#39;&quot;&#39; -f 4 Pass ( ID of container Instead of dl ) : this above command gives ip of latest container # get ports $ docker inspect -f &#39; -&gt; &#39; 274d2292a137 | name_of_container . . . 6.0 Limiting Risk with Resource Controls . . 6.1 Memory Limits . $ docker container run -d --name ch6_mariadb --memory 256m --cpu-shares 1024 --cap-drop net_raw -e MYSQL_ROOT_PASSWORD=test mariadb:5.5 # This container only uses the 256M memory . . 6.2 CPU Limits . $ docker container run -d -P --name ch6_wordpress --memory 512m --cpu-shares 512 --cap-drop net_raw --link ch6_mariadb:mysql -e WORDPRESS_DB_PASSWORD=test wordpress:5.0.0-php7.2-apache # if total cpu share is 1536 - 512 ( 33% ) : this container consume 33% of cpu shares $ docker container run -d -P --name ch6_wordpress --memory 512m --cpus 0.75 --cap-drop net_raw --link ch6_mariadb:mysql -e WORDPRESS_DB_PASSWORD=test wordpress:5.0.0-php7.2-apache # This Container : consumed max 75% of cpu cores # also we can use {--cpuset-cpus 0-4} (cores) . . 6.3 Access to devices . $ docker container run -it --rm --device /dev/video0:/dev/video0 ubuntu:16.04 ls -al /dev # --device flag will mount external device to container . . 6.4 Sharing Memory ( IPC : Interprocess Communication ) . # Producer $ docker container run -d -u nobody --name ch6_ipc_producer --ipc shareable dockerinaction/ch6_ipc -producer # Consumer $ docker container run -d --name ch6_ipc_consumer --ipc container:ch6_ipc_producer dockerinaction/ch6_ipc -consumer # we can see the process of one container in another : by using docker logs { they share the memory space } # IMP NOTE : In docker to clean Volumes $ docker rm -vf name_of_container . . 6.5 Understanding Users . # if we want to setup a container with diff user then $ docker container run --rm --user nobody busybox:1.29 id $ docker container run --rm -u 1000:1000 busybox:1.29 /bin/bash -c &quot;echo This is important info &gt; /logFiles/important.log&quot; # with this userID:GroupID we can access the file system of this users . . 6.6 OS features Access with Capabilities . Linux capabilities can be set by using cap-add and cap-drop. See https://docs.docker.com/engine/reference/run/#/runtime-privilege-and-linux-capabilities for details. This should be used for greater security. . SYS_MODULE —Insert/remove kernel modules SYS_RAWIO — Modify kernel memory SYS_NICE — Modify priority of processes SYS_RESOURCE — Override resource limits SYS_TIME — Modify the system clock AUDIT_CONTROL — Configure audit subsystem MAC_ADMIN — Configure MAC configuration SYSLOG — Modify kernel print behavior NET_ADMIN — Configure the network SYS_ADMIN — Catchall for administrative functions $ docker container run --rm -u nobody --cap-add sys_admin ubuntu:16.04 /bin/bash -c &quot;capsh --print | grep sys_admin&quot; # this --cap-add sys_admin will add admin facilities to container # we can inspect docker with .HostConfig.CapAdd &amp;&amp; .HostConfig.CapDrop show capabilities # Give access to a single device: $ docker run -it --device=/dev/ttyUSB0 debian bash # Give access to all devices: $ docker run -it --privileged -v /dev/bus/usb:/dev/bus/usb debian bash # Docker Container with full privileges $ docker container run --rm --privileged ubuntu:16.04 capsh ls /dev # check out list of mounted devices . . 6.7 Additional Security with Docker . $ docker container run --rm -it --security-opt seccomp=path_to_the_secomp conf file ubuntu:16.04 sh # For Linux Security Modules ( LSM ) The LSM security option values are specified in one of seven formats: - To prevent a container from gaining new privileges after it starts, use &#39;no-new-privileges&#39; - To set a SELinux user label, use the form label=user:username, where is the name of the user you want to use for the label. - To set a SELinux role label, use the form label=role:role where is the name of the role you want to apply to processes in the container. - To set a SELinux type label, use the form label=type:type , where is the type name of the processes in the container. - To set a SELinux-level label, use the form &#39;label:level:label&#39; , where is the level at which processes in the container should run. Levels are specified as low-high pairs. Where abbreviated to the low level only, SELinux will inter-pret the range as single level. - To disable SELinux label confinement for a container, use the form label=disable # NOTE : Avoid Running Containers in privileged mode whenever possible . . 6.8 Using Socat to monitor docker api traffic . In this technique you’ll insert a proxy Unix domain socket between your request and the server’s socket to see what passes through it. Note that you’ll need root or sudo privileges to make this work. . # We need a socat ( Install socat as per the OS package Maneger ) $ socat -v UNIX-LISTEN:/tmp/dockerapi.sock,fork UNIX-CONNECT:/var/run/docker.sock &amp; In this command, -v makes the output readable, with indications of the flow of data.The UNIX-LISTEN part tells socat to listen on a Unix socket, fork ensures that socatdoesn’t exit after the first request, and UNIX-CONNECT tells socat to connect toDocker’s Unix socket. The &amp; specifies that the command runs in the background.If you usually run the Docker client with sudo, you’ll need to do the same thing here as well. # List all containers $ docker -H unix:///tmp/dockerapi.sock ps -a # This will show how client request to daemon . . 6.9 Setting TimeZone in Containers . # Runs a command to display the time zone on the host $ date +%Z // UTC # change TimeZone FROM centos:7 RUN rm -rf /etc/localtime RUN ln -s /usr/share/zoneinfo/Asia/Kolkata /etc/localtime CMD date +%Z # Build Image $ docker build -t timezone_change . $ docker run timezone_change Asia/Kolkata . . 6.10 Locale Management . locale will be set in the environment through the LANG,LANGUAGE, and locale-gen variables . # you are getting encoding error if correct locale is not set. # Check for locale $ env | grep LANG LANG=en_GB.UTF-8 This is British English, with text encoded in UTF-8. # Set Locale FROM ubuntu:16.04 RUN apt-get update &amp;&amp; apt-get install -y locales RUN locale-gen en_US.UTF-8 ENV LANG en_US.UTF-8 ENV LANGUAGE en_US:en CMD env $ $ docker build -t encoding . . . .",
            "url": "https://hacstac.github.io/Notes/docker/2020/08/29/Docker-Easy-Parts.html",
            "relUrl": "/docker/2020/08/29/Docker-Easy-Parts.html",
            "date": " • Aug 29, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://hacstac.github.io/Notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hacstac.github.io/Notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}