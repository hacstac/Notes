{
  
    
        "post0": {
            "title": "Step By Step Process To Deploy Your Own Kubernetes Cluster",
            "content": ". Overview . Kubernetes is one of orchestration system which are getting popular for last five years. It’s originally made by Google but now it’s maintained by it’s huge community from all over the world. That’s why it might be become a new standard for container orchestration. . But having Kubernetes cluster might be a bit expensive. You have to run at least two or 3 nodes to run the cluster and try several demo projects on that cluster. That could be overkill for someone. But how if we have a Kubernetes that might run in our local computer? That should be an interesting thing. . Hardware Need ( Must Have ) . Raspberry PI (3/4 2GB RAM): Minimum 2 ( 1 Master, 1 Worker ) | Power Cable ( 15W Typc-C for Rasp4 or Your can use POE Hat ) | Ethernet Cable ( Minimum 5e/6 ( Rasp support upto 1Gbps )) | Micro SD Card ( Min 8 GB, Recommanded 32 GB ) | Good Case With FAN ( Saves your rasp from dirt and excessive heat ) | . Software Requirement . Etcher or Raspberry PI Imager | Ubuntu 20.4 64Bit ARM ( OS ) | . . Setup Process . Part - 1: Setup RaspberryPIs . Use Raspberry PI Imager (Recommanded) or Etcher to boot ubuntu 20.4 64Bit on SD Card | When Imager flashes the OS in the SD Card open the boot drive and create a file name ssh (Without any extension). | Boot your Rasps | Check the rasp IP from your router or use nmap to network scan like this : nmap -sP 192.168.0.0/24 | ssh to rasps ( Ex: ssh ubuntu@10.0.1.49 ) | Default username : ubuntu &amp; default password : ubuntu | Do all the basics things you always do with your rasps like setup static IP, setup dotfiles and other stuff. | . Part - 2: Setup MicroK8s . Microk8s provides a single command installation of the latest Kubernetes release on a local machine for development and testing. Setup is quick, fast (~30 sec) and supports many plugins including Istio with a single command. Since K8s is not the easiest thing to get started with, having a tool that would make it easy for you to get going is very desirable. . microk8s is strictly for Linux. There is no VM involved. It is distributed and runs as a snap — a pre-packaged application (similar to a Docker container). Snaps can be used on all major Linux distributions, including Ubuntu, Linux Mint, Debian and Fedora. . A. Setup Docker and Do Some Other Tweaks Before Installing Kubernetes . It is not a surprise we are going to use Docker Engine for the container runtime. Despite there are alternatives in rkt, cri-o and others. However, at a closer look, we can see Kubernetes uses containerd. We use docker because it is most famous conatiner system and quite easy to deploy. . # Do this on all of your nodes ( Rasps ) # Install Docker $ curl -sSL https://get.docker.com | sh -- ## Enable cgroups (Control Groups). Cgroups allow the Linux kernel to limit and isolate resources. # Practically speaking, this allows Kubernetes to better manage resources used by the containers it runs and increases security by isolating containers from one another. # Inspect Docker $ docker info (...) Cgroup Driver: cgroups (...) # If docker info shows this you need to change Cgroup to systemd, To allow systemd to act as the cgroups manager. # Create a /etc/docker/daemon.json ( Add the content to that file ) $ vim /etc/docker/daemon.json { &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;], &quot;log-driver&quot;: &quot;json-file&quot;, &quot;log-opts&quot;: { &quot;max-size&quot;: &quot;100m&quot; }, &quot;storage-driver&quot;: &quot;overlay2&quot;, &quot;insecure-registries&quot; : [&quot;localhost:32000&quot;] } # Restart Docker and inspect again $ sudo systemctl restart docker # Inspect $ docker info (...) Cgroup Driver: systemd (...) # Enable cgroups limit support # Enable limit support, as shown by the warnings in the docker info output above. You need to modify the kernel command line to enable these options at boot. # For the Raspberry Pi 4, add the following to the /boot/firmware/cmdline.txt file: Manually cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1 --or-- With sed # Note the space before &quot;cgroup_enable=cpuset&quot;, to add a space after the last existing item on the line $ sudo sed -i &#39;$ s/$/ cgroup_enable=cpuset cgroup_enable=memory cgroup_memory=1 swapaccount=1/&#39; /boot/firmware/cmdline.txt -- ## Allow Iptables to see bridged traffic # According to the documentation, Kubernetes needs iptables to be configured to see bridged network traffic. # Enable net.bridge.bridge-nf-call-iptables and -iptables6 $ cat &lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF # To check $ sudo sysctl --system . B. Setup MicroK8s . You may follow the installation instruction of MicroK8S in the official documentation. . Start with installing MicroK8s using Snap which will take only a few seconds. . # Do this on all the nodes ( Only Installtion step, Rest of the steps are for just master server ) ## Install MicroK8s $ sudo snap install microk8s --channel=1.19 --classic # Add user to group microk8s &amp; give user permission to ~/.kube $ sudo usermod -a -G microk8s user $ sudo chown -f -R user ~/.kube -- ## Check MicroK8s is Running $ sudo microk8s.status microk8s is running high-availability: no datastore master nodes: 10.0.1.2:19001 datastore standby nodes: none addons: disabled: dashboard # The Kubernetes dashboard dns # CoreDNS ha-cluster # Configure high availability on the current node helm # Helm 2 - the package manager for Kubernetes metrics-server # K8s Metrics Server for API access to service metrics storage # Storage class; allocates storage from host directory helm3 # Helm 3 - Kubernetes package manager host-access # Allow Pods connecting to Host services smoothly ingress # Ingress controller for external access metallb # Loadbalancer for your Kubernetes cluster rbac # Role-Based Access Control for authorisation registry # Private image registry exposed on localhost:32000 # Microk8s comes with a set of tools: microk8s.config microk8s.docker microk8s.inspect microk8s.kubectl microk8s.start microk8s.stop microk8s.disable microk8s.enable microk8s.istioctl microk8s.reset microk8s.status -- ## Check the nodes ( shows only master node ) $ microk8s.kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready &lt;none&gt; 33m v1.19.0-34+09a4aa08bb9e93 ## Add an alias for microk8s.kubectl to saving a ton of time $ sudo snap alias microk8s.kubectl kubectl ## Add AddOns $ sudo microk8s.enable dns dashboard ingress helm helm3 storage metrics-server prometheus # To Check $ microk8s status microk8s is running high-availability: no datastore master nodes: 10.0.1.2:19001 datastore standby nodes: none addons: enabled: dashboard # The Kubernetes dashboard dns # CoreDNS ha-cluster # Configure high availability on the current node helm # Helm 2 - the package manager for Kubernetes metrics-server # K8s Metrics Server for API access to service metrics ingress # Ingress controller for external access helm3 # Helm 3 - Kubernetes package manager storage # Storage class; allocates storage from host directory disabled: host-access # Allow Pods connecting to Host services smoothly metallb # Loadbalancer for your Kubernetes cluster rbac # Role-Based Access Control for authorisation registry # Private image registry exposed on localhost:32000 # To get details of all of your namespaces ( this returns your all the running services, pods, deployements and namespaces etc) $ sudo microk8s.kubectl get all --all-namespaces . C. Access the Kubernetes Dashboard . # We use headless OS, So we have no option to access servicex other than exposing to the intenal network. # By defauly Kubernetes dashboard is not accessible on local network, but if you using raspbionOS ( GUI ) then you can access to cluster ip in you raspi. # --Exposing-- $ kubectl -n kube-system edit service kubernetes-dashboard # Change type: ClusterIP to NodePort # Get service port $ microk8s.kubectl --namespace=kube-system get service kubernetes-dashboard NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes-dashboard NodePort 10.152.183.188 &lt;none&gt; 443:30355/TCP 22m # Dashboard Access : https://MasterServer:Port ( Ex: https://10.0.1.86:30355 ) -- ## Kubernetes dashboard needs authentication to acess to the dashboard # Method 1 : Generate Token $ token=$(microk8s kubectl -n kube-system get secret | grep default-token | cut -d &quot; &quot; -f1) $ microk8s kubectl -n kube-system describe secret $token # Copy Token and Paste on Dashboard Login # If you again need this token $ sudo microk8s.kubectl -n kube-system get secret # Look for something like this kubernetes-dashboard-token-r62xm $ sudo microk8s.kubectl -n kube-system describe secret kubernetes-dashboard-token-r62xm # Shows Secret on Terminal Window --or-- # Method 2 : Setup a Proxy # To Setup Proxy $ sudo microk8s.kubectl proxy --accept-hosts=.* --address=0.0.0.0 &amp; # Edit Dashboard Yaml $ sudo microk8s.kubectl -n kube-system edit deploy kubernetes-dashboard -o yaml # Add ( - --enable-skip-login ) spec: containers: - args: - --enable-skip-login # when login to dashboard just use ( skip ) option # Access the server ( Master Server IP: 10.0.1.86 ) # http://10.0.1.86:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/ -- . E. Add Nodes To The Cluster . # On Master Server $ microk8s add-node # It give join command like this: microk8s join 10.0.1.86:25000/6cae23f7273dc6700f439f8c19abc7de # On Worker Nodes $ microk8s join 10.0.1.86:25000/6cae23f7273dc6700f439f8c19abc7de # To Check $ microk8s kubectl get nodes NAME STATUS ROLES AGE VERSION master Ready &lt;none&gt; 64m v1.19.0-34+09a4aa08bb9e93 node1 Ready &lt;none&gt; 2m49s v1.19.0-34+09a4aa08bb9e93 . F. Deploy Some Fun . F1. Install Portainer . Portainer is a lightweight management UI that allows you to easily manage your different Docker environments. Portainer provides an easy and simple solution for managing Docker containers and Swarm services through a web interface. Portainer supports a wide range of features for managing the Docker containers, such as managing the creation and deletion of Swarm services, user authentication, authorizations, connecting, executing commands in the console of running containers, and viewing containers’ logs. . ## Install Portainer # Using Arkade ( Works on arm &amp; amd64 ) # If Arkade shows cluster unreachable, use (kubectl config view --raw &gt;~/.kube/config) $ curl -sLS https://dl.get-arkade.dev | sudo sh $ arkade install portainer # Access Portainer UI on http://http://10.0.1.86:30777 ( Master Server IP : 10.0.1.86 ) # If you are using ( arm64 ( 64 bit OS on Rasps )) $ curl -LO https://raw.githubusercontent.com/portainer/portainer-k8s/master/portainer-nodeport.yaml $ kubectl apply -f portainer-nodeport.yaml # If it says it not exposed # microk8s kubectl expose deployment portainer --type=NodePort . . F2. Install Linkding ( Bookmark Manager ) . Linkding is a self-hosted bookmark service : sissbruecker/linkding . . # I m installing linkding with persistent storage ( To know about What persistent storage is check out my kubernetes 101 post ) # YAML for Persistent Volume ( pv.yml ) apiVersion: v1 kind: PersistentVolume metadata: name: data spec: accessModes: - ReadWriteOnce capacity: storage: 8Gi hostPath: path: /home/user/data storageClassName: development # YAML for Persistent Volume Claim ( pvc.yml ) apiVersion: v1 kind: PersistentVolumeClaim metadata: name: data spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: development # YAML for Linkding Container Deployment ( Install Linkding With Persistent Storage ) apiVersion: apps/v1 kind: Deployment metadata: name: linkding labels: application: frontend spec: replicas: 1 selector: matchLabels: application: frontend template: metadata: labels: application: frontend spec: containers: - name: linkding image: sissbruecker/linkding ports: - containerPort: 9090 imagePullPolicy: IfNotPresent volumeMounts: - name: data mountPath: /etc/linkding/data volumes: - name: data persistentVolumeClaim: claimName: data -- ## Create Deployments $ kubectl apply -f ./pv.yml $ kubectl apply -f ./pvc.yml $ kubectl apply -f ./linkding.yml # Apply Deployments $ kubectl get deployments NAME READY UP-TO-DATE AVAILABLE AGE linkding 1/1 1 1 102s # Expose to the local network $ microk8s kubectl expose deployment linkding --type=NodePort # Get the Port $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE linkding NodePort 10.152.183.77 &lt;none&gt; 9090:31071/TCP 30s # Access the Linkding Bookmark Manager ( http://10.0.1.86:31071 ) -- # To create username password for Linkding ( Go to Portainer -&gt; Application -&gt; Linkding -&gt; Console Access # python manage.py createsuperuser --username=user --email=admin@example.com # user = user # password = you set in above command . . F3. Install CodeServer . CodeServer is nothing but VS Code on browser ( To use on any machine anywhere and access it in the browser. ) : codercom/code-server . ## Installing code-server with also persitent storage because, give freedom to access code file in your desired folder -- # We already create PersitentVolume, but we need another claim for VSCode # YAML for Persistent Volume Claim ( code-server-pvc.yml ) apiVersion: v1 kind: PersistentVolumeClaim metadata: name: code-data spec: accessModes: - ReadWriteOnce resources: requests: storage: 3Gi storageClassName: development # YAML for code-server container deployment code-server.yml apiVersion: apps/v1 kind: Deployment metadata: labels: app: code-server name: code-server spec: selector: matchLabels: app: code-server replicas: 3 template: metadata: labels: app: code-server spec: containers: - image: codercom/code-server:latest imagePullPolicy: IfNotPresent name: code-server env: - name: PASSWORD value: &quot;password&quot; volumeMounts: - name: data mountPath: /home/coder/project volumes: - name: data persistentVolumeClaim: claimName: code-data -- # Apply Deployments $ kubectl apply -f ./code-server-pvc.yml $ kubectl apply -f ./code-server.yml # Expose to internal network $ kubectl expose deploy code-server --type=NodePort --port=80 --target-port=8080 # Access the server on http://10.0.1.86 # To Scale the deployment $ kubectl scale deployment code-server --replicas=5 . . Conclusion . You should now have an operational Kubernetes master and several worker nodes ready to accept workloads. . I hope MicroK8 will be a great help for newcomers into Kubernetes to try it out and learn Kubernetes by playing with it. If you gave it a shot &amp; liked it, leave me a comment here! . .",
            "url": "https://hacstac.github.io/Notes/kubernetes/2020/09/15/MicroK8s-Installation-Guide.html",
            "relUrl": "/kubernetes/2020/09/15/MicroK8s-Installation-Guide.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Kubernetes 101",
            "content": ". 1.0 What is Kubernetes . Kubernetes is an open-source platform/tool created by Google. It is written in GO-Lang. So currently Kubernetes is an open-source project under Apache 2.0 license. Sometimes in the industry, Kubernetes is also known as “K8s”. With Kubernetes, you can run any Linux container across private, public, and hybrid cloud environments. Kubernetes provides some edge functions, such as Loadbalancer, Service discovery, and Roled Based Access Control(RBAC). . Basically, kubernetes is a software that allows us to deploy, manage and scale applications. The applications will be packed in containers and kubernetes groups them into units. It allows us to span our application over thousands of servers while looking like one single unit. . Key Features of Kubernetes Horizontal Scaling | Auto Scaling | Health check &amp; Self-healing | Load Balancer | Service Discovery | Automated rollbacks &amp; rollouts | Canary Deployment | . | . . 2.0 Why we need Kubernetes . First, we need to familier with few terms: . Monolithic Applications: Years ago, most software applications were big monoliths, running either as a single process or as a small number of processes spread across a handful of servers. These legacy systems are still widespread today. They have slow release cycles and are updated relatively infrequently. At the end of every release cycle, developers pack- age up the whole system and hand it over to the ops team, who then deploys and monitors it. In case of hardware failures, the ops team manually migrates it to the remaining healthy servers. So these components that are all tightly coupled together and have to be developed, deployed, and managed as one entity, because they all run as a single OS process. . Problems with Monolithic Applications: Change of one part of the application require a redeployment of the whole application. Requires powerful servers, Uses Vertical Scaling ( which is Very Expensive ) and If one components creates problem, the whole application becomes unscalable. | . Microservices Applications: Today, these big monolithic legacy applications are slowly being broken down into smaller, independently running components called microservices. Each microservice runs as an independent process and communicates with other microservices through simple, well defined interfaces ( APIs ). Microservices communicate through synchronous protocols such as HTTP, over which they usually expose RESTful (REpresentational State Transfer) APIs, or through asyn- chronous protocols such as AMQP (Advanced Message Queueing Protocol). . Microservices are decoupled from each other, they can be developed, deployed, updated, and scaled individually. This enables you to change components quickly and as often as necessary to keep up with today’s rapidly changing business requirements. . Problems with Microservices Based Applications: The bigger numbers of deployable components and increasingly larger datacenters, it becomes increasingly difficult to configure, manage, and keep the whole system running smoothly. It’s much harder to figure out where to put each of those components to achieve high resource utilization and thereby keep the hardware costs down and the one of biggest problem with scaling microservices is multiple apps that are running on same host mey have conflicting dependencies. One solution of this is applications could run in the exact same environment during development and in production so they have the exact same operating system, libraries, system configuration, networking environment, and everything else but this will not solve the conflicting, versions of libraries or different environment requirements to solve this issue we have great technology called containers. ( You are also provide Dedicated VM to particular service but it is not a ideal solution because it increases hardware cost and management) | . Doing all this manually is hard work. We need automation, which includes automatic scheduling of those components to our servers, automatic configuration, supervision, and failure-handling. This is where Kubernetes comes in. . . Kubernetes provides you with a framework to run distributed systems resiliently. It takes care of scaling and failover for your application, provides deployment patterns, and more. For example, Kubernetes can easily manage a canary deployment for your system. . Before used Kubernetes, you need to prepare your infrastructure to deploy a new microservice. I believe it cost you a few days or weeks. Without Kubernetes, large teams would have to manually script the deployment workflows. With Kubernetes, you don’t need to create your deployment script manually and it will reduce the amount of time and resources spent on DevOps. . 2.1 What kubernetes provides . Service discovery and load balancing: Kubernetes can expose a container using the DNS name or using their own IP address. If traffic to a container is high, Kubernetes is able to load balance and distribute the network traffic so that the deployment is stable. | Storage orchestration: Kubernetes allows you to automatically mount a storage system of your choice, such as local storages, public cloud providers, and more. Automated rollouts and rollbacks You can describe the desired state for your deployed containers using Kubernetes, and it can change the actual state to the desired state at a controlled rate. For example, you can automate Kubernetes to create new containers for your deployment, remove existing containers and adopt all their resources to the new container. | Automatic bin packing: You provide Kubernetes with a cluster of nodes that it can use to run containerized tasks. You tell Kubernetes how much CPU and memory (RAM) each container needs. Kubernetes can fit containers onto your nodes to make the best use of your resources. | Self-healing: Kubernetes restarts containers that fail, replaces containers, kills containers that don’t respond to your user-defined health check, and doesn’t advertise them to clients until they are ready to serve. | Secret and configuration management: Kubernetes lets you store and manage sensitive information, such as passwords, OAuth tokens, and SSH keys. You can deploy and update secrets and application configuration without rebuilding your container images, and without exposing secrets in your stack configuration. | . 2.2 What kubernetes not provides . Kubernetes is a lot of good things, I hope to have been clear exposing all Kubernetes benefits. The main problem from Kubernetes newbie is that they discover it is not a PaaS (Platform as a Service) system like they suppose. Kubernetes is a lot of things but not an “all included” service. It is great and reduces the amount of work, especially on the sysadmin side, but doesn’t offer you any apart from the infrastructure. . Said that most of the things you are looking for in a fully-managed system are there: simplified deployments, scaling, load balancing, logging, and monitoring. Usually, you get a standard configuration from your hosting but you can theoretically customize it if you really need it. . It does not limit the types of applications supported. Everything is written into the container so every container application, no matter on technology, can be run. The counterpart is that you still have to define the container by hand. | It doesn’t offer an automated deployment. You just have to push to a docker repository your built images, no more. This is quite easy if you already work in a process with Continuous Integration, Delivery, and Deployment (CI/CD), but consider that without it will be quite tricky. | It does not provide any application-level services, just infrastructure. If you need a database, you have to buy a service or run it into a dedicated container. Of course, taking charges of backups and so on. | Most of the interactions with the system are by a command line that wraps API. That’s very good because it allows automating each setting. Commands syntax is very simple but, if you are looking to a system that is managed fully by a UI you are looking to the worst place. | . . 3.0 How does Kubernetes work . The system is composed of a master node and any number of worker nodes. When the developer submits a list of apps to the master, Kubernetes deploys them to the cluster of worker nodes. What node a component lands on doesn’t (and shouldn’t) matter—neither to the developer nor to the system administrator. . . Kubernetes will run your containerized app somewhere in the cluster, provide information to its components on how to find each other, and keep all of them running. Because your application doesn’t care which node it’s running on, Kubernetes can relocate the app at any time, and by mixing and matching apps, achieve far better resource utilization than is possible with manual scheduling. . 3.1 Architecture of Kubernetes Cluster . We’ve seen a bird’s-eye view of Kubernetes’ architecture. Now let’s take a closer look at what a Kubernetes cluster is composed of. At the hardware level, a Kubernetes cluster is composed of many nodes, which can be split into two types: . The master node, which hosts the Kubernetes Control Plane that controls and manages the whole Kubernetes system | Worker nodes that run the actual applications you deploy | . . 3.1.1 Master ( Control Plane ) . The Control Plane is what controls the cluster and makes it function. It consists of multiple components that can run on a single master node or be split across multiple nodes and replicated to ensure high availability. These components are . Kubernetes API Server: The application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster ( which admin and other control plane communicated with ). . | Scheduler: Scheduler watches API server for new Pod requests. It communicates with Nodes to create new pods and to assign work to nodes while allocating resources or imposing constraints. . | Controller Manager: Component on the master that runs controllers. Includes Node controller, Endpoint Controller, Namespace Controller, etc. ( Performs cluster-level functions, such as replicating components, keeping track of worker nodes, handling node failures, and so on ) . Node Controller: Responsible for noticing and responding when nodes go down. | Replication Controller: Responsible for maintaining the correct number of pods for every replication controller object in the system. | Endpoints Controller: Populates the Endpoints object (that is, it joins Services and Pods). | Service Account and Token Controllers: Create default accounts and API access tokens for new namespaces. | Cloud-Controller-Manager: Cloud-controller-manager runs controllers that interact with the underlying cloud providers. The cloud-controller-manager binary is an alpha feature introduced in Kubernetes release 1.6. Cloud-controller-manager runs cloud-provider-specific controller loops only. You must disable these controller loops in the Kube-controller-manager. You can disable the controller loops by setting the –cloud-provider flag to external when starting the Kube-controller-manager. | Node Controller: For checking the cloud provider to determine if a node has been deleted in the cloud after it stops responding. | Route Controller: For setting up routes in the underlying cloud infrastructure. | Service Controller: For creating, updating, and deleting cloud provider load balancers. | Volume Controller: For creating, attaching, and mounting volumes, and interacting with the cloud provider to orchestrate volumes. | . | etcd: A reliable distributed data store that persistently stores the cluster configuration. . | . The components of the Control Plane hold and control the state of the cluster, but they don’t run your applications. This is done by the (worker) nodes. . 3.1.2 Slave ( Worker Nodes ) . The worker nodes are the machines that run your containerized applications. The task of running, monitoring, and providing services to your applications is done by the following components: . Docker, rkt, or another container runtime, which runs your containers . | Kubelet: Kubectl registering the nodes with the cluster, watches for work assignments from the scheduler, instantiate new Pods, report back to the master. Container Engine: Responsible for managing containers, image pulling, stopping the container, starting the container, destroying the container, etc. . | Kube Proxy: Responsible for forwarding app user requests to the right pod (load-balances network traffic between application components). . | The sequence of deployment: DevOps -&gt; API Server -&gt; Scheduler -&gt; Cluster -&gt;Nodes -&gt; Kubelet -&gt; Container Engine -&gt; Spawn Container in Pod . | The sequence of App user request: App user -&gt; Kube proxy -&gt; Pod -&gt; Container(Your app is run here) . | . 3.2 The Six Layers of K8s . . Deployments create and manage ReplicaSets, which create and manage Pods, which run on Nodes, which have a container runtime, which run the app code you put in your Docker image. . The levels shaded blue are higher-level K8s abstractions. The green levels represent Nodes and Node subprocess that you should be aware of, but may not touch. . 3.2.1 Deployments . Although pods are the basic unit of computation in Kubernetes, they are not typically directly launched on a cluster. Instead, pods are usually managed by one more layer of abstraction: the deployment. A deployment’s primary purpose is to declare how many replicas of a pod should be running at a time. When a deployment is added to the cluster, it will automatically spin up the requested number of pods, and then monitor them. If a pod dies, the deployment will automatically re-create it. Using a deployment, you don’t have to deal with pods manually. You can just declare the desired state of the system, and it will be managed for you automatically. . 3.2.2 ReplicaSet . The Deployment creates a ReplicaSet that will ensure your app has the desired number of Pods. ReplicaSets will create and scale Pods based on the triggers you specify in your Deployment. Replication Controllers perform the same function as ReplicaSets, but Replication Controllers are old school. ReplicaSets are the smart way to manage replicated Pods in 2019. . 3.2.3 Pods . All containers will run in a pod. Pods abstract the network and storage away from the underlying containers. Your app will run here. Each pod has one unique IP address assigned which means one pod can communicate with each other like a traditional container in a docker environment. Each container inside the pod can reach all other pods into the virtual network, but cannot deep to the other container on other pods. That’s important to guarantee the pod abstraction: nobody has to know how the Pods are composed internally. Moreover, the IP assigned is volatile so you must use always the service name (resolved to the right IP directly). Pods are used as the unit of replication in Kubernetes. If your application becomes too popular and a single pod instance can’t carry the load, Kubernetes can be configured to deploy new replicas of your pod to the cluster as necessary. Even when not under heavy load, it is standard to have multiple copies of a pod running at any time in a production system to allow load balancing and failure resistance. Pods handle Volumes, Secrets, and configuration for containers. Pods are ephemeral. They are intended to be restarted automatically when they die. . Note: Worker Node is already explained in above section . 3.3 Key Terms used with Kubernetes . 3.3.1 Services . The name “service” in informatics science is overused. In Kubernetes scope think to a service like something you want to serve. A Kubernetes service involves a set of pods and may offer a complex feature or just expose a single Pod with a single container. So you can have a service that provides a CMS feature, with database and web server inside, or two different services, one for the database and one for the webserver. That’s up to you. Basically it is abstraction layer on top of a set of ephemeral pods (think of this as the ‘face’ of a set of pods) . 3.3.2 Ingress . By default, Kubernetes provides isolation between pods and the outside world. If you want to communicate with a service running in a pod, you have to open up a channel for communication. This is referred to as ingress. To do this there is an ingress controller that does something similar to a load balancer. It virtually forwards traffic from outside to the services. During this step, based on the ingress implementation you have chosen, you can add HTTPS encryption, route traffic based on the hostname or URL segments. The only service can be linked to the ingress controller, not Pods. There are multiple ways to add ingress to your cluster. The most common ways are by adding either an Ingress controller, or a LoadBalancer. . 3.3.3 Volume . By default Pod storage is volatile. This is important to know because at the first restart you will lose everything. Kubernetes volumes allow mapping some part of the hard drive of containers to a safe place. That space can be shared between containers. The mount point can be any part of the container, but a volume cannot be mount into another one. . PersistentVolumes and PersistentVolumeClaims: To help abstract away infrastructure specifics, K8s developed PersistentVolumes and PersistentVolumeClaims. Unfortunately the names are a bit misleading, because vanilla Volumes can have persistent storage, as well. PersisententVolumes (PV) and PersisentVolumeClaims (PVC) add complexity compared to using Volumes alone. However, PVs are useful for managing storage resources for large projects. With PVs, a K8s user still ends up using a Volume, but two steps are required first. . A PersistentVolume is provisioned by a Cluster Administrator (or it’s provisioned dynamically). | An individual Cluster user who needs storage for a Pod creates PersistentVolumeClaim manifest. It specifies how much and what type of storage they need. K8s then finds and reserves the storage needed. | The user then creates a Pod with a Volume that uses the PVC. PersistentVolumes have lifecycles independent of any Pod. In fact, the Pod doesn’t even know about the PV, just the PVC. PVCs consume PV resources, analogously to how Pods consume Node resources. . 3.3.4 Namespaces . Think to namespace like the feature that makes Kubernetes multitenant. The namespace is the tenant level. Each namespace can partitioning resources to isolate services, ingress, and many other things. This feature is good to have a strong separation between application, delegate safely to different teams, and have separated environments in a single infrastructure. It is a virtual cluster on top of an underlying physical cluster . 3.3.5 Labels and Selectors . Labels are key-value pairs used to tag objects. Objects are the items that you create in a Kubernetes cluster (pods, deployments, replica sets, services, volumes etc). Selectors are used to collect objects based on tags. . 3.3.6 StatefulSets . As we know, a ReplicaSet creates and manages Pods. If a Pod shuts down because a Node fails, a ReplicaSet can automatically replace the Pod on another Node. You should generally create a ReplicaSet through a Deployment rather than creating it directly, because it’s easier to update your app with a Deployment. . . Sometimes your app will need to keep information about its state. You can think of state as the current status of your user’s interaction with your app. So in a video game it’s all the unique aspects of the user’s character at a point in time. What do you do when your app has state you need to keep track of? Use a StatefulSet. . Like a ReplicaSet, a StatefulSet manages deployment and scaling of a group of Pods based on a container spec. Unlike a Deployment, a StatefulSet’s Pods are not interchangeable. Each Pod has a unique, persistent identifier that the controller maintains over any rescheduling. StatefulSets for good for persistent, stateful backends like databases. The state information for the Pod is held in a Volume associated with the StatefulSet. . 3.3.7 DaemonSets . DaemonSets are for continuous process. They run one Pod per Node. Each new Node added to the cluster automatically gets a Pod started by the DaemonSet. DaemonSets are useful for ongoing background tasks such as monitoring and log collection. StatefulSets and DaemonSets are not controlled by a Deployment. Although they are at the same level of abstraction as a ReplicaSet, there is not a higher level of abstraction for them in the current API. . 3.3.8 ETCD . It stores the configuration information which can be used by each of the nodes in the cluster. It is a high availability key-value store that can be distributed among multiple nodes. It is accessible only by Kubernetes API server as it may have some sensitive information. It is a distributed key-value store which is accessible to all. ETCD is a distributed reliable key-value store used by Kubernetes to store all data used to manage the cluster. Think of it this way, when you have multiple nodes and multiple masters in your cluster, etcd stores all that information on all the nodes in the cluster in a distributed manner. ETCD is responsible for implementing locks within the cluster to ensure there are no conflicts between the Masters. . 3.3.9 Cluster IP . Only has a Virtual IP (also called Cluster IP). This service can be used within a cluster only Acts like a traffic router to your Pods inside the cluster. Each port exposed by a pod will need a service if you want a client to talk to it via that port. By default, the service port is the same as the port exposed by the Pod. Different methods to access this service: From any node inside the cluster: &lt;cluster IP&gt;:&lt;service port&gt; From the node where a replica of the Pod is running: &lt;pod IP&gt;:&lt;pod port&gt;. . 3.3.10 Node Port . For this service, a physical port on the node is mapped to the service port and the service connects to the Pod via the pod port. Remember, this will also have a Virtual IP (i.e. cluster IP). Different methods to access this service: From outside the cluster, if any node in the cluster has a public IP: &lt;node public IP&gt;:&lt;node port&gt; From any node inside the cluster: &lt;cluster IP&gt;:&lt;service port&gt; From the node where a replica of the Pod is running: &lt;pod IP&gt;:&lt;pod port&gt;. . 3.3.11 Load Balancer . Giving public access to a node in your cluster is not a recommended method, When you want to give public access to a service, create a load balancer service (not getting into ingress at this stage). For each service that you want to give public access, you need to create a load balancer service, This type of service will generally work only in a cloud environment. Remember, this will also have a Virtual IP (i.e. cluster IP) This service will have a node port too Different methods to access this service: From outside the cluster: &lt;load balancer dns&gt;:&lt;service port&gt; From outside the cluster, if any node in the cluster has a public IP: &lt;node public IP&gt;:&lt;node port&gt; From any node inside the cluster: &lt;cluster IP&gt;:&lt;service port&gt; From the node where a replica of the Pod is running: &lt;pod IP&gt;:&lt;pod port&gt;. . 3.3.12 External Name . This maps a service to endpoints completely outside of the cluster. . .",
            "url": "https://hacstac.github.io/Notes/kubernetes/2020/09/15/Kubernetes-Introduction.html",
            "relUrl": "/kubernetes/2020/09/15/Kubernetes-Introduction.html",
            "date": " • Sep 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Docker Hard Parts [Part - 2]",
            "content": "7.0 Building Images automatically with DockerFiles . . 7.1 Instructions . .dockerignore | FROM Sets the Base Image for subsequent instructions. | MAINTAINER (deprecated - use LABEL instead) Set the Author field of the generated images. | RUN execute any commands in a new layer on top of the current image and commit the results. | CMD provide defaults for an executing container. | EXPOSE informs Docker that the container listens on the specified network ports at runtime. NOTE: does not actually make ports accessible. | ENV sets environment variable. | ADD copies new files, directories or remote file to container. Invalidates caches. Avoid ADD and use COPY instead. | COPY copies new files or directories to container. By default this copies as root regardless of the USER/WORKDIR settings. Use --chown=&lt;user&gt;:&lt;group&gt; to give ownership to another user/group. (Same for ADD.) | ENTRYPOINT configures a container that will run as an executable. | VOLUME creates a mount point for externally mounted volumes or other containers.* USER sets the user name for following RUN / CMD / ENTRYPOINT commands. | WORKDIR sets the working directory. | ARG defines a build-time variable. | ONBUILD adds a trigger instruction when the image is used as the base for another build. | STOPSIGNAL sets the system call signal that will be sent to the container to exit. | LABEL apply key/value metadata to your images, containers, or daemons. | . NOTE : If your running dangerous commands ( add a logic that it fails if your shell is outside docker ) . # Shell script fails if it’s run outside a container #!/bin/bash if ! [ -f /.dockerenv ] then echo &#39;Not in a Docker container, exiting.&#39; exit 1 fi . . 7.2 Getting Practical . 7.2.1 Packaging Git with Dockerfile . # Create a file name Dockerfile FROM ubuntu:latest LABEL maintainer=&quot;dia@allingeek.com&quot; RUN apt-get update &amp;&amp; apt-get install -y git ENTRYPOINT [&quot;git&quot;] # Instantly Create Dockerfile Images $ docker build -t htop - &lt;&lt; EOF FROM alpine RUN apk --no-cache add htop EOF $ docker image build --tage ubuntu-git:auto . FROM ubuntu:latest — Tells Docker to start from the latest Ubuntu image just as you did when creating the image manually. LABEL maintainer — Sets the maintainer name and email for the image. Provid- ing this information helps people know whom to contact if there’s a problem with the image. This was accomplished earlier when you invoked commit. RUN apt-get update &amp;&amp; apt-get install -y git — Tells the builder to run the provided commands to install Git. ENTRYPOINT [&quot;git&quot;] — Sets the entrypoint for the image to git. # --file or -f will read from diff file like &#39;BuildScript or anything&#39; # --quiet or -q will run in quiet mode . . 7.2.2 Dockerignore . .dockerignore file will help us to exclude some files to add to the image during the build | CLI modifies the context to exclude files and directories that match patterns in it. This helps to avoid unnecessarily sending large or sensitive files and directories to the daemon and potentially adding them to images using ADD or COPY. | . . 7.2.3 File System Instructions . - COPY : Will copy files from the filesystem where the image is being built. - VOLUME : Same as --volume flag ( bound mount volume ) - CMD : This is a closely related to ENTRYPOINT - ADD : This operates similarly to the COPY instruction with two imp differences: - fetch remote sources files if a Url is specified - Extract the files of any source determined to be an archive file - ONBUILD : This instruction let the other instructions to execute ( if resulting image is used as base img for another build ) # Example # ADD : We can ADD large no of files to a container without any problem # Docker will unpack tarfiles of most standard types (.gz, .bz2, .xz, .tar). # some.tar FROM Debian RUN mkdir -p /opt/libeatmydata ADD some.tar.gz /opt/libeatmydata/ RUN ls -lRt /opt/libeatmydata # ONBUILD : Use the ONBUILD command to automate and encapsulate the building of an image. # GO Example ( Outyet : Simple Go App ) $ git clone https://github.com/golang/example $ cd example/outyet $ docker build -t outyet . $ docker run --publish 8080:8080 --name outyet1 -d outyet With ONBUILD FROM golang:onbuild EXPOSE 8080 golang:onbuild Dockerfile FROM golang:1.7 RUN mkdir -p /go/src/app WORKDIR /go/src/app CMD [&quot;go-wrapper&quot;, &quot;run&quot;] ONBUILD COPY . /go/src/app ONBUILD RUN go-wrapper download ONBUILD RUN go-wrapper install The result of this technique is that you have an easy way to build an image that only contains the code required to run it, and no more. There are also other examples of ONBUILD exists : node:onbuild , python:onbuild # ENTRYPOINT : Sets the entrypoint for the image # Basic Shell Script for clean logs #!/bin/bash echo &quot;Cleaning logs over $1 days old&quot; find /log_dir -ctime &quot;$1&quot; -name &#39;*log&#39; -exec rm {} ; # Create a DockerFile ( Create a container with clean_log script ) FROM ubuntu:17.04 ADD clean_log /usr/bin/clean_log RUN chmod +x /usr/bin/clean_log ENTRYPOINT [&quot;/usr/bin/clean_log&quot;] CMD [&quot;7&quot;] $ docker build -t log-cleaner . $ docker run -v /var/log/myapplogs:/log_dir log-cleaner 365 # Clean The logs of over a year ( default 7 days if no arg given ) . . 7.2.4 Create a Maintainable Dockerfiles . - ARG : arg defines a variable thta users can provide to docker when building and a image. Ex: Dockerfile ARG VERSION=unknown ENV VERSION=&quot;${VERSION}&quot; LABEL base.version=&quot;${VERSION}&quot; $ version=0.6; docker image build -t dockerinaction/mailer-base:${version} -f mailer-base.df --build-arg VERSION=${version} . $ docker image inspect --format &#39;&#39; dockerinaction/mailer-base:0.6 { &quot;base.name&quot;: &quot;Mailer Archetype&quot;, &quot;base.version&quot;: &quot;0.6&quot;, &quot;maintainer&quot;: &quot;dia@allingeek.com&quot; } . . 7.2.5 Init System for Docker . Most Popular init systems are : runit, tini, BusyBox init, Supervisord, and DAEMON | By Default docker comes with tini init system | . $ docker container run -it --init alpine:3.6 nc -l -p 3000 # Docker ran /dev/init -- nc -l -p 3000 inside the container instead of just nc . . 7.2.6 Health Check In Docker . - There are two ways to specify the health check command: 1. Use a HEALTHCHECK instruction when defining the image 2. On the command-line when running a container 1. This is a 1st Mothed : It is used when we define an Image $ FROM nginx:1.13-alpine HEALTHCHECK --interval=5s --retries=2 CMD nc -vz -w 2 localhost 80 || exit 1 $ docker ps --format &#39;table t t&#39; NAMES IMAGE STATUS healthcheck_ex dockerinaction/healthcheck Up 3 minutes (healthy) # Exit Status Codes - 0: success—The container is healthy and ready for use. - 1: unhealthy—The container is not working correctly. - 2: reserved—Do not use this exit code. 2. Command-line Method $ docker container run --name=healthcheck_ex -d --health-cmd=&#39;nc -vz -w 2 localhost 80 || exit 1&#39; nginx:1.13-alpine . . 7.2.7 Hardening Application Images . - There are Three Methods to hardended the images 1. We can enforce that our images are built from a specific image. 2. we can make sure that regardless of how containers are built from our image, they will have a sensible default user. 3. we should eliminate a common path for root user escalation from programs with setuid or setgid attributes set. 1. Content Addressable Images identifiers ( CAIID ) # Build Images using authentic digest : that digest is known as CAIID # So now how many images we build from these they are authentic. docker pull debian:stable stable: Pulling from library/debian 31c6765cabf1: Pull complete Digest: sha256:6aedee3ef827... # Dockerfile: FROM debian@sha256:6aedee3ef827... ... 2. Create a User &amp; groups $ RUN groupadd -r postgres &amp;&amp; useradd -r -g postgres postgres 3. SUID &amp; GUID $ FROM ubuntu:latest # Set the SUID bit on whoami RUN chmod u+s /usr/bin/whoami # Create an example user and set it as the default RUN adduser --system --no-create-home --disabled-password --disabled-login --shell /bin/sh example USER example # Set the default to compare the container user and # the effective user for whoami CMD printf &quot;Container running as: %s n&quot; $(id -u -n) &amp;&amp; printf &quot;Effectively running whoami as: %s n&quot; $(whoami) Output Container running as: example Effectively running whoami as: root The output of the default command shows that even though you’ve executed the whoami command as the example user, it’s running from the context of the root user. . . 7.2.8 Complete Story of Cache . # --no-cache will downlaod fresh containers from source. it will not use the cache files # Example $ docker build --no-cache . # Busting the Cache # we need this because some time our image build takes so much time. so we need cache up to a certain point. # Method 1 (cheat) : Add a benign comment after the command to invalidate the cache. This works because Docker treats the non-whitespace change to theline as though it were a new command, so the cached layer is not re-used. CMD [&quot;npm&quot;,&quot;start&quot;] #bust the cache # Method 2 ( using ARG ) Use the ARG directive in your Dockerfile to enable surgical cache-busting. If this ARG variable isset to a value never used before on your host, the cache will be busted from that point. WORKDIR todo ARG CACHEBUST=no RUN npm install $ docker build --build-arg CACHEBUST=${RANDOM} . $ echo ${RANDOM} 19856 $ echo ${RANDOM} 26429 # If not using Bash $ docker build --build-arg CACHEBUST=$(date +%s) . # Method 3 ( Using ADD ) There are two useful features of ADD that you can use to your advantage in this context: it caches the contents of the file it refers to, and it can take a network resource as an argument. # Git Repo Example It means if repo is not changed it uses cache or if git repo is changed it rebuild the image from scratch. But it will vary from resources type to resource type. we can take help of Github API here : It has URLs foreach repository that return JSON for the most recent commits. When a new commit ismade, the content of the response changes. FROM ubuntu:16.04 ADD https://api.github.com/repos/nodejs/node/commits /dev/null RUN git clone https://github.com/nodejs/node . . 7.2.9 Flattening Images . # This is imp because images can reveal the imp information Example : # create a docker file : It has sensitive information FROM debian RUN echo &quot;My Big Secret&quot; &gt;&gt; /tmp/secret_key RUN cat /tmp/secret_key RUN rm /tmp/secret_key $ docker build -t secret . # But now problem arise $ docker history secret ... 5e39caf7560f 3 days ago /bin/sh -c echo &quot;My Big Secret&quot; &gt;&gt; /tmp/se 14 B ... $ docker run 5b376ff3d7cd cat /tmp/secret_key My Big Secret But if someone could download this image from public repo and insect the history &amp; run thi command : It reveal the secret. # To get rid of this type of problem : we need to remove intermediate layering # we need to export the image as a trivially run container and then re-import and tag the resulting image: $ docker run -d secret /bin/true - new_id # Runs a docker export, taking a conatiner iD as arg and outgoing a tar file of fs contents. # This is piped to docker import which takes tar file and create a new image. $ docker export new_id | docker import - new_secret $ docker history new_secret . . 7.3 Refer . Examples | Best practices for writing Dockerfiles | Michael Crosby has some more Dockerfiles best practices / take 2. | Building Good Docker Images / Building Better Docker Images | Managing Container Configuration with Metadata | How to write excellent Dockerfiles | . . . 8.0 Registry &amp; Repository . . A repository is a hosted collection of tagged images that together create the file system for a container. . A registry is a host – a server that stores repositories and provides an HTTP API for managing the uploading and downloading of repositories. . 8.1 Enviornment . docker login to login to a registry. | docker logout to logout from a registry. | docker search searches registry for image. | docker pull pulls an image from registry to local machine. | docker push pushes an image to the registry from local machine. | . NOTE : Refer Chapter 9 of Docker in Action : Public and private software distribution ( This Cover’s Every Basic detail about Registry &amp; Repository ) . 8.2 Setup Local Docker Registry . # To start a registry on local Network $ docker run -d -p 5000:5000 -v $HOME/registry:/var/lib/registry registry:2 # This command makes the registry available on port 5000 of the Docker host(-p 5000:5000). With the -v flag, it makes the registry folder on your host(/var/lib/registry) available in the container as $HOME/registry. The registry’s fileswill therefore be stored on the host in the /var/lib/registry folder. # HOSTNAME is the hostname or IP address of your new reg-istry server # we also use --insecure-registry ( We know our local network is secure :) [Docker will only allow you to pull from registries with a signedHTTPS certificate.] ) # Push the image to the registry $ docker push HOSTNAME:5000/image:tag. . . . 9.0 Docker Compose . Compose is a tool for defining and running multi-container Docker applications. With Compose, you use a YAML file to configure your application’s services. Then, with a single command, you create and start all the services from your configuration. To learn more about all the features of Compose, see the list of features. . The standard filename for Compose files is docker-compose.yml. . 9.1 YAML Basics . A YAML document can include a comment at the end of any line. Com￾ments are marked by a space followed by a hash sign ( #). Any characters that follow until the end of the line are ignored by the parser. | YAML uses three types of data and two styles of describing that data, block and flow. Flow collections are specified similarly to collection literals in JavaScript and other languages. For example, the following is a list of strings in the flow style: [&quot;PersonA&quot;,&quot;PersonB&quot;] | The block style is more common and will be used in this primer except where noted. The three types of data are maps, lists, and scalar values. Maps are defined by a set of unique properties in the form of key/value pairs that are delimited by a colon and space (: ). | Scaler Values : Scaler String image: &quot;alpine&quot; , Scaler Command : command: echo hello world | Scaler Rules : Must not be empty, | Must not contain leading or trailing whitespace characters | Must not begin with an indicator character (for example, - or :) in places where doing so would cause an ambiguity. | Must never contain character combinations using a colon (:) and hash sign (#) | | Lists (or block sequences) are series of nodes in which each element is denoted by a leading hyphen (-) indicator. For example: - item 1 - item 2 - item 3 - # an empty item - item 4 | . | . | Indentation Rules : YAML uses indentation to indicate content scope. Scope determines which block each element belongs to. There are a few rules: Only spaces can be used for indentation. | The amount of indentation does not matter as long as – All peer elements (in the same scope) have the same amount of indentation. – Any child elements are further indented. | . | . These documents are equivalent: . top-level: second-level: # three spaces third-level: # two more spaces - &quot;list item&quot; # single additional indent on items in this list another-third-level: # a third-level peer with the same two spaces fourth-level: &quot;string scalar&quot; # 6 more spaces another-second-level: # a 2nd level peer with three spaces - a list item # list items in this scope have # 15 total leading spaces - a peer item # A peer list item with a gap in the list # every scope level adds exactly 1 space top-level: second-level: third-level: - &quot;list item&quot; another-third-level: fourth-level: &quot;string scalar&quot; another-second-level: - a list item - a peer item . 9.2 Docker Compose Basics . By using the following command you can start up your application: . # first install docker-compose on your system (eg: Ubuntu ) $ sudo apt install docker-compose $ docker-compose -f &lt;docker-compose-file&gt; up . You can also run docker-compose in detached mode using -d flag, then you can stop it whenever needed by the following command: . docker-compose stop . You can bring everything down, removing the containers entirely, with the down command. Pass --volumes to also remove the data volume. . Let understand Docker compose with an Example : . 9.2.1 Example yml . # wikijs.yml version: &#39;2&#39; services: db: image: postgres:11-alpine environment: POSTGRES_DB: wiki POSTGRES_PASSWORD: wikijsrocks POSTGRES_USER: wikijs logging: driver: &quot;none&quot; restart: unless-stopped volumes: - db-data:/var/lib/postgresql/data wiki: image: requarks/wiki:2 depends_on: - db environment: DB_TYPE: postgres DB_HOST: db DB_PORT: 5432 DB_USER: wikijs DB_PASS: wikijsrocks DB_NAME: wiki restart: unless-stopped ports: - &quot;80:3000&quot; volumes: db-data: # we can create a docker stack with this yml file ( it established a wikijs and postgresql db ) $ docker stack deploy -c wikijs.yml wikijs # if we use docker swarn or Kubernetes we can create a replicas of this images into 3 diff servers -- deploy: replicas: 3 -- # add this to end of yml and again deploy it. It will update the container # To Check The State of stack $ docker stack ps --format &#39; t&#39; wikijs # To remove a service from stack ( like limit the replicas from 3 to 2 ) $ docker stack deploy -c wikijs.yml --prune wikijs We Need To use Prue Here : Because without Prune it will not completely remove services which causes problmes ( like for Instead of using postgressql we want to use a mysql so if we updated our stack yml file and deploy it. it will add mysql db but doesnt remove postgres containers so to remove postgres we use prune ) The --prune flag will clean up any resource in the stack that isn’t explicitly referenced in the Compose file used for the deploy operation. # Prune The new [Data Management Commands](https://github.com/docker/docker/pull/26108) have landed as of Docker 1.13: * `docker system prune` * `docker volume prune` * `docker network prune` * `docker container prune` * `docker image prune` Note : There is a problem here everytime a container replaced docker will create a new volume space for container and its replicas. This would cause problems in a real-world system. So, to get rid of this EX: volumes: pgdata: # empty definition uses volume defaults services: postgres: image: dockerinaction/postgres:11-alpine volumes: - type: volume source: pgdata # The named volume above target: /var/lib/postgresql/data environment: POSTGRES_PASSWORD: example The file defines a volume named pgdata, and the postgres service mounts that volume at /var/lib/postgresql/data. That location is where the Postgre￾SQL software will store any database schema or data. Inspect $ docker stack deploy -c databases.yml --prune my-databases $ docker volume ls DRIVER VOLUME NAME local my-databases_pgdata $ docker service remove my-databases_postgres Then restore the service by using the Compose file: $ docker stack deploy -c databases.yml --prune my-databases . . . 10.0 DevOps Operations With Docker . . 10.1 Convert Your Virtual Box VM To Docker Container . # Process : VM FILE (.vdi or anything) =&gt; TAR =&gt; Import TAR as an Image in Docker. $ sudo apt install qemu-utils # Identify the path to your VM disk image. ( Stop the VM ) # Sets up a variable pointingto your VM disk image. $ VMDISK=&quot;$HOME/VirtualBox VMs/myvm/myvm.vdi&quot; # Initializes a kernelmodule requiredby qemu-nbd $ sudo modprobe nbd # Connects the VM disk to a virtual device node $ sudo qemu-nbd -c /dev/nbd0 -r $VMDISK3((CO1-3)) # Lists the partition numbers available to mount on this disk $ ls /dev/nbd0p* /dev/nbd0p1 /dev/nbd0p2 # Mounts the selected partition at /mnt with qemu-nbd $ sudo mount /dev/nbd0p2 /mnt # Creates a TAR filecalled img.tar from /mnt $ sudo tar cf img.tar -C /mnt . # Unmounts and cleans up after qemu-nbd $ sudo umount /mnt &amp;&amp; sudo qemu-nbd -d /dev/nbd0 # Dockerfile FROM scratch ADD img.tar / $ docker build . . . 10.2 Host Like Container . Containers are not virtual machines—there are significant differences—and pretending there aren’t can cause confusion and issues down the line. . Differences between VMs and Docker containers: Docker is application-oriented, whereas VMs are operating-system oriented. | Docker containers share an operating system with other Docker containers. Incontrast, VMs each have their own operating system managed by a hypervisor. | Docker containers are designed to run one principal process, not manage mul-tiple sets of processes. | . | . docker run -d phusion/baseimage [ This Image designed to run multiple processes.] docker exec -i -t container_ID /bin/bash ps -ef [ It starts (cron, sshd, and syslog). It Much like a host. ] . . 10.3 Running GUI in Containers . Process : Create an image with your user credentials and the program, and bind mount your Xserver to it. Note : another method is to setup VNC server on container . # Dockerfile for setting up firefox on container FROM ubuntu:14.04 RUN apt-get update RUN apt-get install -y firefox RUN groupadd -g GID USERNAME RUN useradd -d /home/USERNAME -s /bin/bash -m USERNAME -u UID -g GID USER USERNAME ENV HOME /home/USERNAME CMD /usr/bin/firefox $ docker build -t gui . $ docker run -v /tmp/.X11-unix:/tmp/.X11-unix -h $HOSTNAME -v $HOME/.Xauthority:/home/$USER/.Xauthority -e DISPLAY=$DISPLAY gui # It will popup firefox ( which runs on container ) . . 10.4 Using Docker Machine to Provision Docker Hosts . Docker-machine is a tool just like a vagrant. EX: we can setup VM with virtualBox with docker command. walkthrough: . # Install Docker Machine on Linux $ curl -L https://github.com/docker/machine/releases/download/v0.16.2/docker-machine-`uname -s`-`uname -m` &gt;/tmp/docker-machine &amp;&amp; chmod +x /tmp/docker-machine &amp;&amp; sudo cp /tmp/docker-machine /usr/local/bin/docker-machine # Create a VM by docker daemon on Oracle Virtual Box $ docker-machine create --driver virtualbox host1 # Run this command to set the DOCKER_HOST environment variable, which sets the default host that Docker commands will be run on $ docker-machine env host1 $ eval $(docker-machine env host1) # we can direct to VM $ docker-machine ssh host1 Commands of Docker-Machine: create : Creates a new Machine ls : List Machines stop : Stop Machines start : Start Machines restart : Restart Machines rm : Destroys the machine inspect : Returns a JSON representation of the machine’s metadata config : Return the config of machine ip : Returns the IP address of the machine url : Returns a URL for the Docker daemon on the machine upgrade : Upgrades the Docker version on the host to the latest . . 10.5 Build Images using a Chef Solo . Chef : It is a configuration Management Tool ( using this can reduce the amount of work required to configure Images ) . Here we setup hello world apache website ( Example ). | . # Need a working code $ git clone https://github.com/docker-in-practice/docker-chef-solo-example.git # All the working code in there $ cd into it ( there is a Dockerfile and some other files and folders ( chef recepies etc ) $ docker build -t chef-example . $ docker run -it -p 8080:80 chef-example # This is a one time written code we can anywhere to deploy a website ( in such a small steps ) . . 10.6 CI Operations With Docker . CI : Continious Integration . . 10.6.1 Steps . Check out a clean copy of the source code defining the image and build scripts so the origin and process used to build the image is known. | Retrieve or generate artifacts that will be included in the image, such as the application package and runtime libraries. | Build the image by using a Dockerfile. | Verify that the image is structured and functions as intended. | (Optional) Verify that the image does not contain known vulnerabilities. | Tag the image so that it can be consumed easily. | Publish the image to a registry or another distribution channel. | . 10.6.2 Method 1 : Builds a Image Using DockerHub Workflow ( Test and Push Images ) . # For this you will Git Repo and docker Hub Repo # Link Docker Hub to to git repo ( it take code from git repo and compile and create a desired Image ( like other ci tools do ) # wait for the docker hub to build to complete # Remember this is a basic solution . 10.6.3 Method 2 : Setting up a package cache for faster Builds . While building the images it will take caches instead of download everytime from internet. . # We are using squid Proxy Here $ sudo apt-get install squid-deb-proxy $ check for port 8000 $ create a Docker File using a apt proxy FROM debian RUN apt-get update -y &amp;&amp; apt-get install net-tools RUN echo &quot;Acquire::http::Proxy &quot;http://$( route -n | awk &#39;/^0.0.0.0/ {print $2}&#39; ):8000 &quot;;&quot; &gt; /etc/apt/apt.conf.d/30proxy RUN echo &quot;Acquire::http::Proxy::ppa.launchpad.net DIRECT;&quot; &gt;&gt; /etc/apt/apt.conf.d/30proxy CMD [&quot;/bin/bash&quot;] This will cache all the webpages and apt package we downlaod after running this container : if again download them they will be download in miliseconds . 10.6.3 Method 3 : Running the Jenkins Master Withing the Docker Container . Portable Jenkins Server . # download git clone https://github.com/docker-in-practice/jenkins.git. # Put your required plugins in jenkins_plugins.txt Dockerfile FROM jenkins COPY jenkins_plugins.txt /tmp/jenkins_plugins.txt RUN /usr/local/bin/plugins.sh /tmp/jenkins_plugins.txt USER root RUN rm /tmp/jenkins_plugins.txt RUN groupadd -g 999 docker RUN addgroup -a jenkins docker USER jenkins $ docker build -t jenkins . $ docker run --name jenkins -p 8080:8080 -p 50000:50000 -v /var/run/docker.sock:/var/run/docker.sock -v /tmp:/var/jenkins_home -d jenkins # go to localhost:8080 : Copy master password from logs # Reliably Upgrade a Jenkins Server Dockerfile FROM docker ADD jenkins_updater.sh /jenkins_updater.sh RUN chmod +x /jenkins_updater.sh ENTRYPOINT /jenkins_updater.sh Shell script to backup and restart Jenkins #!/bin/sh set -e set -x if ! docker pull jenkins | grep up.to.date then docker stop jenkins docker rename jenkins jenkins.bak.$(date +%Y%m%d%H%M) cp -r /var/docker/mounts/jenkins_home /var/docker/mounts/jenkins_home.bak.$(date +%Y%m%d%H%M) docker run -d --restart always -v /var/docker/mounts/jenkins_home:/var/jenkins_home --name jenkins -p 8080:8080 jenkins fi # docker Command to run the Jenkins Updater $ docker run --rm -d -v /var/lib/docker:/var/lib/docker -v /var/run/docker.sock:/var/run/docker.sock -v/var/docker/mounts:/var/docker/mounts dockerinpractice/jenkins-updater # to automate the process add this command to crontab 0 * * * * dokcker_command . . 10.7 CD Operations with Docker . CD : Continious Delivery ( CI + Deployment ) . # Copy an Image Between Two Registries Process =&gt; Pulling the image from the registry -&gt; retag -&gt; pushing the new Image $ docker tag -f $OLDREG/$MYIMAGE $NEWREG/$MYIMAGE $ docker push $NEWREG/$MYIMAGE $ docker rmi $OLDREG/$MYIMAGE $ docker image prune -f # Copy an Image btw Two Machine With a very low-bandwidth Connection Process =&gt; Here we use backup tool called Bup ( creates a bup data tool ) # Example ( Not Exact data is used so please Don&#39;t judge me ) # pull two images like Ubuntu:18.04 &amp; 19.10 ( Both are example = 65 MB Each = 130 MB ) $ mkdir bup_pool $ alias dbup=&quot;docker run --rm -v $(pwd)/bup_pool:/pool -v /var/run/docker.sock:/var/run/docker.sock dockerinpractice/dbup&quot; $ dbup save ubuntu:18.04 $ du -sh bup_pool ( 74 MB ) $ dbup save ubuntu:19.10 $ du -sh bup_pool ( 96 MB ) ( Saves 35 MB ) # On other machine ( rsync from host1 to host2 ) $ dbup load ubuntu:18.04 # Also copies files between host with TAR Docker export : creates Container To TAR Docker Import : TAR to Image Docker save : Image To TAR Docker load : TAR to Docker Image Example : Transfer docker Image directory over ssh $ docker export $(docker run -d debian:7.3 true) | ssh user@host docker import . . 10.8 Cordination Between Containers . We need coordination between containers : Like if we take an example of one server and one python based echo client. P1: If we start the client container first : It will lead to failure P2 : forgetting to remove the containers will result in problems when you try to restart P3 : Naming containers incorrectly will result in failure. So how to get rid of this types of problem : we need a solution,where we can run the container without any problem. . # Solution : create a compose file version: &quot;3&quot; services: echo-server: image: server expose: - &quot;2000&quot; client: image: client links: - echo-server:talkto # with this we can start container in correct order &amp; also we call rebuild the container anywhere $ docker-compose up Attaching to dockercompose_server_1, dockercompose_client_1 client_1 | Received: Hello, world client_1 | client_1 | Received: Hello, world client_1 | . . 11.0 Security With Docker . . This is where security tips about Docker go. The Docker security page goes into more detail. . First things first: Docker runs as root. If you are in the docker group, you effectively have root access. If you expose the docker unix socket to a container, you are giving the container root access to the host. . Docker should not be your only defense. You should secure and harden it. . For an understanding of what containers leave exposed, you should read Understanding and Hardening Linux Containers by Aaron Grattafiori. This is a complete and comprehensive guide to the issues involved with containers, with a plethora of links and footnotes leading on to yet more useful content. The security tips following are useful if you’ve already hardened containers in the past, but are not a substitute for understanding. . 11.1 Security Tips . For greatest security, you want to run Docker inside a virtual machine. This is straight from the Docker Security Team Lead – slides / notes. Then, run with AppArmor / seccomp / SELinux / grsec etc to limit the container permissions. See the Docker 1.10 security features for more details. . Docker image ids are sensitive information and should not be exposed to the outside world. Treat them like passwords. . See the Docker Security Cheat Sheet by Thomas Sjögren: some good stuff about container hardening in there. . Check out the docker bench security script, download the white papers. . Snyk’s 10 Docker Image Security Best Practices cheat sheet . You should start off by using a kernel with unstable patches for grsecurity / pax compiled in, such as Alpine Linux. If you are using grsecurity in production, you should spring for commercial support for the stable patches, same as you would do for RedHat. It’s $200 a month, which is nothing to your devops budget. . Since docker 1.11 you can easily limit the number of active processes running inside a container to prevent fork bombs. This requires a linux kernel &gt;= 4.3 with CGROUP_PIDS=y to be in the kernel configuration. . docker run --pids-limit=64 . Also available since docker 1.11 is the ability to prevent processes from gaining new privileges. This feature have been in the linux kernel since version 3.5. You can read more about it in this blog post. . docker run --security-opt=no-new-privileges . From the Docker Security Cheat Sheet (it’s in PDF which makes it hard to use, so copying below) by Container Solutions: . Turn off interprocess communication with: . docker -d --icc=false --iptables . Set the container to be read-only: . docker run --read-only . Verify images with a hashsum: . docker pull debian@sha256:a25306f3850e1bd44541976aa7b5fd0a29be . Set volumes to be read only: . docker run -v $(pwd)/secrets:/secrets:ro debian . Define and run a user in your Dockerfile so you don’t run as root inside the container: . RUN groupadd -r user &amp;&amp; useradd -r -g user user USER user . 11.2 Security Videos . Using Docker Safely | Securing your applications using Docker | Container security: Do containers actually contain? | Linux Containers: Future or Fantasy? | . 11.3 Security Roadmap . The Docker roadmap talks about seccomp support. There is an AppArmor policy generator called bane, and they’re working on security profiles. . . Till 21 Aprill . Path to Complete . Docker access | Security Measures In Docker | Securing Access to Docker | Security from Outside docker | . . 12.0 Monitoring Docker . 12.1 Monitoring . . 12.2 Resource Control . . 12.3 Some Advance Approches .",
            "url": "https://hacstac.github.io/Notes/docker/2020/09/01/Docker-Hard-Parts.html",
            "relUrl": "/docker/2020/09/01/Docker-Hard-Parts.html",
            "date": " • Sep 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Docker Easy Parts [Part - 1]",
            "content": "1.0 What is a Docker . . 1.1 Introduction . Defination : Docker is a tool designed to make it easier to create, deploy, and run applications by using containers. Containers allow a developer to package up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package. . Docker container technology was launched in 2013 as an open source Docker Engine. Containers encapsulate an application as a single executable package of software that bundles application code together with all of the related configuration files, libraries, and dependencies required for it to run. . Containerized applications are “isolated” in that they do not bundle in a copy of the operating system. Instead, an open source Docker engine is installed on the host’s operating system and becomes the conduit for containers to share an operating system with other containers on the same computing system. . . . 1.2 Below is a list of common Docker terms . Docker Engine is a client-server application with 3 major components - a server which is a type of long-running program called a daemon process; a REST API which specifies interfaces that programs can use to talk to the daemon and instruct it what to do; a command line interface (CLI) client. . | Docker daemon (dockerd) listens for Docker API requests and manages Docker objects such as images, containers, networks, and volumes. . | Docker client (docker) is the primary way that many Docker users interact with Docker. When you use commands such as docker run, the client sends these commands to dockerd, which carries them out. The docker command uses the Docker API. The Docker client can communicate with more than one daemon. . | Image is a read-only template with instructions for creating a Docker container. You might create your own images or you might only use those created by others and published in a registry. To build your own image, you create a Dockerfile with a simple syntax for defining the steps needed to create the image and run it. . | Docker registry stores Docker images. Docker Hub is a public registry that anyone can use, and Docker is configured to look for images on Docker Hub by default. . | Container is a runnable instance of an image. Containers are made possible by operating system (OS) process isolation and virtualization, which enable multiple application components to share the resources of a single instance of an OS kernel. . | . 1.3 Architecture of docker . Docker on your host machine is (at the time of writing) split into two parts—a daemon with a RESTful API and a client that talks to the daemon. You invoke the Docker client to get information from or give instructions to the daemon; the daemon is a server that receives requests and returns responses from the client using the HTTP protocol. In turn, it will make requests to other services to send and receive images, also using the HTTP protocol. The server will accept requests from the command-line client or anyone else authorized to connect. The daemon is also responsible for taking care of your images and containers behind the scenes, whereasthe client acts as the intermediary between you and the RESTful API. . . . . . 2.0 Installation . . Docker can operate on most of the Operating Systems In Industries : Windows, MacOS, Most Of flavours of Linux ( Ubuntu, RHEL, Arch) | Docker Operates on both AMD64 and ARM Based Systems | . # Simple script to install docker on Linux $ curl -sSL https://get.docker.com/ | sh # Check Docker Verison $ docker version --format &#39;&#39; 19.03.8 # Dump Raw JSON DATA : Like Kernel, Architecture , details , build time etc $ docker version --format &#39;&#39; # Running Docker without sudo $ sudo usermod -aG docker username or $ sudo addgroup -a username docker # restart docker . . . 3.0 Docker Basics . . 3.1 Day To Day Docker Commands . . docker create creates a container but does not start it. | docker rename allows the container to be renamed. | docker run creates and starts a container in one operation. | docker rm deletes a container. | docker update updates a container’s resource limits. | docker images shows all images. | docker cp copies files or folders between a container and the local filesystem. | docker build creates image from Dockerfile. | docker commit creates image from a container, pausing it temporarily if it is running. | docker rmi removes an image. . | docker start starts a container so it is running. | docker stop stops a running container. | docker restart stops and starts a container. | docker pause pauses a running container, “freezing” it in place. | docker unpause will unpause a running container. | docker wait blocks until running container stops. | docker kill sends a SIGKILL to a running container. | docker attach will connect to a running container. . | docker ps shows running containers. | docker logs gets logs from container. (You can use a custom log driver, but logs is only available for json-file and journald in 1.10). | docker inspect looks at all the info on a container (including IP address). | docker events gets events from container. | docker port shows public facing port of container. | docker top shows running processes in container. | docker stats shows containers’ resource usage statistics. | docker diff shows changed files in the container’s FS. | docker history shows history of image. | docker tag tags an image to a name (local or registry). | . . 3.2 Getting Practical . . 3.2.1 List Images . docker images // show images docker ps -a docker ps // shows started containers -a = all containers . 3.2.2 Start/Stop/Restart . docker stop/start/restart . Note : If you want to detach from a running container, use Ctrl + P, Ctrl + Q. If you want to integrate a container with a host process manager, start the daemon with -r=false then use docker start -a. . 3.2.3 Logs . docker logs {Name_of_container} . 3.2.4 Rename . docker rename new_name current_name // rename container . 3.2.5 Create container . docker create nginx // will only create a container . 3.2.6 Example Run . $ docker run --interactive --tty --link web:web --name web_test busybox:1.29 /bin/sh -d = detach automatically the container (run container in background and print container ID) --interactive --tty or -t that will allocate a pseudo-TTY session --link = link to other container --name = name of docker container $ docker exec web_test ps // show extra process run with this containers . 3.2.7 Docker Run with SHELL Variables . CID=$(docker create nginx:latest) echo $CID // assigns to a Shell variable MAILER_CID=$(docker run -d dockerinaction/ch2_mailer) WEB_CID=$(docker create nginx) $ docker start $AGENT_CID $ docker start $WEB_CID . 3.2.8 Env Variables . $ docker run -d --name wpdb -e MYSQL_ROOT_PASSWORD=ch2demo mysql:5.7 $ docker run --env MY_ENVIRONMENT_VAR=&quot;this is a test&quot; busybox:1.29 env # --env flag, or -e for short, can be used to inject any environment variable. . 3.2.9 With Read Only Option . docker run -d --name wp --read-only wordpress:5.0.0-php7.2-apache // create a container with only readonly options . 3.2.10 Inspect . # The docker inspect command will display all the metadata(JSON) $ docker inspect --format &quot;&quot; wp // Prints true if container is running $ docker inspect --format &quot;&quot; Id/name // Show ip of container . 3.2.11 Diff ( Filesystem check ) . $ docker run -d --name wp_writable wordpress:5.0.0-php7.2-apache # let’s check where Apache changed the container’s filesystem with the docker $ docker container diff wp_writable A - A file or directory was added D - A file or directory was deleted C - A file or directory was changed C /run C /run/apache2 A /run/apache2/apache2.pid . 3.2.12 Clean Up . docker rm -f {container_ID) docker rm -vf $(docker ps -a -q) docker rmi [name] // Remove an image . 3.2.13 Executing Commands . # docker exec to execute a command in container. # To enter a running container, attach a new shell process to a running container called foo, use: $ docker exec -it foo /bin/bash. # exec Modes : # 1 Basic : Runs the command in the container synchronously on the command line $ docker exec name_of_container echo &quot;Hello User&quot; Hello User # 2 Daemon : Runs the command in the background on the container $ docker exec -d name_C find / -ctime 7 -name &#39;*log&#39; -exec rm {} ; # 3 Interactive : Runs the command and allows the user to interact with it $ docker exec -it ubuntu /bin/bash . 3.2.15 Linking containers for port isolation . Note : This is an older method of declaring container communication—Docker’s link flag. This isn’t the recommended way of working anymore. . # Example # This will allow us communication between containers without using user-defined networks. $ docker run --name wp-mysql -e MYSQL_ROOT_PASSWORD=yoursecretpassword -d mysql $ docker run --name wordpress --link wp-mysql:mysql -p 10003:80 -d wordpress . 3.2.16 Search a Docker Image . docker search node docker pull node // Pull the Image by Name ( on Hub ) docker run -it node /bin/bash ( Start Node Container ) . 3.2.17 Cleanly Kill Containers . # Always use docker stop ( it actually stops the containers ). # Docker kill will send immediate signal which will kill process while running ( so they can create temp files ) kill Term 15 docker kill Kill 9 docker stop Term 15 . 3.2.18 Docker Prune . # Prune commands docker system prune docker volume prune docker network prune docker container prune docker image prune # Example # Nuclear Option ( if you want to remove all containers of your host machine ) [Removes all : Runnig &amp; exited] $ docker ps -a -q | xargs --no-run-if-empty docker rm -f # To keep running containers $ docker ps -a -q --filter status=exited | xargs --no-run-if-empty docker rm # To list out all exited &amp; failed Containers $ comm -3 &lt;(docker ps -a -q --filter=status=exited | sort) &lt;(docker ps -a -q --filter=exited=0 | sort) | xargs --no-run-if-empty docker inspect &gt; error_containers # Prune Volumes # List out all docker voluems $ docker volume ls # Delete Unused Volumes $ docker volume prune . 3.2.19 Space Occupied By docker System . $ docker system df TYPE TOTAL ACTIVE SIZE RECLAIMABLE Images 7 1 1.963GB 1.885GB (95%) Containers 1 1 0B 0B Local Volumes 2 1 242.4MB 242.3MB (99%) Build Cache 0 0 0B 0B . 3.2.20 Container Stats . $ docker stats ID CONTAINER ID NAME CPU % MEM USAGE / LIMIT MEM % NET I/O BLOCK I/O PIDS . 3.2.21 Tag . docker image tag ubuntu-git:latest ubuntu-git:2.7 . 3.2.22 Commit . $ docker container commit -a &quot;@dockerinaction&quot; -m &quot;Added git&quot; image-dev ubuntu-git # Outputs a new unique image identifier like: # bbf1d5d430cdf541a72ad74dfa54f6faec41d2c1e4200778e9d4302035e5d143 # Build a New Image For Commited Image $ docker container run -d ubuntu-git . 3.2.23 Set an EntryPoint . docker container run --name cmd-git --entrypoint git ubuntu-git . . 3.2.24 Versioning Best Practice . # Docker official Repo&#39;s are the best example of tagging an image # Example for go lang 1.x 1.9 1.9.6 1.9-stretch 1.10-alpine latest # this is an example of tags to build that don&#39;t confuse the end user . . 3.3 States of Docker . Docker container can be in one of six states: . Created | Running | Restarting | Paused | Removing | Exited (also used if the container has never been started) | . . Created : A container that has been created (e.g. with docker create) but not started | Running : A currently running container | Paused : A container whose processes have been paused | Exited : A container that ran and completed (“stopped” in other contexts, although a created container is technically also “stopped”) | Dead : A container that the daemon tried and failed to stop (usually due to a busy device or resource used by the container) | Restarting : A container that is in the process of being restarted | . 3.3.1 Restart State . Using the --restart flag at container-creation time, you can tell Docker to do any of the following: . Never restart (default) | Attempt to restart when a failure is detected | Attempt for some predetermined time to restart when a failure is detected | Always restart the container regardless of the condition . | Methods . no = = Don’t restart when the container exits | always == Always restart when the container exits | unless-stopped == Always restart, but remember explicitly stopping | on-failure[:max-retry] == Restart only on failure | . | . # Example Run $ docker run -d --name backoff-detector --restart always busybox:1.29 date $ docker logs -f backoff-detector . . 3.4 Init &amp; PID Systems for Docker . Several such init systems could be used inside a container. The most popular include runit, Yelp/dumb-init, tini, supervisord, and tianon/gosu . . $ docker run -d -p 80:80 --name lamp-test tutum/lamp $ docker top lamp-test $ docker exec lamp-test ps $ docker exec lamp-test kill &lt;PID&gt; // kill a process $ docker run --entrypoint=&quot;cat&quot; wordpress:5.0.0-php7.2-apache /usr/local/bin/docker-entrypoint.sh # If you run through the displayed script, you’ll see how it validates the environment variables against the dependencies of the software and sets default values. Once the script has validated that WordPress can execute . . 3.5 Software Installation Simplified . Three main ways to install Docker images: . Using Docker registries | Using image files with docker save and docker load | Building images with Dockerfiles | . we can install software in three other ways: . You can use alternative repository registries or run your own registry. | You can manually load images from a file. | You can download a project from some other source and build an image by using a provided Dockerfile. | . Note : - Keep in mind for Tags with images [latest, stable, alpha, Beta] . Download image from another regestry instead of docker hub : docker pull quay.io/dockerinaction/ch3_hello_registry:latest | [REGISTRYHOST:PORT/][USERNAME/]NAME[:TAG] | . 3.5.1 Installing Images using dockerfile . git clone https://github.com/dockerinaction/ch3_dockerfile.git docker build -t dia_ch3/dockerfile:latest ch3_dockerfile . . 3.6 Backup &amp; Restore Docker Images . docker export turns container filesystem into tarball archive stream to STDOUT. | docker import creates an image from a tarball. | docker load loads an image from a tar archive as STDIN, including images and tags (as of 0.7). | docker save saves an image to a tar archive stream to STDOUT with all parent layers, tags &amp; versions (as of 0.7). | . 3.6.1 Comparison . Docker export : Container To TAR Docker Import : TAR to Image Docker save : Image To TAR Docker load : TAR to Image . 3.6.2 Getting Practical . $ docker login/logout // get access to private repo on docker hub $ docker save [image_name] $ docker save -o myfile.tar image_name:latest // saves tar file in current directory $ docker load –i myfile.tar # Load/Save image - Load an image from file: $ docker load &lt; my_image.tar.gz # Save an existing image: $ docker save my_image:my_tag | gzip &gt; my_image.tar.gz # Import/Export container # Import a container as an image from file: $ cat my_container.tar.gz | docker import - my_image:my_tag # Export an existing container: $ docker export my_container | gzip &gt; my_container.tar.gz # Difference between loading a saved image and importing an exported container as an image Loading an image using the load command creates a new image including its history. Importing a container as an image using the import command creates a new image excluding the history which results in a smaller image size compared to loading an image. # Save the State of Docker Image: # we can save the state of image by commiting ( like we do in source control ) $ docker commit my_container # Creates a new Image ID # Restore State of Conatiner $ docker run [options] New_Image_ID # There is problem here. Docker Images ID&#39;s are 256Bit long, So there is no way to remember. what stuff we commit ( solution : is Tagging ) # Tag $ docker tag ID_OF_IMAGE imagename $ docker run imagename ( instead of 256Bit long ID ) # We can also Reffer to a specific image in builds # mention ID of Specific Build of Image ( like we did in previous steps ) and use it docker file. # Remember : This is image is locally available ( docker is not looking this on Docker HUB ) FROM 8eaa4ff06b53 ## Walkthrough of Saving States # Install 2048 Game ( for this we need VNC viewer ( TigerVNC ) $ docker run -d -p 5901:5901 -p 6080:6080 --name win2048 imiell/win2048 $ vncviewer localhost:1 (:1 If you have no X display on host ) # connect to port 5901 &amp; default password for vnc viewer is &#39;vncpass&#39; # Save a Game ( Commit Container ) $ docker commit win2048 1((co14-1)) $ docker tag ID 2048tag:$(date +%s) # Return To the Save Game $ docker run -d -p 5901:5901 -p 6080:6080 --name win2048 my2048tag:$mytag . . 3.7 Generating Dependency graph of Docker Image . # genrate a tree of dependecies of image $ git clone https://github.com/docker-in-practice/docker-image-graph $ cd docker-image-graph $ docker build -t dockerinpractice/docker-image-graph $ docker run --rm -v /var/run/docker.sock:/var/run/docker.sock dockerinpractice/docker-image-graph &gt; docker_images.png . . 3.8 Tricks for Making an Image Smaller . 3.8.1 Method 1 : Reduce the size of Third Party Image . # Step 1 : Remove Unnecessary file # Step 2 : Flatten the Image ( describe in this book ) # Step 3 : Check Which Packages we dont need ( $ dpkg -l | awk &#39;{print $2}&#39; # Step 4 : Remove Packages ( apt-get purge -y package name ) # Step 5 : Clean the cache ( apt-get autoremove, apt-get clean ) # Step 6 : Remove all the man pages and other doc files : $ rm -rf /usr/share/doc/* /usr/share/man/* /usr/share/info/* # Step 7 : Clean the Temp data &amp; logs in (/var) $ find /var | grep &#39; .log$&#39; | xargs rm -v # Step 8 : Commit The Image ( These Steps Creates a Much Smaller Image ) . 3.8.2 Method 2 : Tiny Docker Images with BusyBox and Alpine . Small, usable OSs that can be embedded onto a low-power or cheap computer have existed since Linux began. . # BusyBox ( Weight of BusyBox 2.5 MB ) # BusyBox is so small ( so it can&#39;t uses the bash. It uses ash ) $ docker run -it busybox /bin/ash # problem is that busybox don&#39;t uses any package manager : so for installing packages $ docker run -it progrium/busybox /bin/ash (Size: 5 MB ) # its uses opkg package manager $ opkg-install bash &gt; /dev/null $ bash ( Size of contianer is 6 MB with Bash Shell ) ( get ready to play with bash ) # Alpine ( 36 MB ) Package Manager : APK FROM gliderlabs/alpine:3.6 RUN apk-install mysql-client ENTRYPOINT [&quot;mysql&quot;] list of packages : https://pkgs.alpinelinux.org/packages . 3.8.3 Method 3 : The GO model of minimal containers . # we can minimal Web server with go [ 5 MB Web Server ] https://github.com/docker-in-practice/go-web-server Dockerfile FROM golang:1.4.2 RUN CGO_ENABLED=0 go get -a -ldflags &#39;-s&#39; -installsuffix cgo github.com/docker-in-practice/go-web-server CMD [&quot;cat&quot;,&quot;/go/bin/go-web-server&quot;] $ docker build -t go-webserver . $ mkdir -p go-web-server &amp;&amp; cd go-web-server $ docker run go-webserver &gt; go-web-server $ chmod +x go-web-server $ echo Hi &gt; page.html FROM scratch ADD go-web-server /go-web-server ADD page.html /page.html ENTRYPOINT [&quot;/go-web-server&quot;] $ docker build -t go-web-server . $ docker images | grep go-web-server $ docker run -p 8080:8080 go-web-server -port 8080 . Note : Remember One Large image is much efficient than some small images : Because it saves space on your HDD and save network bandwidth also &amp; Easy to maintainable. . Example : One Ubuntu Image with Node, Python, Nginx and other services ( around 1GB) (assigns only 1 IP) Many small container are request internet ( so they consume bandwidth more &amp; also they will consume more space than 1 GB ) . . . 4.0 Storage Volumes . . 4.1 Day To Day Volumes Command . docker volume create | docker volume rm | docker volume ls | docker volume inspect | . . 4.2 Types of Volumes . The three most common types of storage mounted into containers: . Bind mounts | In-memory storage | Docker volumes | . 4.2.1 Bind Mounts . Bind mounts are mount points used to remount parts of a filesystem tree onto other locations. When working with containers, bind mounts attach a user-specified location on the host filesystem to a specific point in a container file tree. . CONF_SRC=~/example.conf; CONF_DST=/etc/nginx/conf.d/default.conf; LOG_SRC=~/example.log; LOG_DST=/var/log/nginx/custom.host.access.log; docker run -d --name diaweb --mount type=bind,src=${CONF_SRC},dst=${CONF_DST} --mount type=bind,src=${LOG_SRC},dst=${LOG_DST} -p 80:80 nginx:latest . 4.2.2 In-Memory Storage . Most service software and web applications use private key files, database passwords, API key files, or other sensitive configuration files, and need upload buffering space. In these cases, it is important that you never include those types of files in an image or write them to disk. Instead, you should use in-memory storage. You can add in-memory storage to containers with a special type of mount. . # 1777 permissions in octal # tmpfs-size=16k memory-based filesystem into a containerdocker run --rm --mount type=tmpfs,dst=/tmp,tmpfs-size=16k,tmpfs-mode=1770 --entrypoint mount alpine:latest -v . 4.2.3 Docker Volumes . Docker volumes are named filesystem trees managed by Docker. They can be implemented with disk storage on the host filesystem, or another more exotic backend such as cloud storage. All operations on Docker volumes can be accomplished using the docker volume subcommand set. . docker volume create --driver local --label example=location location-example docker volume inspect --format &quot;&quot; location-example . . 4.3 Moving Docker to a different Partition . # Stop Docker Daemon ( service docker stop ) $ $ dockerd -g /home/dockeruser/mydocker . Note : This will wipe all the containers and images from your previous Docker daemon. . . 4.4 Access Filesystem from Docker Container . # This will mount /dotfiles folder to container $ docker run -v /home/hacstac/dotfiles:~/dotfiles -t debian bash . . 4.5 Share Volumes Across the internet . # In this we use a technology called Resilio # Example ( 2 Machines ) : Setup Resilio on both Machine - That synchronized a volume ( Connected through a Secret Key ) # Machine 1 $ docker run -d -p 8888:8888 -p 55555:55555 --name resilio ctlc/btsync # docker logs resilio Or ( Lazy ) -&gt; Use Portainer ( Logs ) # copy secret key $ docker run -it --volumes-from resilio ubuntu /bin/bash # create Data in ubuntu : touch /data/shared_from_server # Machine 2 # setup resilio client $ docker run -d --name resilio-client -p 8888:8888 -p 55555:55555 ctlc/btsync key_of_server # Setup ubuntu $ docker run -it --volumes-from resilio-client ubuntu /bin/bash # our data folder is now available in this &amp; if you create a file in here then it will synchronized to Machine 1 also. . . 4.6 Using a Centralized Data Volumes For Containers . # Create a volume with docker which store a data which you need in other docker containers $ docker run -v /codebase --name codebase busybox # access the codebase $ docker run -it --volumes-from codebase ubuntu /bin/bash . . 4.7 Mounting the remote file systems . # This will need FUSE Kernel Module to be loaded on Host OS ( filesystem and userspace ) # Required Root Access ( Danger ) # 4.7.1. SSHFS # Local Host $ docker run -it --privileged debian /bin/bash # Inside a Container $ sudo apt-get update &amp;&amp; apt-get install sshfs $ LOCALPATH=/path/to/directory/ $ mkdir $LOCALPATH $ sshfs -o nonempty user@host:/path/to/directory $LOCALPATH # to unmount fusermount -u /path/to/local/directory # Now the remote folder is mount on LOCALPATH # 4.7.2 NFS # Install a NFS on host ( because docker doesn&#39;t support NFS ) $ apt-get install nfs-kernel-server $ mkdir /export $ chmod 777 /export $ mount --bind /opt/test/db /export # add this to fstab ( if you want to persist over reboot ) /etc/fstab file: /opt/test/db /export none bind 0 0 # add this to /etc/exports /export 127.0.0.1(ro,fsid=0,insecure,no_subtree_check,async) # to Read/Write : change ro to rw &amp; add no_root_squash (After async) # To open to the internet replace localhost to * ( Danger : Think about it ) $ mount -t nfs 127.0.0.1:/export /mnt $ exportfs -a $ service nfs-kernel-server restart # Run a container $ docker run -it --name nfs-client --privileged -v /mnt:/mnt busybox /bin/true # Mount on other container $ docker run -it --volumes-from nfs-client debian /bin/bash . . . 5.0 Networks in Docker . . 5.1 Day To Day Network Commands . docker network create | docker network rm | docker network ls | docker network inspect | docker network connect | docker network disconnect | . . 5.2 Examples . 5.2.1 To list all networks . $ docker network ls NETWORK ID NAME DRIVER SCOPE f32f6d51e8c8 bridge bridge local 366d3d1f4719 hacstac_default bridge local 6c08bddba2c4 host host local b726554d155b none null local . 5.2.2 To Create a New Network . $ docker network create --driver bridge --label project=dockerinaction --label chapter=5 --attachable --scope local --subnet 10.0.42.0/24 --ip-range 10.0.42.128/25 user-network $ docker run -it --network user-network --name network-explorer alpine:3.8 sh # CTRL-P + CTRL-Q Detech # docker attach network-explorer # walkthrough $ docker network create my_network [ Create a Network ] $ docker network connect my_network blog1 [ blog1 container connect to a network my_network ] $ docker run -it --network my_network ubuntu:16.04 bash [ Now this ubuntu container have access to blog1 Container ] $ ip -f inet -4 -o addr // this will list loopback and assign ip subnet address . 5.2.3 Create a another bridge network . $ docker network create --driver bridge --attachable --scope local --subnet 10.0.43.0/24 --ip-range 10.0.43.128/25 user-network2 # Attach priviously created container attach to this network $ docker network connect user-network2 network-explorer # then this container lists two ethernet addresses # scan with nmap $ nmap -sn 10.0.42.* -sn 10.0.43.* -oG /dev/stdout | grep Status . 5.2.4 With Network - none : Means container with no external excess . $ docker run --rm --network none alpine:3.8 ping -w 2 1.1.1.1 # it will try to ping 1.1.1.1 but it failed because this container hace no external network access # NodePort Publishing # 8080:8000 will denote 8080 port of host machine and 8000 port of container $ docker run -d -p 8080 --name listener alpine:3.8 $ docker port listener // To view port of running container . 5.2.5 DNS with docker . # Feature 1 : --hostname will add hostname : so we open with DN $ docker run --rm --hostname barker alpine:3.8 nslookup barker Server: 10.0.2.3 Address 1: 10.0.2.3 Name: barker Address 1: 172.17.0.22 barker - # Feature 2 : --dns : set dns server on container $ docker run --rm --dns 8.8.8.8 alpine:3.8 nslookup docker.com # Feature 3 : --dns-search : allows us to specify a DNS searchdomain, which is like a default hostname suffix $ docker run --rm --dns-search docker.com --dns 1.1.1.1 alpine:3.8 cat /etc/resolv.conf # Will display contents that look like: # search docker.com # nameserver 1.1.1.1 # Feature 4 : --add-host $ docker run --rm --hostname mycontainer --add-host docker.com:127.0.0.1 --add-host test:10.10.10.2 alpine:3.8 cat /etc/hosts 172.17.0.45 mycontainer 127.0.0.1 localhost ::1 localhost ip6-localhost ip6-loopbackfe00 ::0 ip6-localnetff00::0 ip6-mcastprefixff02 ::1 ip6-allnodesff02::2 ip6-allrouters 10.10.10.2 test 127.0.0.1 docker.com . 5.2.6 You can specify a specific IP address for a container . # create a new bridge network with your subnet and gateway for your ip block $ docker network create --subnet 203.0.113.0/24 --gateway 203.0.113.254 iptastic # run a nginx container with a specific ip in that block $ docker run --rm -it --net iptastic --ip 203.0.113.2 nginx # curl the ip from any other place (assuming this is a public ip block duh) $ curl 203.0.113.2 . 5.2.7 Open a Docker Daemon to the World . # first of stop the docker service $ sudo service docker stop / or $ sudo systemctl stop docker # Checks for docker daemon ?? $ ps -ef | grep -E &#39;docker(d| -d| daemon) b&#39; | grep -v grep # Expose to local host : 2375 $ sudo docker daemon -H tcp://0.0.0.0:2375 # To connect $ docker -H tcp://&lt;your host&#39;s ip&gt;:2375 &lt;subcommand&gt; . 5.2.8 Get an IP &amp; Ports of a Docker Container . $ alias dl=&#39;docker ps -l -q&#39; ( latest container ID ) $ docker inspect $(dl) | grep -wm1 IPAddress | cut -d &#39;&quot;&#39; -f 4 Pass ( ID of container Instead of dl ) : this above command gives ip of latest container # get ports $ docker inspect -f &#39; -&gt; &#39; 274d2292a137 | name_of_container . . . 6.0 Limiting Risk with Resource Controls . . 6.1 Memory Limits . $ docker container run -d --name ch6_mariadb --memory 256m --cpu-shares 1024 --cap-drop net_raw -e MYSQL_ROOT_PASSWORD=test mariadb:5.5 # This container only uses the 256M memory . . 6.2 CPU Limits . $ docker container run -d -P --name ch6_wordpress --memory 512m --cpu-shares 512 --cap-drop net_raw --link ch6_mariadb:mysql -e WORDPRESS_DB_PASSWORD=test wordpress:5.0.0-php7.2-apache # if total cpu share is 1536 - 512 ( 33% ) : this container consume 33% of cpu shares $ docker container run -d -P --name ch6_wordpress --memory 512m --cpus 0.75 --cap-drop net_raw --link ch6_mariadb:mysql -e WORDPRESS_DB_PASSWORD=test wordpress:5.0.0-php7.2-apache # This Container : consumed max 75% of cpu cores # also we can use {--cpuset-cpus 0-4} (cores) . . 6.3 Access to devices . $ docker container run -it --rm --device /dev/video0:/dev/video0 ubuntu:16.04 ls -al /dev # --device flag will mount external device to container . . 6.4 Sharing Memory ( IPC : Interprocess Communication ) . # Producer $ docker container run -d -u nobody --name ch6_ipc_producer --ipc shareable dockerinaction/ch6_ipc -producer # Consumer $ docker container run -d --name ch6_ipc_consumer --ipc container:ch6_ipc_producer dockerinaction/ch6_ipc -consumer # we can see the process of one container in another : by using docker logs { they share the memory space } # IMP NOTE : In docker to clean Volumes $ docker rm -vf name_of_container . . 6.5 Understanding Users . # if we want to setup a container with diff user then $ docker container run --rm --user nobody busybox:1.29 id $ docker container run --rm -u 1000:1000 busybox:1.29 /bin/bash -c &quot;echo This is important info &gt; /logFiles/important.log&quot; # with this userID:GroupID we can access the file system of this users . . 6.6 OS features Access with Capabilities . Linux capabilities can be set by using cap-add and cap-drop. See https://docs.docker.com/engine/reference/run/#/runtime-privilege-and-linux-capabilities for details. This should be used for greater security. . SYS_MODULE —Insert/remove kernel modules SYS_RAWIO — Modify kernel memory SYS_NICE — Modify priority of processes SYS_RESOURCE — Override resource limits SYS_TIME — Modify the system clock AUDIT_CONTROL — Configure audit subsystem MAC_ADMIN — Configure MAC configuration SYSLOG — Modify kernel print behavior NET_ADMIN — Configure the network SYS_ADMIN — Catchall for administrative functions $ docker container run --rm -u nobody --cap-add sys_admin ubuntu:16.04 /bin/bash -c &quot;capsh --print | grep sys_admin&quot; # this --cap-add sys_admin will add admin facilities to container # we can inspect docker with .HostConfig.CapAdd &amp;&amp; .HostConfig.CapDrop show capabilities # Give access to a single device: $ docker run -it --device=/dev/ttyUSB0 debian bash # Give access to all devices: $ docker run -it --privileged -v /dev/bus/usb:/dev/bus/usb debian bash # Docker Container with full privileges $ docker container run --rm --privileged ubuntu:16.04 capsh ls /dev # check out list of mounted devices . . 6.7 Additional Security with Docker . $ docker container run --rm -it --security-opt seccomp=path_to_the_secomp conf file ubuntu:16.04 sh # For Linux Security Modules ( LSM ) The LSM security option values are specified in one of seven formats: - To prevent a container from gaining new privileges after it starts, use &#39;no-new-privileges&#39; - To set a SELinux user label, use the form label=user:username, where is the name of the user you want to use for the label. - To set a SELinux role label, use the form label=role:role where is the name of the role you want to apply to processes in the container. - To set a SELinux type label, use the form label=type:type , where is the type name of the processes in the container. - To set a SELinux-level label, use the form &#39;label:level:label&#39; , where is the level at which processes in the container should run. Levels are specified as low-high pairs. Where abbreviated to the low level only, SELinux will inter-pret the range as single level. - To disable SELinux label confinement for a container, use the form label=disable # NOTE : Avoid Running Containers in privileged mode whenever possible . . 6.8 Using Socat to monitor docker api traffic . In this technique you’ll insert a proxy Unix domain socket between your request and the server’s socket to see what passes through it. Note that you’ll need root or sudo privileges to make this work. . # We need a socat ( Install socat as per the OS package Maneger ) $ socat -v UNIX-LISTEN:/tmp/dockerapi.sock,fork UNIX-CONNECT:/var/run/docker.sock &amp; In this command, -v makes the output readable, with indications of the flow of data.The UNIX-LISTEN part tells socat to listen on a Unix socket, fork ensures that socatdoesn’t exit after the first request, and UNIX-CONNECT tells socat to connect toDocker’s Unix socket. The &amp; specifies that the command runs in the background.If you usually run the Docker client with sudo, you’ll need to do the same thing here as well. # List all containers $ docker -H unix:///tmp/dockerapi.sock ps -a # This will show how client request to daemon . . 6.9 Setting TimeZone in Containers . # Runs a command to display the time zone on the host $ date +%Z // UTC # change TimeZone FROM centos:7 RUN rm -rf /etc/localtime RUN ln -s /usr/share/zoneinfo/Asia/Kolkata /etc/localtime CMD date +%Z # Build Image $ docker build -t timezone_change . $ docker run timezone_change Asia/Kolkata . . 6.10 Locale Management . locale will be set in the environment through the LANG,LANGUAGE, and locale-gen variables . # you are getting encoding error if correct locale is not set. # Check for locale $ env | grep LANG LANG=en_GB.UTF-8 This is British English, with text encoded in UTF-8. # Set Locale FROM ubuntu:16.04 RUN apt-get update &amp;&amp; apt-get install -y locales RUN locale-gen en_US.UTF-8 ENV LANG en_US.UTF-8 ENV LANGUAGE en_US:en CMD env $ $ docker build -t encoding . . . .",
            "url": "https://hacstac.github.io/Notes/docker/2020/08/29/Docker-Easy-Parts.html",
            "relUrl": "/docker/2020/08/29/Docker-Easy-Parts.html",
            "date": " • Aug 29, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://hacstac.github.io/Notes/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://hacstac.github.io/Notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}