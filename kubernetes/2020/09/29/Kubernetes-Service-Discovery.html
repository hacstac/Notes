<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Kubernetes Service Discovery | Notebook</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Kubernetes Service Discovery" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="DeepDive in Kubernetes Services" />
<meta property="og:description" content="DeepDive in Kubernetes Services" />
<link rel="canonical" href="https://hacstac.github.io/Notes/kubernetes/2020/09/29/Kubernetes-Service-Discovery.html" />
<meta property="og:url" content="https://hacstac.github.io/Notes/kubernetes/2020/09/29/Kubernetes-Service-Discovery.html" />
<meta property="og:site_name" content="Notebook" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-09-29T00:00:00-05:00" />
<script type="application/ld+json">
{"description":"DeepDive in Kubernetes Services","url":"https://hacstac.github.io/Notes/kubernetes/2020/09/29/Kubernetes-Service-Discovery.html","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"https://hacstac.github.io/Notes/kubernetes/2020/09/29/Kubernetes-Service-Discovery.html"},"headline":"Kubernetes Service Discovery","dateModified":"2020-09-29T00:00:00-05:00","datePublished":"2020-09-29T00:00:00-05:00","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/Notes/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://hacstac.github.io/Notes/feed.xml" title="Notebook" /><link rel="shortcut icon" type="image/x-icon" href="/Notes/images/favicon.ico"><link href="https://unpkg.com/@primer/css/dist/primer.css" rel="stylesheet" />
<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.0.7/css/all.css">

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/Notes/">Notebook</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/Notes/about/">About Me</a><a class="page-link" href="/Notes/search/">Search</a><a class="page-link" href="/Notes/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Kubernetes Service Discovery</h1><p class="page-description">DeepDive in Kubernetes Services</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-09-29T00:00:00-05:00" itemprop="datePublished">
        Sep 29, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      24 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/Notes/categories/#Kubernetes">Kubernetes</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul class="section-nav">
<li class="toc-entry toc-h2"><a href="#11-what-is-service-discovery">1.1 What is Service Discovery</a></li>
<li class="toc-entry toc-h2"><a href="#12-why-we-need-service-discovery">1.2 Why we need service discovery</a></li>
<li class="toc-entry toc-h2"><a href="#13--the-service-object">1.3  The Service Object</a>
<ul>
<li class="toc-entry toc-h3"><a href="#131-testing-your-service-from-within-the-cluster">1.3.1 Testing your service from within the cluster</a>
<ul>
<li class="toc-entry toc-h4"><a href="#remotely-executing-commands-in-running-containers">Remotely executing commands in running containers</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#132-configuring-session-affinity-on-the-service">1.3.2 Configuring session affinity on the service</a></li>
<li class="toc-entry toc-h3"><a href="#133-exposing-multiple-ports-in-the-same-service">1.3.3 Exposing multiple ports in the same service</a></li>
<li class="toc-entry toc-h3"><a href="#134-discovering-services">1.3.4 Discovering services</a>
<ul>
<li class="toc-entry toc-h4"><a href="#method-1-discovering-services-through-env-variables">Method 1: Discovering services through env variables</a></li>
<li class="toc-entry toc-h4"><a href="#method-2-discovering-services-through-dns">Method 2: Discovering services through DNS</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#135-connect-to-services-from-outside-the-cluster">1.3.5 Connect to services from outside the cluster</a>
<ul>
<li class="toc-entry toc-h4"><a href="#creating-the-service-endpoints">Creating the service endpoints</a></li>
<li class="toc-entry toc-h4"><a href="#creating-an-alias-for-an-external-service">Creating an alias for an external service</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#136-exposing-services-to-external-clients">1.3.6 Exposing services to external clients</a>
<ul>
<li class="toc-entry toc-h4"><a href="#1361-nodeport">1.3.6.1 NodePort</a></li>
<li class="toc-entry toc-h4"><a href="#1362-external-loadbalancer">1.3.6.2 External LoadBalancer</a></li>
<li class="toc-entry toc-h4"><a href="#1363--ingress">1.3.6.3  Ingress</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#137-signaling-when-a-pod-is-ready-to-accept-connections">1.3.7 Signaling when a pod is ready to accept connections</a>
<ul>
<li class="toc-entry toc-h4"><a href="#readiness-probes">Readiness probes</a></li>
</ul>
</li>
<li class="toc-entry toc-h3"><a href="#138-using-a-headless-service-for-discovering-individual-pods">1.3.8 Using a headless service for discovering individual pods</a></li>
<li class="toc-entry toc-h3"><a href="#139-troubleshooting-best-practices">1.3.9 Troubleshooting Best Practices</a></li>
</ul>
</li>
</ul><hr>

<h2 id="11-what-is-service-discovery">
<a class="anchor" href="#11-what-is-service-discovery" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.1 What is Service Discovery</h2>

<p>A Kubernetes Service is a resource you create to make a single, constant point of entry to a group of pods providing the same service. Each service has an IP address and port that never change while the service exists. Clients can open connections to that IP and port, and those connections are then routed to one of the pods backing that service. This way, clients of a service don’t need to know the location of individual pods providing the service, allowing those pods to be moved around the cluster at any time.</p>

<p>Service-discovery is tool that help to solve the problem of finding which processes are listening at which addresses for which services. A good service-discovery system will enable users to resolve this information quickly and reliably. A good system is also low-latency; clients are updated soon after the information associated with a service changes.</p>

<p>If we understand this with an example: Where you have a frontend web server and a backend data base server. There may be multiple pods that all act as the frontend, but there may only be a single backend database pod. You need to solve two problems to make the system function:</p>

<ul>
  <li>External clients need to connect to the frontend pods without caring if there’s
only a single web server or hundreds.</li>
  <li>The frontend pods need to connect to the backend database. Because the database runs inside a pod, it may be moved around the cluster over time, causing its IP address to change. You don’t want to reconfigure the frontend pods every time the backend database is moved.</li>
</ul>

<p>By creating a service for the frontend pods and configuring it to be accessible from outside the cluster, you expose a single, constant IP address through which external clients can connect to the pods. Similarly, by also creating a service for the backend pod, you create a stable address for the backend pod. The service address doesn’t change even if the pod’s IP address changes. Additionally, by creating the service, you also enable the frontend pods to easily find the backend service by its name through either environment variables or DNS.</p>

<p><img src="https://s3.ap-south-1.amazonaws.com/akash.r/Devops_Notes_screenshots/Kubernetes/Service_Discovery.png" alt="Service Discovery"></p>

<hr>

<h2 id="12-why-we-need-service-discovery">
<a class="anchor" href="#12-why-we-need-service-discovery" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.2 Why we need service discovery</h2>

<p>To solve the problem of multi node networking, where each pods share same private network and all those pods should be accessible through a single IP address. To access these pods outside a private network, we need a Kubernetes Services.</p>

<p>If we consider a scenario, where in non kubernetes env where a sysadmin would configure each client app by specifying the exact IP address or hostname of the server providing the service in the client’s configuration files, doing the same in Kubernetes wouldn’t work, because:</p>

<ul>
  <li>Pods are ephemeral—They may come and go at any time, whether it’s because a pod is removed from a node to make room for other pods, because someone scaled down the number of pods, or because a cluster node has failed.</li>
  <li>Kubernetes assigns an IP address to a pod after the pod has been scheduled to a node and before it’s started —Clients thus can’t know the IP address of the server pod up front.</li>
  <li>Horizontal scaling means multiple pods may provide the same service—Each of those pods has its own IP address. Clients shouldn’t care how many pods are backing the service and what their IPs are. They shouldn’t have to keep a list of all the individual IPs of pods.</li>
</ul>

<p>That’s why we need Kubernetes Services.</p>

<hr>

<h2 id="13--the-service-object">
<a class="anchor" href="#13--the-service-object" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3  The Service Object</h2>

<p>Real service discovery in Kubernetes starts with a Service object. A Service object is a way to create a named label selector. As we will see, the Service object does some other nice things for us, too.</p>

<p>Just as the kubectl run command is an easy way to create a Kubernetes deployment,
we can use kubectl expose to create a service. Let’s create some deployments and
services so we can see how they work:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">## 1.0 Create a Deployment</span>

<span class="c"># Boot the deployment directly by the kubectl command like ( given belowe ) or use a YAML file to setup a deployment.</span>
<span class="c"># Boot up the Deployment</span>
kubectl run cyberchef <span class="nt">--image</span><span class="o">=</span>mpepping/cyberchef <span class="nt">--replicas</span><span class="o">=</span>3 <span class="nt">--port</span><span class="o">=</span>8000 <span class="nt">--labels</span><span class="o">=</span><span class="s2">"ver=1, env=dev"</span>

<span class="nt">--------------------------------------------------------------------------------------</span>
<span class="c">## 2.0 Expose a Deployment</span>

<span class="c"># Creating the service using the YAML</span>

<span class="nt">---cyberchef_service</span>.yml
apiVersion: v1
kind: Service
metadata:
  name: cyberchef
spec:
  ports:
  - port: 80 <span class="c"># Host Port</span>
    targetPort: 8000 <span class="c"># Container Port</span>
  selector:
    app: cyberchef

<span class="nv">$ </span>kubectl create <span class="nt">-f</span> ./cyberchef_service.yml

<span class="nt">--------------------------------------------------------------------------------------</span>
<span class="c">## 3.0 Get the Services</span>

<span class="c"># Get the clusterIP using getting the service</span>
<span class="nv">$ </span>kubectl get svc <span class="nt">-o</span> wide
NAME         CLUSTER-IP         PORT<span class="o">(</span>S<span class="o">)</span>           SELECTOR
cyberchef    10.115.242.13      80/TCP            <span class="nv">ver</span><span class="o">=</span>1, <span class="nb">env</span><span class="o">=</span>dev
</code></pre></div></div>

<p>The list shows that the IP address assigned to the service is 10.115.242.13. Because this is the cluster IP, it’s only accessible from inside the cluster. The primary purpose of services is exposing groups of pods to other pods in the cluster, but you’ll usually also want to expose services externally.</p>

<h3 id="131-testing-your-service-from-within-the-cluster">
<a class="anchor" href="#131-testing-your-service-from-within-the-cluster" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3.1 Testing your service from within the cluster</h3>

<p>You can send requests to your service from within the cluster in a few ways:</p>

<ul>
  <li>The obvious way is to create a pod that will send the request to the service’s cluster IP and log the response. You can then examine the pod’s log to see what the service’s response was.</li>
  <li>You can ssh into one of the Kubernetes nodes and use the curl command.</li>
  <li>You can execute the curl command inside one of your existing pods through the kubectl exec command.</li>
</ul>

<h4 id="remotely-executing-commands-in-running-containers">
<a class="anchor" href="#remotely-executing-commands-in-running-containers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Remotely executing commands in running containers</h4>

<p>The kubectl exec command allows you to remotely run arbitrary commands inside an existing container of a pod. This comes in handy when you want to examine the contents, state, and/or environment of a container. List the pods with the kubectl get pods command and choose one as your target for the exec command.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">$ </span>kubectl <span class="nb">exec </span>cyberchef-d987s <span class="nt">--</span> curl <span class="nt">-s</span> http://10.115.242.13
&lt;<span class="o">!</span><span class="nt">--</span>
    CyberChef - The Cyber Swiss Army Knife
.......
.......
<span class="nt">--</span><span class="o">&gt;</span>
&lt; Full Front Page HTML <span class="o">&gt;</span>
</code></pre></div></div>

<h3 id="132-configuring-session-affinity-on-the-service">
<a class="anchor" href="#132-configuring-session-affinity-on-the-service" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3.2 Configuring session affinity on the service</h3>

<p>If you execute the same command a few more times, you should hit a different pod with every invocation, because the service proxy normally forwards each connection to a randomly selected backing pod, even if the connections are coming from the same client. If, on the other hand, you want all requests made by a certain client to be redirected to the same pod every time, you can set the service’s sessionAffinity property to ClientIP</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">sessionAffinity</span><span class="pi">:</span> <span class="s">ClientIP</span>
  <span class="s">...</span>
</code></pre></div></div>

<p>This makes the service proxy redirect all requests originating from the same client IP to the same pod. As an exercise, you can create an additional service with session affinity set to ClientIP and try sending requests to it.</p>

<h3 id="133-exposing-multiple-ports-in-the-same-service">
<a class="anchor" href="#133-exposing-multiple-ports-in-the-same-service" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3.3 Exposing multiple ports in the same service</h3>

<p>Your service exposes only a single port, but services can also support multiple ports. For example, if your pods listened on two ports—let’s say 8080 for HTTP and 8443 for HTTPS—you could use a single service to forward both port 80 and 443 to the pod’s ports 8080 and 8443. You don’t need to create two different services in such cases. Using a single, multi-port service exposes all the service’s ports through a single cluster IP.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cyberchef</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">http</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8080</span>
  <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">https</span>
    <span class="na">port</span><span class="pi">:</span> <span class="m">443</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8443</span>
  <span class="na">selector</span><span class="pi">:</span>
    <span class="na">app</span><span class="pi">:</span> <span class="s">cyberchef</span>
</code></pre></div></div>

<h3 id="134-discovering-services">
<a class="anchor" href="#134-discovering-services" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3.4 Discovering services</h3>

<h4 id="method-1-discovering-services-through-env-variables">
<a class="anchor" href="#method-1-discovering-services-through-env-variables" aria-hidden="true"><span class="octicon octicon-link"></span></a>Method 1: Discovering services through env variables</h4>

<p>When a pod is started, Kubernetes initializes a set of environment variables pointing to each service that exists at that moment. If you create the service before creating the client pods, processes in those pods can get the IP address and port of the service by inspecting their environment variables.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># To check out the env variables</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>cyberchef-d978l2 <span class="nb">env
</span><span class="nv">PATH</span><span class="o">=</span>/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
<span class="nv">HOSTNAME</span><span class="o">=</span>cyberchef-d978l2
<span class="nv">KUBERNETES_SERVICE_HOST</span><span class="o">=</span>10.115.115.10
<span class="nv">KUBERNETES_SERVICE_PORT</span><span class="o">=</span>80
</code></pre></div></div>

<p>Environment variables are one way of looking up the IP and port of a service, but isn’t this usually the domain of DNS. Let look at the DNS way</p>

<h4 id="method-2-discovering-services-through-dns">
<a class="anchor" href="#method-2-discovering-services-through-dns" aria-hidden="true"><span class="octicon octicon-link"></span></a>Method 2: Discovering services through DNS</h4>

<p>In the kubernetes env, One of the pod called as kube-dns. The kube-system namespace also includes a corresponding service with the same name. As the name suggests, the pod runs a DNS server, which all other pods running in the cluster are automatically configured to use (Kubernetes does that by modifying each container’s /etc/resolv.conf file). Any DNS query performed by a process running in a pod will be handled by Kubernetes’ own DNS server, which knows all the services running in your system.</p>

<p>Each service gets a DNS entry in the internal DNS server, and client pods that know the name of the service can access it through its fully qualified domain name (FQDN) instead of resorting to environment variables. Now we connect to the service through its FQDN</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Ex frontend.default.svc.cluster.local</span>

<span class="c"># frontend = Service Name</span>
<span class="c"># default = NameSpace</span>
<span class="c"># svc.cluster.local = Configurable cluster domain suffix</span>

<span class="nv">$ </span>kubectl <span class="nb">exec </span>it cyberchef-973l2 bash
-&gt;<span class="nv">$ </span>curl http://cyberchef.default.svc.cluster.local
</code></pre></div></div>

<h3 id="135-connect-to-services-from-outside-the-cluster">
<a class="anchor" href="#135-connect-to-services-from-outside-the-cluster" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3.5 Connect to services from outside the cluster</h3>

<p>In most cases we need apps will exposed to external services through the Kubernetes services feature. Instead of having the service redirect connections to pods in the cluster, you want it to redirect to external IP(s) and port(s). This allows you to take advantage of both service load balancing and service discovery. Client pods running in the cluster can connect to the external service like they connect to internal services.</p>

<ul>
  <li>Service Endpoints: An Endpoints resource (yes, plural) is a list of IP addresses and ports exposing a service. The Endpoints resource is like any other Kubernetes resource, so you can display its basic info with <code class="language-plaintext highlighter-rouge">kubectl get endpoints</code>:</li>
</ul>

<h4 id="creating-the-service-endpoints">
<a class="anchor" href="#creating-the-service-endpoints" aria-hidden="true"><span class="octicon octicon-link"></span></a>Creating the service endpoints</h4>

<p>If you create a service without a pod selector, Kubernetes won’t even create the
Endpoints resource (after all, without a selector, it can’t know which pods to include
in the service). It’s up to you to create the Endpoints resource to specify the list of endpoints for the service. To create a service with manually managed endpoints, you need to create both a Service and an Endpoints resource.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Creating an endpoints resource for a service without a selector</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Endpoints</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cyberchef-service</span>
<span class="na">subsets</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">addresses</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">ip</span><span class="pi">:</span> <span class="s">11.11.11.11</span> <span class="c1"># Ip of endpoint that the service will forward connections to</span>
    <span class="pi">-</span> <span class="na">ip</span><span class="pi">:</span> <span class="s">22.22.22.22</span>
    <span class="na">ports</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div>

<p>The Endpoints object needs to have the same name as the service and contain the list
of target IP addresses and ports for the service. After both the Service and the Endpoints resource are posted to the server, the service is ready to be used like any regular service with a pod selector. Containers created after the service is created will include the environment variables for the service, and all connections to its IP:port pair will be load balanced between the service’s endpoints.</p>

<h4 id="creating-an-alias-for-an-external-service">
<a class="anchor" href="#creating-an-alias-for-an-external-service" aria-hidden="true"><span class="octicon octicon-link"></span></a>Creating an alias for an external service</h4>

<p>Instead of exposing an external service by manually configuring the service’s End-
points, a simpler method allows you to refer to an external service by its fully qualified domain name (FQDN).</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cyberchef-service</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">type</span><span class="pi">:</span> <span class="s">ExternalName</span>
  <span class="na">externalName</span><span class="pi">:</span> <span class="s">someapi.somecompany.com</span> <span class="c1"># API Endpoint</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div>

<p>After the service is created, pods can connect to the external service through the cyberchef-service.default.svc.cluster.local domain name (or even external- service) instead of using the service’s actual FQDN. This hides the actual service name and its location from pods consuming the service, allowing you to modify the service definition and point it to a different service any time later, by only changing the externalName attribute or by changing the type back to ClusterIP and creating an Endpoints object for the service—either manually or by specifying a label selector on the service and having it created automatically.</p>

<h3 id="136-exposing-services-to-external-clients">
<a class="anchor" href="#136-exposing-services-to-external-clients" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3.6 Exposing services to external clients</h3>

<p>We need apps like web server to the outside, so external can access them.</p>

<p>You have a few ways to make a service accessible externally:</p>

<ul>
  <li>
    <p>Setting the service type to <strong>NodePort</strong>—For a NodePort service, each cluster node
opens a port on the node itself (hence the name) and redirects traffic received
on that port to the underlying service. The service isn’t accessible only at the
internal cluster IP and port, but also through a dedicated port on all nodes.</p>
  </li>
  <li>
    <p>Setting the service type to <strong>LoadBalancer</strong>, an extension of the NodePort type— This makes the service accessible through a dedicated load balancer, provisioned
from the cloud infrastructure Kubernetes is running on. The load balancer redirects traffic to the node port across all the nodes. Clients connect to the service
through the load balancer’s IP.</p>
  </li>
  <li>
    <p>Creating an <strong>Ingress</strong> resource, a radically different mechanism for exposing multiple services through a single IP address— It operates at the HTTP level (network layer 7) and can thus offer more features than layer 4 services.</p>
  </li>
</ul>

<h4 id="1361-nodeport">
<a class="anchor" href="#1361-nodeport" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3.6.1 NodePort</h4>

<p>Creating a NodePort service, you make Kubernetes reserve a port on all its nodes (the same port number is used across all of them) and forward incoming connections to the pods that are part of the service. This is similar to a regular service (their actual type is ClusterIP), but a NodePort service can be accessed not only through the service’s internal cluster IP, but also through any node’s IP and the reserved node port.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">---cyberchef_nodeport</span>.yml
apiVersion: v1
kind: Service
metadata:
  name: cyberchef-nodeport
spec:
  <span class="nb">type</span>: NodePort
  ports:
  - port: 80
    targetPort: 8000
    nodePort: 30123
  selector:
  app: cyberchef

<span class="nt">--or--</span>

<span class="c"># Directly using the kubectl</span>
<span class="nv">$ </span>kubectl expose deployment cyberchef <span class="nt">--type</span><span class="o">=</span>NodePort <span class="nt">--port</span><span class="o">=</span>80 <span class="nt">--targetPort</span><span class="o">=</span>8000

<span class="c"># Examining a NodePort service</span>
<span class="nv">$ </span>kubectl get svc cyberchef
NAME             CLUSTER-IP       EXTERNAL-IP    PORT<span class="o">(</span>S<span class="o">)</span>        AGE
cyberchef        10.111.254.223   &lt;nodes&gt;        80:30123/TCP   2m
</code></pre></div></div>

<p>Look at the EXTERNAL-IP column. It shows <code class="language-plaintext highlighter-rouge">nodes</code>, indicating the service is accessible
through the IP address of any cluster node. The PORT(S) column shows both the internal port of the cluster IP (80) and the node port (30123).</p>

<p>When an external client connects to a service through the node port (this also includes cases when it goes through the load balancer first), the randomly chosen pod may or may not be running on the same node that received the connection. An additional network hop is required to reach the pod, but this may not always be desirable. You can prevent this additional hop by configuring the service to redirect external traffic only to pods running on the node that received the connection. This is done by setting the externalTrafficPolicy field in the service’s spec section:</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">spec</span><span class="pi">:</span>
  <span class="na">externalTrafficPolicy</span><span class="pi">:</span> <span class="s">Local</span>
</code></pre></div></div>

<p>If a service definition includes this setting and an external connection is opened
through the service’s node port, the service proxy will choose a locally running pod. If no local pods exist, the connection will hang (it won’t be forwarded to a random global pod, the way connections are when not using the annotation). You therefore need to ensure the load balancer forwards connections only to nodes that have at least one such pod.</p>

<h4 id="1362-external-loadbalancer">
<a class="anchor" href="#1362-external-loadbalancer" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3.6.2 External LoadBalancer</h4>

<p>Finally, if you have support from the cloud that you are running on (and your cluster is configured to take advantage of it), you can use the LoadBalancer type. This builds on the NodePort type by additionally configuring the cloud to create a new load balancer and direct it at nodes in your cluster. Edit the cyberchef service again (kubectl edit service cyberchef) and change spec.type to LoadBalancer.</p>

<p>If you do a kubectl get services right away you’ll see that the EXTERNAL-IP column for cyberchef now says <code class="language-plaintext highlighter-rouge">pending</code>. Wait a bit and you should see a public address assigned by your cloud. You can look in the console for your cloud account and see the configuration work that Kubernetes did for you.</p>

<p>If Kubernetes is running in an environment that doesn’t support LoadBalancer
services, the load balancer will not be provisioned, but the service will still behave like a NodePort service. That’s because a LoadBalancer service is an extension of a NodePort service.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">---cyberchef_loadbalancer</span>.yml
apiVersion: v1
kind: Service
metadata:
  name: cyberchef
spec:
  <span class="nb">type</span>: LoadBalancer
  ports:
  - port: 80
    targetPort: 8000
    selector:
      app: cyberchef

<span class="c"># Connecting through a load balancer</span>
<span class="nv">$ </span>kubectl get svc cyberchef
NAME             CLUSTER-IP       EXTERNAL-IP     PORT<span class="o">(</span>S<span class="o">)</span>        AGE
cyberchef        10.111.254.223   210.110.15.4    80:30123/TCP   2m
<span class="c"># Access at http://210.110.15.4</span>
</code></pre></div></div>

<h4 id="1363--ingress">
<a class="anchor" href="#1363--ingress" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3.6.3  Ingress</h4>

<p>One important reason is that each LoadBalancer service requires its own load balancer with its own public IP address, whereas an Ingress only requires one, even when providing access to dozens of services. When a client sends an HTTP request to the Ingress, the host and path in the request determine which service the request is forwarded to Ingresses operate at the application layer of the network stack (HTTP) and can provide features such as cookie-based session affinity and the like, which services can’t.</p>

<p>To make ingress work, you need to install ingress as per your kubernetes enviornment.</p>

<div class="language-yml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cyberchef</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">cyberchef.example.com</span>
    <span class="na">http</span><span class="pi">:</span>
      <span class="na">paths</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
        <span class="na">backend</span><span class="pi">:</span>
        <span class="na">serviceName</span><span class="pi">:</span> <span class="s">cyberchef-nodeport</span>
        <span class="na">servicePort</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div>

<p>This defines an Ingress with a single rule, which makes sure all HTTP requests received by the Ingress controller, in which the host cyberchef.example.com is requested, will be sent to the cyberchef-nodeport service on port 80.</p>

<p>To access your service through <code class="language-plaintext highlighter-rouge">http://cyberchef.example.com</code>, you’ll need to make sure the domain name resolves to the IP of the Ingress controller.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># List Ingress</span>
<span class="nv">$ </span>kubectl get ingresses
NAME       HOSTS                  ADDRESS         PORTS   AGE
cyberchef  cyberchef.example.com  192.168.99.100  80      29m

<span class="c"># Note: Make sure you need to add this to your hosts file ( /etc/hosts )</span>
192.168.99.100  cyberchef.example.com
</code></pre></div></div>

<ul>
  <li>
    <p>How ingress works??
The client first performed a DNS lookup of cyberchef.example.com, and the DNS server (or the local operating system) returned the IP of the Ingress controller. The client then sent an HTTP request to the Ingress controller and specified cyberchef.example.com in the Host header. From that header, the controller determined which service the client is trying to access, looked up the pod IPs through the Endpoints object associated with the service, and forwarded the client’s request to one of the pods.</p>
  </li>
  <li>
    <p>Exposing multiple services through the same ingress</p>
  </li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">apps.example.com</span>
  <span class="na">http</span><span class="pi">:</span>
    <span class="na">paths</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/cyberchef</span>
      <span class="na">backend</span><span class="pi">:</span>
        <span class="na">serviceName</span><span class="pi">:</span> <span class="s">cyberchef</span>
        <span class="na">servicePort</span><span class="pi">:</span> <span class="m">80</span>
    <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/linkding</span>
      <span class="na">backend</span><span class="pi">:</span>
        <span class="na">serviceName</span><span class="pi">:</span> <span class="s">linkding</span>
        <span class="na">servicePort</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div>

<ul>
  <li>Exposing diff services to diff hosts</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nn">---</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">cyberchef.example.com</span>
    <span class="na">http</span><span class="pi">:</span>
     <span class="na">paths</span><span class="pi">:</span>
     <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
       <span class="na">backend</span><span class="pi">:</span>
         <span class="na">serviceName</span><span class="pi">:</span> <span class="s">cyberchef</span>
         <span class="na">servicePort</span><span class="pi">:</span> <span class="m">80</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">linkding.example.com</span>
    <span class="na">http</span><span class="pi">:</span>
      <span class="na">paths</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
        <span class="na">backend</span><span class="pi">:</span>
          <span class="na">serviceName</span><span class="pi">:</span> <span class="s">linkding</span>
          <span class="na">servicePort</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div>

<ul>
  <li>Configuring Ingress to handle TLS traffic ( for HTTPS )</li>
</ul>

<p>When a client opens a TLS connection to an Ingress controller, the controller terminates the TLS connection. The communication between the client and the controller is encrypted, whereas the communication between the controller and the backend pod isn’t. The application running in the pod doesn’t need to support TLS. For example, if the pod runs a web server, it can accept only HTTP traffic and let the Ingress controller take care of everything related to TLS. To enable the controller to do that, you need to attach a certificate and a private key to the Ingress. The two need to be stored in a Kubernetes resource called a Secret, which is then referenced in the Ingress manifest.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>openssl genrsa <span class="nt">--out</span> tls.key 2048
openssl req <span class="nt">-new</span> <span class="nt">-x509</span> tls.key <span class="nt">-out</span> tls.cert <span class="nt">-days</span> 360 <span class="nt">-subj</span> /CN<span class="o">=</span>cyberchef.example.com

kubectl create secret tls tls-secret <span class="nt">--cert</span><span class="o">=</span>tls.cert <span class="nt">--key</span><span class="o">=</span>tls.key
</code></pre></div></div>

<p>The private key and the certificate are now stored in the Secret called tls-secret. Now, you can update your Ingress object so it will also accept HTTPS requests for cyberchef.example.com.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">extensions/v1beta1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Ingress</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cyberchef</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">tls</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">hosts</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">cyberchef.example.com</span>
    <span class="na">secretName</span><span class="pi">:</span> <span class="s">tls-secret</span>
  <span class="na">rules</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">host</span><span class="pi">:</span> <span class="s">cyberchef.example.com</span>
    <span class="na">http</span><span class="pi">:</span>
      <span class="na">paths</span><span class="pi">:</span>
      <span class="pi">-</span> <span class="na">path</span><span class="pi">:</span> <span class="s">/</span>
        <span class="na">backend</span><span class="pi">:</span>
          <span class="na">serviceName</span><span class="pi">:</span> <span class="s">cyberchef-nodeport</span>
          <span class="na">servicePort</span><span class="pi">:</span> <span class="m">80</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Create Ingress</span>
<span class="nv">$ </span>kubectl apply <span class="nt">-f</span> cyberchef-ingress-tls.yaml

<span class="nv">$ </span>curl <span class="nt">-k</span> <span class="nt">-v</span> https://cyberchef.example.com/cyberchef
<span class="k">*</span> About to connect<span class="o">()</span> to cyberchef.example.com port 443 <span class="o">(</span><span class="c">#0)</span>
...
<span class="k">*</span> Server certificate:
<span class="k">*</span>
 subject: <span class="nv">CN</span><span class="o">=</span>cyberchef.example.com
...
<span class="o">&gt;</span> GET /cyberchef HTTP/1.1
<span class="o">&gt;</span> ...
</code></pre></div></div>

<h3 id="137-signaling-when-a-pod-is-ready-to-accept-connections">
<a class="anchor" href="#137-signaling-when-a-pod-is-ready-to-accept-connections" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3.7 Signaling when a pod is ready to accept connections</h3>

<p>Consider a scenario where a new pod started with proper labels is created, it becomes part of the service and requests start to be redirected to the pod. But what if the pod isn’t ready to start serving requests immediately?</p>

<p>The pod may need time to load either configuration or data, or it may need to perform a warm-up procedure to prevent the first user request from taking too long and affecting the user experience. In such cases you don’t want the pod to start receiving requests immediately, especially when the already-running instances can process requests properly and quickly. It makes sense to not forward requests to a pod that’s in the process of starting up until it’s fully ready.</p>

<h4 id="readiness-probes">
<a class="anchor" href="#readiness-probes" aria-hidden="true"><span class="octicon octicon-link"></span></a>Readiness probes</h4>

<p>The readiness probe is invoked periodically and determines whether the specific pod should receive client requests or not. When a container’s readiness probe returns success, it’s signaling that the container is ready to accept requests.</p>

<ul>
  <li>
    <p>How readiness probes works
When a container is started, Kubernetes can be configured to wait for a configurable amount of time to pass before performing the first readiness check. After that, it invokes the probe periodically and acts based on the result of the readiness probe. If a pod reports that it’s not ready,</p>
  </li>
  <li>TYPES OF READINESS PROBES
    <ul>
      <li>An Exec probe, where a process is executed. The container’s status is determined by the process’ exit status code.</li>
      <li>An HTTP GET probe, which sends an HTTP GET request to the container and the HTTP status code of the response determines whether the container is ready or not.</li>
      <li>A TCP Socket probe, which opens a TCP connection to a specified port of the
container. If the connection is established, the container is considered ready.</li>
    </ul>
  </li>
  <li>
    <p>Difference between liveness probe and rediness probe
Liveness probes keep pods healthy by killing off unhealthy containers and replacing
them with new, healthy ones, whereas readiness probes make sure that only pods that
are ready to serve requests receive them.</p>
  </li>
  <li>Why we need readiness probe
Imagine that a group of pods (for example, pods running application servers) depends on a service provided by another pod (a backend database, for example). If at any point one of the frontend pods experiences connectivity problems and can’t reach the database anymore, it may be wise for its readiness probe to signal to Kubernetes that the pod isn’t ready to serve any requests at that time. If other pod instances aren’t experiencing the same type of connectivity issues, they can serve requests normally. A readiness probe makes sure clients only talk to those healthy pods and never notice there’s anything wrong with the system.</li>
</ul>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">ReplicationController</span>
<span class="nn">...</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="s">...</span>
  <span class="s">template</span><span class="pi">:</span>
    <span class="s">...</span>
  <span class="na">spec</span><span class="pi">:</span>
    <span class="na">containers</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="na">name</span><span class="pi">:</span> <span class="s">cyberchef</span>
      <span class="na">image</span><span class="pi">:</span> <span class="s">mpepping/cyberchef</span>
      <span class="na">readinessProbe</span><span class="pi">:</span>
        <span class="na">exec</span><span class="pi">:</span>
          <span class="na">command</span><span class="pi">:</span>
          <span class="pi">-</span> <span class="s">ls</span>
          <span class="pi">-</span> <span class="s">/var/ready</span>
</code></pre></div></div>

<p>The readiness probe will periodically perform the command ls /var/ready inside the container. The ls command returns exit code zero if the file exists, or a non-zero exit code otherwise. If the file exists, the readiness probe will succeed; otherwise, it will fail. Note: To check status check logs.</p>

<p>This mock readiness probe is useful only for demonstrating what readiness probes do.
In the real world, the readiness probe should return success or failure depending on whether the app can (and wants to) receive client requests or not. Manually removing pods from services should be performed by either deleting the pod or changing the pod’s labels instead of manually flipping a switch in the probe.</p>

<h3 id="138-using-a-headless-service-for-discovering-individual-pods">
<a class="anchor" href="#138-using-a-headless-service-for-discovering-individual-pods" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3.8 Using a headless service for discovering individual pods</h3>

<p>If you want to connect to all of the pods. you need a IP of an individual pod, which is can’t possible in default configuration in kubernetes.  One option is to have the client call the Kubernetes API server and get the list of pods and their IP addresses through an API call, but because you should always strive to keep your apps Kubernetes-agnostic, using the API server isn’t ideal.</p>

<p>Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups. Usually, when you perform a DNS lookup for a service, the DNS server returns a single IP—the service’s cluster IP. But if you tell Kubernetes you don’t need a cluster IP for your service (you do this by setting the clusterIP field to None in the service specification ), the DNS server will return the pod IPs instead of the single service IP.</p>

<p>Instead of returning a single DNS A record, the DNS server will return multiple A records for the service, each pointing to the IP of an individual pod backing the ser- vice at that moment. Clients can therefore do a simple DNS A record lookup and get the IPs of all the pods that are part of the service. The client can then use that information to connect to one, many, or all of them.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># All you need to do is to set ClusterIP to None</span>
<span class="na">apiVersion</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">kind</span><span class="pi">:</span> <span class="s">Service</span>
<span class="na">metadata</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">cyberchef-headless</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">clusterIP</span><span class="pi">:</span> <span class="s">None</span>
  <span class="na">ports</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="na">port</span><span class="pi">:</span> <span class="m">80</span>
    <span class="na">targetPort</span><span class="pi">:</span> <span class="m">8000</span>
<span class="na">selector</span><span class="pi">:</span>
  <span class="na">app</span><span class="pi">:</span> <span class="s">cyberchef</span>
</code></pre></div></div>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Setup and dns lookup container ( We need nslookup and dif : tutum/dnsutils )</span>

<span class="nv">$ </span>kubectl run dnsutils <span class="nt">--image</span><span class="o">=</span>tutum/dnsutils <span class="nt">--generator</span><span class="o">=</span>run-pod/v1 <span class="nt">--command</span> <span class="nt">--</span> <span class="nb">sleep </span>infinity

<span class="c"># Perform DNSLookup for headless</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>dnsutils nslookup cyberchef-headless
...
Name: cyberchef-headless.default.svc.cluster.local
Address: 10.108.1.4

Name: cyberchef-headless.default.svc.cluster.local
Address: 10.108.2.5 <span class="c"># Pod IP</span>

<span class="c"># Perform DNSLookup for non-headless</span>
<span class="nv">$ </span>kubectl <span class="nb">exec </span>dnsutils nslookup cyberchef
...
Name: cyberchef.default.svc.cluster.local
Address: 10.111.249.153 <span class="c">## ClusterIP</span>
</code></pre></div></div>

<h3 id="139-troubleshooting-best-practices">
<a class="anchor" href="#139-troubleshooting-best-practices" aria-hidden="true"><span class="octicon octicon-link"></span></a>1.3.9 Troubleshooting Best Practices</h3>

<ul>
  <li>First, make sure you’re connecting to the service’s cluster IP from within the cluster, not from the outside.</li>
  <li>Don’t bother pinging the service IP to figure out if the service is accessible (remember, the service’s cluster IP is a virtual IP and pinging it will never work).</li>
  <li>If you’ve defined a readiness probe, make sure it’s succeeding; otherwise the pod won’t be part of the service.</li>
  <li>To confirm that a pod is part of the service, examine the corresponding Endpoints object with kubectl get endpoints.</li>
  <li>If you’re trying to access the service through its FQDN or a part of it (for example, myservice.mynamespace.svc.cluster.local or myservice.mynamespace) and it doesn’t work, see if you can access it using its cluster IP instead of the FQDN.</li>
  <li>Check whether you’re connecting to the port exposed by the service and not the target port.</li>
  <li>Try connecting to the pod IP directly to confirm your pod is accepting connec- tions on the correct port.</li>
  <li>If you can’t even access your app through the pod’s IP, make sure your app isn’t only binding to localhost.</li>
</ul>

<hr>

  </div><a class="u-url" href="/Notes/kubernetes/2020/09/29/Kubernetes-Service-Discovery.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/Notes/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/Notes/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/Notes/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>A Daily Notebook for @Akash</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/AkashRajvanshi" title="AkashRajvanshi"><svg class="svg-icon grey"><use xlink:href="/Notes/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/Akash_Rajvanshi" title="Akash_Rajvanshi"><svg class="svg-icon grey"><use xlink:href="/Notes/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
